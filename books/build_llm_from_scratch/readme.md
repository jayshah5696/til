# Build LLM from scratch - Sebastian Raschka

## 1. Understanding Large Language Models

- Before LLM tasks such as text classification needed handcrafted features, but they underperform LLM for complex understanding tasks
- Deeplearning advancement, compute capacity and vast amount of data caused LLM to significantly improved performace
- earlier NLP models were task specific, and LLM allows general capabilities
- Significant improvement is due to Transformet architecture

### 1.1 What is an LLM?
- Large in LLM is both model's size and immense dataset on which its trained
- Most LLM works on next token prediction - its sensible as it harness the inherent sequential nature of language
- Transformer architecture allows model to play selective attention to different parts of the input while making predictions
- Since LLMs are capable of generating text --> generative AI