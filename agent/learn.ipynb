{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Tool\n",
    "\n",
    "from tavily import TavilyClient\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from rich.pretty import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_client = TavilyClient(api_key=os.environ['TVLY_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = tavily_client.search(\"What is anomaly detection?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['results'][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_=\"What can LLM Agents DO?\"\n",
    "results = tavily_client.search(query_,\n",
    "                               include_raw_content=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(results['results'][0]['raw_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of user-agents\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.3',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1',\n",
    "    'Mozilla/5.0 (Linux; Android 8.0.0; SM-G960F Build/R16NW) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.84 Mobile Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko'\n",
    "]\n",
    "\n",
    "# Proxy details\n",
    "PROXIES = {\n",
    "    'http': 'http://your_proxy_server:port',\n",
    "    'https': 'https://your_proxy_server:port'\n",
    "}\n",
    "\n",
    "def get_text_from_url(url: str) -> str:\n",
    "    \"\"\"Fetches the text content from a given URL.\n",
    "\n",
    "    Args:\n",
    "        url: The URL to fetch the content from.\n",
    "    Returns:\n",
    "        The text content of the page, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Simulate human-like behavior with random sleep durations\n",
    "    time.sleep(random.randint(2, 5))  # Sleep for 2-5 seconds\n",
    "    try:\n",
    "        # Randomly select a user-agent\n",
    "        headers = {'User-Agent': random.choice(USER_AGENTS)}\n",
    "        response = get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Extract text from all elements, stripping whitespace\n",
    "        text = ' '.join([s.get_text(strip=True) for s in soup.find_all()])\n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while fetching content from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of urls\n",
    "urls = [result['url'] for result in results['results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_texts = [get_text_from_url(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from aiohttp import ClientSession\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (make sure you have a .env file with your OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, texts: List[dict], chunk_size: int = 500, embedding_model: str = \"text-embedding-3-small\"):\n",
    "        self.chunks = self.chunk_texts(texts, chunk_size)\n",
    "        self.bm25 = self.create_bm25()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.client = OpenAIEmbeddings(model=self.embedding_model)\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "\n",
    "    def chunk_texts(self, texts: List[dict], chunk_size: int) -> List[dict]:\n",
    "        chunks = []\n",
    "        for item in texts:\n",
    "            text = item['content']\n",
    "            url = item['url']\n",
    "            words = text.split()\n",
    "            for i in range(0, len(words), chunk_size):\n",
    "                chunk = ' '.join(words[i:i+chunk_size])\n",
    "                chunks.append({'content': chunk, 'url': url})\n",
    "        return chunks\n",
    "\n",
    "    def create_bm25(self):\n",
    "        tokenized_chunks = [chunk['content'].split() for chunk in self.chunks]\n",
    "        return BM25Okapi(tokenized_chunks)\n",
    "\n",
    "    def create_embeddings(self):\n",
    "        if self.embeddings is None:\n",
    "            texts = [chunk['content'] for chunk in self.chunks]\n",
    "            self.embeddings = self.client.embed_documents(texts)\n",
    "        return self.embeddings\n",
    "\n",
    "    def create_faiss_index(self):\n",
    "        if self.index is None:\n",
    "            embeddings = self.create_embeddings()\n",
    "            dimension = len(embeddings[0])\n",
    "            embeddings = np.array(embeddings)\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "            self.index.add(embeddings)\n",
    "        return self.index\n",
    "\n",
    "    def keyword_search(self, query: str, top_k: int = 5) -> List[dict]:\n",
    "        scores = self.bm25.get_scores(query.split())\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        return [self.chunks[i] for i in top_indices]\n",
    "\n",
    "    def semantic_search(self, query: str, top_k: int = 5) -> List[dict]:\n",
    "        query_embedding = self.client.embed_documents([query])[0]\n",
    "        index = self.create_faiss_index()\n",
    "        distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "        return [self.chunks[i] for i in indices[0]]\n",
    "\n",
    "    def combined_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[dict]:\n",
    "        keyword_scores = self.bm25.get_scores(query.split())\n",
    "        query_embedding = self.client.embed_documents([query])[0]\n",
    "        index = self.create_faiss_index()\n",
    "        distances, indices = index.search(np.array([query_embedding]), len(self.chunks))\n",
    "        semantic_scores = np.zeros(len(self.chunks))\n",
    "        semantic_scores[indices[0]] = 1 / (1 + distances[0])\n",
    "        combined_scores = alpha * keyword_scores + (1 - alpha) * semantic_scores\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        return [self.chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_texts = [{'content': get_text_from_url(url), 'url': url} for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword search results:\n",
      "[{'content': \"to review and assess its effectiveness. LLM-based agents use internal feedback mechanisms, drawing on existing models to refine their strategies. They also interact with humans to adjust their plans based on human feedback and preferences. Agents can also gather insights from their environments, both real and virtual, using outcomes and observations to refine their plans further.Two effective methods for incorporating feedback in planning areReActandReflexion.ReAct, for instance, helps an LLM solve complex tasks by cycling through a sequence of thought, action, and observation, repeating these steps as needed. It takes in feedback from the environment, which can include observations as well as input from humans or other models. This method allows the LLM to adjust its approach based on real-time feedback, enhancing its ability to answer questions more effectively.Tools useTools in this term are various resources that help LLM agents connect with external environments to perform certain tasks. These tasks might include extracting information from databases, querying, coding, and anything else the agent needs to function. When an LLM agent uses these tools, it follows specific workflows to carry out tasks, gather observations, or collect the information needed to complete subtasks and fulfill user requests.Here are some examples of how different systems integrate these tools:MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task.In one test, an LLM was trained to use a calculator for arithmetic problems. The study found that while the LLM could handle direct math queries, it struggled with word problems that required extracting numbers and operations from text. This highlights the importance of knowing when and how to use external tools effectively.Here’s anexamplewhere GPT 4 is asked to tell the answer to 4.1 * 7.9, and it fails:Source\\u200dToolformerandTALM (Tool Augmented Language Models): These models are specifically fine-tuned to interact with external APIs effectively. For instance, the model could be trained to use a financial API to analyze stock market trends or predict currency fluctuations, allowing it to provide real-time financial insights directly to users.HuggingGPT: This framework uses ChatGPT to manage tasks by selecting the best models available on the HuggingFace platform to handle specific requests and then summarizing the outcomes.API-Bank: A benchmark that tests how well LLMs can use 53 commonly used APIs to handle tasks like scheduling, health data management, or smart home control.How SuperAnnotate helps improve LLM agentsSuperAnnotate works with several leading companies that develop LLM Agent systems, helping them build better models more quickly.We help enterprises with:Fine-tuning:Fine-tuningcan enhance an agent's performance on specific tasks or teach it new capabilities. It involves training your model on large datasets of input-output pairs that clearly demonstrate what you need the model to achieve.Evaluation:Figuring out how the agent will perform on data it hasn't seen before can be tough. Most evaluation datasets out there are geared towards academic domains and may not fit your specific\", 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'arranging them like branches on a tree. chain of thought (CoT) method Tree of thought (ToT) Single-path vs. Multi-path reasoning:Source Single-path vs. Multi-path reasoning:Source Source There are also methods that use a hierarchical approach or structure plans like adecision tree, considering all possible options before finalizing a plan. While LLM-based agents are generally knowledgeable, they sometimes struggle with tasks that require specialized knowledge. Integrating these agents with domain-specific planners has proven to improve their performance. decision tree Plan reflection After creating a plan, it’s important for agents to review and assess its effectiveness. LLM-based agents use internal feedback mechanisms, drawing on existing models to refine their strategies. They also interact with humans to adjust their plans based on human feedback and preferences. Agents can also gather insights from their environments, both real and virtual, using outcomes and observations to refine their plans further. Two effective methods for incorporating feedback in planning areReActandReflexion. ReAct Reflexion ReAct, for instance, helps an LLM solve complex tasks by cycling through a sequence of thought, action, and observation, repeating these steps as needed. It takes in feedback from the environment, which can include observations as well as input from humans or other models. This method allows the LLM to adjust its approach based on real-time feedback, enhancing its ability to answer questions more effectively. Tools use Tools in this term are various resources that help LLM agents connect with external environments to perform certain tasks. These tasks might include extracting information from databases, querying, coding, and anything else the agent needs to function. When an LLM agent uses these tools, it follows specific workflows to carry out tasks, gather observations, or collect the information needed to complete subtasks and fulfill user requests. Here are some examples of how different systems integrate these tools: MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task. MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task. MRKL In one test, an LLM was trained to use a calculator for arithmetic problems. The study found that while the LLM could handle direct math queries, it struggled with word problems that required extracting numbers and operations from text. This highlights the importance of knowing when and how to use external tools effectively. Here’s anexamplewhere GPT 4 is asked to tell the answer to 4.1 * 7.9, and it fails: example Source Source Source \\u200dToolformerandTALM (Tool Augmented Language Models): These models are specifically fine-tuned to interact with external APIs effectively. For instance, the model could be trained to use a financial API to analyze stock market trends or predict currency fluctuations, allowing it to provide', 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'recommend a more adaptive strategy where agents tackle sub-tasks one by one, allowing for greater flexibility.Tree of thought (ToT)is another approach that takes the CoT technique further by exploring different paths to solve a problem. It breaks the problem into several steps, generating multiple ideas at each step and arranging them like branches on a tree.Single-path vs. Multi-path reasoning:SourceThere are also methods that use a hierarchical approach or structure plans like adecision tree, considering all possible options before finalizing a plan. While LLM-based agents are generally knowledgeable, they sometimes struggle with tasks that require specialized knowledge. Integrating these agents with domain-specific planners has proven to improve their performance.Plan reflectionAfter creating a plan, it’s important for agents to review and assess its effectiveness. LLM-based agents use internal feedback mechanisms, drawing on existing models to refine their strategies. They also interact with humans to adjust their plans based on human feedback and preferences. Agents can also gather insights from their environments, both real and virtual, using outcomes and observations to refine their plans further.Two effective methods for incorporating feedback in planning areReActandReflexion.ReAct, for instance, helps an LLM solve complex tasks by cycling through a sequence of thought, action, and observation, repeating these steps as needed. It takes in feedback from the environment, which can include observations as well as input from humans or other models. This method allows the LLM to adjust its approach based on real-time feedback, enhancing its ability to answer questions more effectively.Tools useTools in this term are various resources that help LLM agents connect with external environments to perform certain tasks. These tasks might include extracting information from databases, querying, coding, and anything else the agent needs to function. When an LLM agent uses these tools, it follows specific workflows to carry out tasks, gather observations, or collect the information needed to complete subtasks and fulfill user requests.Here are some examples of how different systems integrate these tools:MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task.In one test, an LLM was trained to use a calculator for arithmetic problems. The study found that while the LLM could handle direct math queries, it struggled with word problems that required extracting numbers and operations from text. This highlights the importance of knowing when and how to use external tools effectively.Here’s anexamplewhere GPT 4 is asked to tell the answer to 4.1 * 7.9, and it fails:Source\\u200dToolformerandTALM (Tool Augmented Language Models): These models are specifically fine-tuned to interact with external APIs effectively. For instance, the model could be trained to use a financial API to analyze stock market trends or predict currency fluctuations, allowing it to provide real-time financial insights directly to users.HuggingGPT: This framework uses ChatGPT to manage tasks by selecting the best models available on the HuggingFace platform to handle specific requests and then summarizing', 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'thought (ToT)is another approach that takes the CoT technique further by exploring different paths to solve a problem. It breaks the problem into several steps, generating multiple ideas at each step and arranging them like branches on a tree.Single-path vs. Multi-path reasoning:SourceThere are also methods that use a hierarchical approach or structure plans like adecision tree, considering all possible options before finalizing a plan. While LLM-based agents are generally knowledgeable, they sometimes struggle with tasks that require specialized knowledge. Integrating these agents with domain-specific planners has proven to improve their performance.Plan reflectionAfter creating a plan, it’s important for agents to review and assess its effectiveness. LLM-based agents use internal feedback mechanisms, drawing on existing models to refine their strategies. They also interact with humans to adjust their plans based on human feedback and preferences. Agents can also gather insights from their environments, both real and virtual, using outcomes and observations to refine their plans further.Two effective methods for incorporating feedback in planning areReActandReflexion.ReAct, for instance, helps an LLM solve complex tasks by cycling through a sequence of thought, action, and observation, repeating these steps as needed. It takes in feedback from the environment, which can include observations as well as input from humans or other models. This method allows the LLM to adjust its approach based on real-time feedback, enhancing its ability to answer questions more effectively.Tools useTools in this term are various resources that help LLM agents connect with external environments to perform certain tasks. These tasks might include extracting information from databases, querying, coding, and anything else the agent needs to function. When an LLM agent uses these tools, it follows specific workflows to carry out tasks, gather observations, or collect the information needed to complete subtasks and fulfill user requests.Here are some examples of how different systems integrate these tools:MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task.In one test, an LLM was trained to use a calculator for arithmetic problems. The study found that while the LLM could handle direct math queries, it struggled with word problems that required extracting numbers and operations from text. This highlights the importance of knowing when and how to use external tools effectively.Here’s anexamplewhere GPT 4 is asked to tell the answer to 4.1 * 7.9, and it fails:Source\\u200dToolformerandTALM (Tool Augmented Language Models): These models are specifically fine-tuned to interact with external APIs effectively. For instance, the model could be trained to use a financial API to analyze stock market trends or predict currency fluctuations, allowing it to provide real-time financial insights directly to users.HuggingGPT: This framework uses ChatGPT to manage tasks by selecting the best models available on the HuggingFace platform to handle specific requests and then summarizing the outcomes.API-Bank: A benchmark that tests how well LLMs can use 53 commonly used APIs to handle', 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'supposed to: ranks and stores the tasks in order of importance. In the end, the execution agent completes the tasks it can and often reveals and queues up upcoming activities. The cycle begins again when the priorities have been rearranged. This multi-agent process is depicted in the following image.\\u200d3. CensusGPTThe US Census Database may be easily accessed and explored with the help of Census GPT, an AI-powered search engine. Natural language processing is used to decipher user queries, making for a quick and easy search experience.Ask a query, and CensusGPT will respond in tabular data and visualizations, resolving the issue of inaccessible census data. Researchers, economists, and anyone interested in using freely available census data to address demographics, ethnicity, and wealth issues will find this tool invaluable. CensusGPT is based on the public-domain TextSQL project, which uses artificial intelligence to transform queries into SQL so that users may \"talk\" to any dataset in their native language.How to implement LLM Agents1. Data CollectionCompile a sizable and varied corpus of text that is pertinent to the area you want the LLM agent to focus on. The language model will be trained using this dataset.2. Preprocessing DataClean up and preprocess the text data gathered by removing noise, inconsistent formatting, and superfluous information. Tokenize the text to break it up into more manageable chunks for model training.3. Training the Language ModelUse ML methods, particularly NLP strategies, to train the language model using the preprocessed dataset. Transformer models and other deep learning architectures are useful for training LLM agents. During training, text sequences are fed to the language model while its parameters are optimized to learn the statistical relationships and patterns found in the data.4. Fine-tuningTo improve performance and adapt the pre-trained language model to your intended use case, fine-tune it to a more specific task or area. To achieve this, the model must be trained on a dataset unique to the job while retaining prior knowledge.5. Evaluation and IterationAssess the LLM agent\\'s performance using the proper metrics, such as perplexity or accuracy, and make necessary model revisions. Improve the agent\\'s abilities over time by iterating the training and fine-tuning procedure.6. Deployment and IntegrationDeploy the LLM agent in a production environment or integrate it into the platform or application you want once its performance is satisfactory. The APIs or interfaces required for communication with the agent.7. Continuous Learning and ImprovementRegularly update and retrain the LLM agent with the most recent knowledge. By doing this, the agent is kept current and keeps being relevant over time.Challenges of LLM agents1. UX issuesThe current agent system uses natural language to connect LLMs with other components like memory and tools. However, LLMs can make formatting errors and occasionally exhibit rebellious behaviour (e.g., refuse to follow an order), so it\\'s hard to trust the results of these models. Because of this, much of the agent demonstration code is dedicated to analysing model output.2. long-term planning and task decompositionLong-term planning and thorough solution space exploration remain formidable obstacles. LLMs are not', 'url': 'https://www.ionio.ai/blog/what-is-llm-agent-ultimate-guide-to-llm-agent-with-technical-breakdown'}]\n",
      "\n",
      "Semantic search results:\n",
      "[{'content': 'just black and white. However, they have limitations, such as a short memory span and a need for precise directions. By working to overcome these challenges, we can enhance their abilities and make them even more effective and adept at complex LLM problems. When you face a problem with no simple answer, you often need to follow several steps, think carefully, and remember what you’ve already tried. LLM agents are designed for exactly these kinds of situations in language model applications. They combine thorough data analysis, strategic planning, data retrieval, and the ability to learn from past actions to solve complex issues. In this article, we\\'ll explore what LLM agents are, their benefits, abilities, practical examples, and the challenges they face. LLM agents LLM agents What are LLM agents? LLM agents are advanced AI systems designed for creating complex text that needs sequential reasoning. They can think ahead, remember past conversations, and use different tools to adjust their responses based on the situation and style needed. Consider a question in the legal field that sounds like this: \"What are the potential legal outcomes of a specific type of contract breach in California?\" \"What are the potential legal outcomes of a specific type of contract breach in California?\" \"What are the potential legal outcomes of a specific type of contract breach in California?\" A basic LLM with aretrieval augmented generation (RAG)system can easily fetch the needed information from legal databases. retrieval augmented generation (RAG) Now, consider a more detailed scenario: \"In light of new data privacy laws, what are the common legal challenges companies face, and how have courts addressed these issues?\" \"In light of new data privacy laws, what are the common legal challenges companies face, and how have courts addressed these issues?\" \"In light of new data privacy laws, what are the common legal challenges companies face, and how have courts addressed these issues?\" This question digs deeper than just looking up facts. It\\'s about understanding new rules, how they affect different companies, and finding out what courts have said about it all. A simple RAG system can pull up relevant laws and cases, but it lacks the ability to connect these laws to actual business situations or analyze court decisions in depth. In such situations, LLM agents step in. When the project demands sequential reasoning, planning, and memory, LLM agents shine. For this question, the agent can break down its tasks into subtasks like so. The first subtask may be accessing legal databases to retrieve the latest laws and regulations. Secondly, it can establish a historical baseline of how similar issues were previously handled. Another subtask can be summarizing legal documents and forecasting future trends based on observed patterns. To complete these subtasks, the LLM agent requires a structured plan, a reliable memory to track progress, and access to necessary tools. These components form the backbone of an LLM agent’s workflow. LLM agent components LLM agents generally consist of four components: Agent/brainPlanningMemoryTool use Agent/brain Planning Memory Tool use', 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'human values is a complex challenge.Prompt dependence:LLM agents operate based on prompts, but these prompts need to be very precise. Even small changes can lead to big mistakes, so creating and refining these prompts can be a delicate process.Managing knowledge:Keeping an LLM agent\\'s knowledge accurate and unbiased is tricky. They must have the right information to make informed decisions, but too much irrelevant information can lead them to draw incorrect conclusions or act on outdated facts.Cost and efficiency:Running LLM agents can be resource-intensive. They often need to process a lot of data quickly, which can be costly and may slow down their performance if not managed well.Addressing these challenges is crucial for improving the effectiveness and reliability of LLM agents in various applications.Final thoughtsIn conclusion, LLM agents are powerful tools for tackling complex LLM tasks. They can plan, find information, remember past interactions, and learn from them, making them indispensable when answers aren\\'t just black and white. However, they have limitations, such as a short memory span and a need for precise directions. By working to overcome these challenges, we can enhance their abilities and make them even more effective and adept at complex LLM problems. Back to blogContentsTable of content Item Back to blogContentsTable of content Item Back to blog Back to blog Contents Table of content Item Table of content Item When you face a problem with no simple answer, you often need to follow several steps, think carefully, and remember what you’ve already tried. LLM agents are designed for exactly these kinds of situations in language model applications. They combine thorough data analysis, strategic planning, data retrieval, and the ability to learn from past actions to solve complex issues.In this article, we\\'ll explore what LLM agents are, their benefits, abilities, practical examples, and the challenges they face.LLM agentsWhat are LLM agents?LLM agents are advanced AI systems designed for creating complex text that needs sequential reasoning. They can think ahead, remember past conversations, and use different tools to adjust their responses based on the situation and style needed.Consider a question in the legal field that sounds like this:\"What are the potential legal outcomes of a specific type of contract breach in California?\"A basic LLM with aretrieval augmented generation (RAG)system can easily fetch the needed information from legal databases.Now, consider a more detailed scenario:\"In light of new data privacy laws, what are the common legal challenges companies face, and how have courts addressed these issues?\"This question digs deeper than just looking up facts. It\\'s about understanding new rules, how they affect different companies, and finding out what courts have said about it all. A simple RAG system can pull up relevant laws and cases, but it lacks the ability to connect these laws to actual business situations or analyze court decisions in depth.In such situations, LLM agents step in. When the project demands sequential reasoning, planning, and memory, LLM agents shine.For this question, the agent can break down its tasks into subtasks like so. The first subtask may be', 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'Integrations & Security Pricing Sign InRequest Demo Sign In Request Demo FeaturedLLMLLM agents: The ultimate guideMay 21, 20248 minThank you for subscribing to our newsletter!Oops! Something went wrong while submitting the form. FeaturedLLMLLM agents: The ultimate guideMay 21, 20248 minThank you for subscribing to our newsletter!Oops! Something went wrong while submitting the form. FeaturedLLMLLM agents: The ultimate guideMay 21, 20248 minThank you for subscribing to our newsletter!Oops! Something went wrong while submitting the form. FeaturedLLMLLM agents: The ultimate guideMay 21, 20248 minThank you for subscribing to our newsletter!Oops! Something went wrong while submitting the form. FeaturedLLMLLM agents: The ultimate guideMay 21, 20248 min FeaturedLLM FeaturedLLM Featured Featured LLM LLM agents: The ultimate guide LLM agents: The ultimate guide May 21, 20248 min May 21, 2024 8 min 8 min Thank you for subscribing to our newsletter!Oops! Something went wrong while submitting the form. Thank you for subscribing to our newsletter!Oops! Something went wrong while submitting the form. Thank you for subscribing to our newsletter!Oops! Something went wrong while submitting the form. Thank you for subscribing to our newsletter! Thank you for subscribing to our newsletter! Oops! Something went wrong while submitting the form. Oops! Something went wrong while submitting the form. Back to blogContentsTable of content ItemWhen you face a problem with no simple answer, you often need to follow several steps, think carefully, and remember what you’ve already tried. LLM agents are designed for exactly these kinds of situations in language model applications. They combine thorough data analysis, strategic planning, data retrieval, and the ability to learn from past actions to solve complex issues.In this article, we\\'ll explore what LLM agents are, their benefits, abilities, practical examples, and the challenges they face.LLM agentsWhat are LLM agents?LLM agents are advanced AI systems designed for creating complex text that needs sequential reasoning. They can think ahead, remember past conversations, and use different tools to adjust their responses based on the situation and style needed.Consider a question in the legal field that sounds like this:\"What are the potential legal outcomes of a specific type of contract breach in California?\"A basic LLM with aretrieval augmented generation (RAG)system can easily fetch the needed information from legal databases.Now, consider a more detailed scenario:\"In light of new data privacy laws, what are the common legal challenges companies face, and how have courts addressed these issues?\"This question digs deeper than just looking up facts. It\\'s about understanding new rules, how they affect different companies, and finding out what courts have said about it all. A simple RAG system can pull up relevant laws and cases, but it lacks the ability to connect these laws to actual business situations or analyze court decisions in depth.In such situations, LLM agents step in. When the project demands sequential reasoning, planning, and memory, LLM agents shine.For this question, the agent can break down its tasks into subtasks like so. The first subtask may be accessing legal databases to retrieve the latest laws and regulations. Secondly, it can establish a historical baseline', 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'LLM agents: The ultimate guide | SuperAnnotateJoin our upcoming webinar “Deriving Business Value from LLMs and RAGs.”Register nowPlatformProductsFine-tuneCreate top-quality training data across al data types.ExploreManage, version and debug your data and create datasets faster.OrchestrateBuild robust CI/CD pipelines using advanced orchestration.Integrate/ETLIntegrate data from your hardware or the cloud.SolutionsLLMs & Gen AIImageNatural LanguageVideoAudioMarketplaceWForceExpert WorkforceProject ManagementResourcesBlogPodcastWebinarDocumentationWhat’s newPython SDKIntegrations & SecurityPricingSign InRequest DemoFeaturedLLMLLM agents: The ultimate guideMay 21, 20248 minThank you for subscribing to our newsletter!Oops! Something went wrong while submitting the form.Back to blogContentsTable of content ItemWhen you face a problem with no simple answer, you often need to follow several steps, think carefully, and remember what you’ve already tried. LLM agents are designed for exactly these kinds of situations in language model applications. They combine thorough data analysis, strategic planning, data retrieval, and the ability to learn from past actions to solve complex issues.In this article, we\\'ll explore what LLM agents are, their benefits, abilities, practical examples, and the challenges they face.LLM agentsWhat are LLM agents?LLM agents are advanced AI systems designed for creating complex text that needs sequential reasoning. They can think ahead, remember past conversations, and use different tools to adjust their responses based on the situation and style needed.Consider a question in the legal field that sounds like this:\"What are the potential legal outcomes of a specific type of contract breach in California?\"A basic LLM with aretrieval augmented generation (RAG)system can easily fetch the needed information from legal databases.Now, consider a more detailed scenario:\"In light of new data privacy laws, what are the common legal challenges companies face, and how have courts addressed these issues?\"This question digs deeper than just looking up facts. It\\'s about understanding new rules, how they affect different companies, and finding out what courts have said about it all. A simple RAG system can pull up relevant laws and cases, but it lacks the ability to connect these laws to actual business situations or analyze court decisions in depth.In such situations, LLM agents step in. When the project demands sequential reasoning, planning, and memory, LLM agents shine.For this question, the agent can break down its tasks into subtasks like so. The first subtask may be accessing legal databases to retrieve the latest laws and regulations. Secondly, it can establish a historical baseline of how similar issues were previously handled. Another subtask can be summarizing legal documents and forecasting future trends based on observed patterns.To complete these subtasks, the LLM agent requires a structured plan, a reliable memory to track progress, and access to necessary tools. These components form the backbone of an LLM agent’s workflow.LLM agent componentsLLM agents generally consist of four components:Agent/brainPlanningMemoryTool useLLM agent structureLet’s discuss each of them.Agent/brainAt the core of an LLM agent is a language model that processes and understands language based on a vast amount of data it\\'s been trained on.When you use an LLM agent, you start by giving it a specific prompt. This prompt is crucial—it guides the agent on how to respond, what tools to use, and', 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'potential legal outcomes of a specific type of contract breach in California?\"A basic LLM with aretrieval augmented generation (RAG)system can easily fetch the needed information from legal databases.Now, consider a more detailed scenario:\"In light of new data privacy laws, what are the common legal challenges companies face, and how have courts addressed these issues?\"This question digs deeper than just looking up facts. It\\'s about understanding new rules, how they affect different companies, and finding out what courts have said about it all. A simple RAG system can pull up relevant laws and cases, but it lacks the ability to connect these laws to actual business situations or analyze court decisions in depth.In such situations, LLM agents step in. When the project demands sequential reasoning, planning, and memory, LLM agents shine.For this question, the agent can break down its tasks into subtasks like so. The first subtask may be accessing legal databases to retrieve the latest laws and regulations. Secondly, it can establish a historical baseline of how similar issues were previously handled. Another subtask can be summarizing legal documents and forecasting future trends based on observed patterns.To complete these subtasks, the LLM agent requires a structured plan, a reliable memory to track progress, and access to necessary tools. These components form the backbone of an LLM agent’s workflow.LLM agent componentsLLM agents generally consist of four components:Agent/brainPlanningMemoryTool useLLM agent structureLet’s discuss each of them.Agent/brainAt the core of an LLM agent is a language model that processes and understands language based on a vast amount of data it\\'s been trained on.When you use an LLM agent, you start by giving it a specific prompt. This prompt is crucial—it guides the agent on how to respond, what tools to use, and the goals it should aim to achieve during the interaction. It\\'s like giving directions to a navigator before a journey.Additionally, you can customize the agent with aspecific persona. This means setting up the agent with certain characteristics and expertise that make it better suited for particular tasks or interactions. It\\'s about tuning the agent to perform tasks in a way that feels right for the situation.Essentially, the core of an LLM agent combines advanced processing abilities with customizable features to effectively handle and adapt to various tasks and interactions.MemoryThe memory of LLM agentshelps them handle complex LLM tasks with a record of what’s been done before. There are two main memory types:Short-term memory:This is like the agent’s notepad, where it quickly writes down important details during a conversation. It keeps track of the ongoing discussion, helping the model respond relevantly to the immediate context. However, this memory is temporary, clearing out once the task at hand is completed.Long-term memory:Think of this as the agent’s diary, storing insights and information from past interactions over weeks or even months. This isn\\'t just about holding data; it\\'s about understanding patterns, learning from previous tasks, and recalling this information to make better decisions in future interactions.By blending these two types of memory, the model can keep', 'url': 'https://www.superannotate.com/blog/llm-agents'}]\n",
      "\n",
      "Combined search results:\n",
      "[{'content': \"to review and assess its effectiveness. LLM-based agents use internal feedback mechanisms, drawing on existing models to refine their strategies. They also interact with humans to adjust their plans based on human feedback and preferences. Agents can also gather insights from their environments, both real and virtual, using outcomes and observations to refine their plans further.Two effective methods for incorporating feedback in planning areReActandReflexion.ReAct, for instance, helps an LLM solve complex tasks by cycling through a sequence of thought, action, and observation, repeating these steps as needed. It takes in feedback from the environment, which can include observations as well as input from humans or other models. This method allows the LLM to adjust its approach based on real-time feedback, enhancing its ability to answer questions more effectively.Tools useTools in this term are various resources that help LLM agents connect with external environments to perform certain tasks. These tasks might include extracting information from databases, querying, coding, and anything else the agent needs to function. When an LLM agent uses these tools, it follows specific workflows to carry out tasks, gather observations, or collect the information needed to complete subtasks and fulfill user requests.Here are some examples of how different systems integrate these tools:MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task.In one test, an LLM was trained to use a calculator for arithmetic problems. The study found that while the LLM could handle direct math queries, it struggled with word problems that required extracting numbers and operations from text. This highlights the importance of knowing when and how to use external tools effectively.Here’s anexamplewhere GPT 4 is asked to tell the answer to 4.1 * 7.9, and it fails:Source\\u200dToolformerandTALM (Tool Augmented Language Models): These models are specifically fine-tuned to interact with external APIs effectively. For instance, the model could be trained to use a financial API to analyze stock market trends or predict currency fluctuations, allowing it to provide real-time financial insights directly to users.HuggingGPT: This framework uses ChatGPT to manage tasks by selecting the best models available on the HuggingFace platform to handle specific requests and then summarizing the outcomes.API-Bank: A benchmark that tests how well LLMs can use 53 commonly used APIs to handle tasks like scheduling, health data management, or smart home control.How SuperAnnotate helps improve LLM agentsSuperAnnotate works with several leading companies that develop LLM Agent systems, helping them build better models more quickly.We help enterprises with:Fine-tuning:Fine-tuningcan enhance an agent's performance on specific tasks or teach it new capabilities. It involves training your model on large datasets of input-output pairs that clearly demonstrate what you need the model to achieve.Evaluation:Figuring out how the agent will perform on data it hasn't seen before can be tough. Most evaluation datasets out there are geared towards academic domains and may not fit your specific\", 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'arranging them like branches on a tree. chain of thought (CoT) method Tree of thought (ToT) Single-path vs. Multi-path reasoning:Source Single-path vs. Multi-path reasoning:Source Source There are also methods that use a hierarchical approach or structure plans like adecision tree, considering all possible options before finalizing a plan. While LLM-based agents are generally knowledgeable, they sometimes struggle with tasks that require specialized knowledge. Integrating these agents with domain-specific planners has proven to improve their performance. decision tree Plan reflection After creating a plan, it’s important for agents to review and assess its effectiveness. LLM-based agents use internal feedback mechanisms, drawing on existing models to refine their strategies. They also interact with humans to adjust their plans based on human feedback and preferences. Agents can also gather insights from their environments, both real and virtual, using outcomes and observations to refine their plans further. Two effective methods for incorporating feedback in planning areReActandReflexion. ReAct Reflexion ReAct, for instance, helps an LLM solve complex tasks by cycling through a sequence of thought, action, and observation, repeating these steps as needed. It takes in feedback from the environment, which can include observations as well as input from humans or other models. This method allows the LLM to adjust its approach based on real-time feedback, enhancing its ability to answer questions more effectively. Tools use Tools in this term are various resources that help LLM agents connect with external environments to perform certain tasks. These tasks might include extracting information from databases, querying, coding, and anything else the agent needs to function. When an LLM agent uses these tools, it follows specific workflows to carry out tasks, gather observations, or collect the information needed to complete subtasks and fulfill user requests. Here are some examples of how different systems integrate these tools: MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task. MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task. MRKL In one test, an LLM was trained to use a calculator for arithmetic problems. The study found that while the LLM could handle direct math queries, it struggled with word problems that required extracting numbers and operations from text. This highlights the importance of knowing when and how to use external tools effectively. Here’s anexamplewhere GPT 4 is asked to tell the answer to 4.1 * 7.9, and it fails: example Source Source Source \\u200dToolformerandTALM (Tool Augmented Language Models): These models are specifically fine-tuned to interact with external APIs effectively. For instance, the model could be trained to use a financial API to analyze stock market trends or predict currency fluctuations, allowing it to provide', 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'up with current conversations and tap into a rich history of interactions. This means it can offer more tailored responses and remember user preferences over time, making each conversation feel more connected and relevant. In essence, the agent is building an understanding that helps it serve you better in each interaction.PlanningThrough planning, LLM agents can reason, break down complicated tasks into smaller, more manageable parts, and develop specific plans for each part. As tasks evolve, agents can also reflect on and adjust their plans, making sure they stay relevant to real-world situations. This adaptability is key to successfully completing tasks.Planning typically involves two main stages: plan formulation and plan reflection.Plan formulationDuring this stage, agents break down a large task into smaller sub-tasks. Some task decomposition approaches suggest creating a detailed plan all at once and then following it step by step. Others, like thechain of thought (CoT) method, recommend a more adaptive strategy where agents tackle sub-tasks one by one, allowing for greater flexibility.Tree of thought (ToT)is another approach that takes the CoT technique further by exploring different paths to solve a problem. It breaks the problem into several steps, generating multiple ideas at each step and arranging them like branches on a tree.Single-path vs. Multi-path reasoning:SourceThere are also methods that use a hierarchical approach or structure plans like adecision tree, considering all possible options before finalizing a plan. While LLM-based agents are generally knowledgeable, they sometimes struggle with tasks that require specialized knowledge. Integrating these agents with domain-specific planners has proven to improve their performance.Plan reflectionAfter creating a plan, it’s important for agents to review and assess its effectiveness. LLM-based agents use internal feedback mechanisms, drawing on existing models to refine their strategies. They also interact with humans to adjust their plans based on human feedback and preferences. Agents can also gather insights from their environments, both real and virtual, using outcomes and observations to refine their plans further.Two effective methods for incorporating feedback in planning areReActandReflexion.ReAct, for instance, helps an LLM solve complex tasks by cycling through a sequence of thought, action, and observation, repeating these steps as needed. It takes in feedback from the environment, which can include observations as well as input from humans or other models. This method allows the LLM to adjust its approach based on real-time feedback, enhancing its ability to answer questions more effectively.Tools useTools in this term are various resources that help LLM agents connect with external environments to perform certain tasks. These tasks might include extracting information from databases, querying, coding, and anything else the agent needs to function. When an LLM agent uses these tools, it follows specific workflows to carry out tasks, gather observations, or collect the information needed to complete subtasks and fulfill user requests.Here are some examples of how different systems integrate these tools:MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing', 'url': 'https://www.superannotate.com/blog/llm-agents'}, {'content': 'user queries, making for a quick and easy search experience.Ask a query, and CensusGPT will respond in tabular data and visualizations, resolving the issue of inaccessible census data. Researchers, economists, and anyone interested in using freely available census data to address demographics, ethnicity, and wealth issues will find this tool invaluable. CensusGPT is based on the public-domain TextSQL project, which uses artificial intelligence to transform queries into SQL so that users may \"talk\" to any dataset in their native language.How to implement LLM Agents1. Data CollectionCompile a sizable and varied corpus of text that is pertinent to the area you want the LLM agent to focus on. The language model will be trained using this dataset.2. Preprocessing DataClean up and preprocess the text data gathered by removing noise, inconsistent formatting, and superfluous information. Tokenize the text to break it up into more manageable chunks for model training.3. Training the Language ModelUse ML methods, particularly NLP strategies, to train the language model using the preprocessed dataset. Transformer models and other deep learning architectures are useful for training LLM agents. During training, text sequences are fed to the language model while its parameters are optimized to learn the statistical relationships and patterns found in the data.4. Fine-tuningTo improve performance and adapt the pre-trained language model to your intended use case, fine-tune it to a more specific task or area. To achieve this, the model must be trained on a dataset unique to the job while retaining prior knowledge.5. Evaluation and IterationAssess the LLM agent\\'s performance using the proper metrics, such as perplexity or accuracy, and make necessary model revisions. Improve the agent\\'s abilities over time by iterating the training and fine-tuning procedure.6. Deployment and IntegrationDeploy the LLM agent in a production environment or integrate it into the platform or application you want once its performance is satisfactory. The APIs or interfaces required for communication with the agent.7. Continuous Learning and ImprovementRegularly update and retrain the LLM agent with the most recent knowledge. By doing this, the agent is kept current and keeps being relevant over time.Challenges of LLM agents1. UX issuesThe current agent system uses natural language to connect LLMs with other components like memory and tools. However, LLMs can make formatting errors and occasionally exhibit rebellious behaviour (e.g., refuse to follow an order), so it\\'s hard to trust the results of these models. Because of this, much of the agent demonstration code is dedicated to analysing model output.2. long-term planning and task decompositionLong-term planning and thorough solution space exploration remain formidable obstacles. LLMs are not as resilient as opposed to people who learn through trial and error because they are unable to change plans in the face of unforeseen errors.3. Trust and privacyUsers have no say over when the tool is executed (LLM does it) or how the utility is invoked. Is it risky to utilise any old third-party plugin or tool? This is a major issue if the programme can access sensitive information or perform \"admin\" level tasks (like sending', 'url': 'https://www.ionio.ai/blog/what-is-llm-agent-ultimate-guide-to-llm-agent-with-technical-breakdown'}, {'content': 'thought (ToT)is another approach that takes the CoT technique further by exploring different paths to solve a problem. It breaks the problem into several steps, generating multiple ideas at each step and arranging them like branches on a tree.Single-path vs. Multi-path reasoning:SourceThere are also methods that use a hierarchical approach or structure plans like adecision tree, considering all possible options before finalizing a plan. While LLM-based agents are generally knowledgeable, they sometimes struggle with tasks that require specialized knowledge. Integrating these agents with domain-specific planners has proven to improve their performance.Plan reflectionAfter creating a plan, it’s important for agents to review and assess its effectiveness. LLM-based agents use internal feedback mechanisms, drawing on existing models to refine their strategies. They also interact with humans to adjust their plans based on human feedback and preferences. Agents can also gather insights from their environments, both real and virtual, using outcomes and observations to refine their plans further.Two effective methods for incorporating feedback in planning areReActandReflexion.ReAct, for instance, helps an LLM solve complex tasks by cycling through a sequence of thought, action, and observation, repeating these steps as needed. It takes in feedback from the environment, which can include observations as well as input from humans or other models. This method allows the LLM to adjust its approach based on real-time feedback, enhancing its ability to answer questions more effectively.Tools useTools in this term are various resources that help LLM agents connect with external environments to perform certain tasks. These tasks might include extracting information from databases, querying, coding, and anything else the agent needs to function. When an LLM agent uses these tools, it follows specific workflows to carry out tasks, gather observations, or collect the information needed to complete subtasks and fulfill user requests.Here are some examples of how different systems integrate these tools:MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task.In one test, an LLM was trained to use a calculator for arithmetic problems. The study found that while the LLM could handle direct math queries, it struggled with word problems that required extracting numbers and operations from text. This highlights the importance of knowing when and how to use external tools effectively.Here’s anexamplewhere GPT 4 is asked to tell the answer to 4.1 * 7.9, and it fails:Source\\u200dToolformerandTALM (Tool Augmented Language Models): These models are specifically fine-tuned to interact with external APIs effectively. For instance, the model could be trained to use a financial API to analyze stock market trends or predict currency fluctuations, allowing it to provide real-time financial insights directly to users.HuggingGPT: This framework uses ChatGPT to manage tasks by selecting the best models available on the HuggingFace platform to handle specific requests and then summarizing the outcomes.API-Bank: A benchmark that tests how well LLMs can use 53 commonly used APIs to handle', 'url': 'https://www.superannotate.com/blog/llm-agents'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "# texts = [\n",
    "#     \"The quick brown fox jumps over the lazy dog.\",\n",
    "#     \"A journey of a thousand miles begins with a single step.\",\n",
    "#     \"To be or not to be, that is the question.\",\n",
    "#     \"All that glitters is not gold.\",\n",
    "#     \"Where there's a will, there's a way.\"\n",
    "# ]\n",
    "texts = dict_of_texts\n",
    "\n",
    "rag = RAGSystem(texts)\n",
    "\n",
    "query = \"LLM and its reasearch\"\n",
    "print(\"Keyword search results:\")\n",
    "print(rag.keyword_search(query))\n",
    "\n",
    "print(\"\\nSemantic search results:\")\n",
    "print(rag.semantic_search(query))\n",
    "\n",
    "print(\"\\nCombined search results:\")\n",
    "print(rag.combined_search(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = rag.combined_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"to review and assess its effectiveness. LLM-based agents use internal feedback mechanisms, drawing on existing models to refine their strategies. They also interact with humans to adjust their plans based on human feedback and preferences. Agents can also gather insights from their environments, both real and virtual, using outcomes and observations to refine their plans further.Two effective methods for incorporating feedback in planning areReActandReflexion.ReAct, for instance, helps an LLM solve complex tasks by cycling through a sequence of thought, action, and observation, repeating these steps as needed. It takes in feedback from the environment, which can include observations as well as input from humans or other models. This method allows the LLM to adjust its approach based on real-time feedback, enhancing its ability to answer questions more effectively.Tools useTools in this term are various resources that help LLM agents connect with external environments to perform certain tasks. These tasks might include extracting information from databases, querying, coding, and anything else the agent needs to function. When an LLM agent uses these tools, it follows specific workflows to carry out tasks, gather observations, or collect the information needed to complete subtasks and fulfill user requests.Here are some examples of how different systems integrate these tools:MRKL(Modular reasoning, knowledge, and language): This system uses a collection of expert modules, ranging from neural networks to simple tools like calculators or weather APIs. The main LLM acts as a router, directing queries to the appropriate expert module based on the task.In one test, an LLM was trained to use a calculator for arithmetic problems. The study found that while the LLM could handle direct math queries, it struggled with word problems that required extracting numbers and operations from text. This highlights the importance of knowing when and how to use external tools effectively.Here’s anexamplewhere GPT 4 is asked to tell the answer to 4.1 * 7.9, and it fails:Source\\u200dToolformerandTALM (Tool Augmented Language Models): These models are specifically fine-tuned to interact with external APIs effectively. For instance, the model could be trained to use a financial API to analyze stock market trends or predict currency fluctuations, allowing it to provide real-time financial insights directly to users.HuggingGPT: This framework uses ChatGPT to manage tasks by selecting the best models available on the HuggingFace platform to handle specific requests and then summarizing the outcomes.API-Bank: A benchmark that tests how well LLMs can use 53 commonly used APIs to handle tasks like scheduling, health data management, or smart home control.How SuperAnnotate helps improve LLM agentsSuperAnnotate works with several leading companies that develop LLM Agent systems, helping them build better models more quickly.We help enterprises with:Fine-tuning:Fine-tuningcan enhance an agent's performance on specific tasks or teach it new capabilities. It involves training your model on large datasets of input-output pairs that clearly demonstrate what you need the model to achieve.Evaluation:Figuring out how the agent will perform on data it hasn't seen before can be tough. Most evaluation datasets out there are geared towards academic domains and may not fit your specific\",\n",
       " 'url': 'https://www.superannotate.com/blog/llm-agents'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default flashrank model for language en\n",
      "Default Model: ms-marco-MiniLM-L-12-v2\n",
      "Loading FlashRankRanker model ms-marco-MiniLM-L-12-v2\n",
      "You don't have the necessary dependencies installed to use FlashRankRanker.\n",
      "Please install the necessary dependencies for FlashRankRanker by running `pip install \"rerankers[flashrank]\"` or `pip install \"rerankers[all]\" to install the dependencies for all reranker types.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List\n",
    "import faiss\n",
    "import fastavro\n",
    "# from fastavro.schema import load_schema\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import re\n",
    "import litellm\n",
    "from rerankers import Reranker\n",
    "from functools import lru_cache  # Import lru_cache\n",
    "\n",
    "\n",
    "class LiteLLMEmbeddingClient:\n",
    "    def __init__(self, model: str, api_key: str):\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "    async def embed_document(self, text: str) -> List[float]:\n",
    "        response = litellm.embedding(input=[text], model=self.model, api_key=self.api_key)\n",
    "        return response['data'][0]['embedding']\n",
    "\n",
    "    async def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        tasks = [self.embed_document(text) for text in texts]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "    async def embed_query(self, query: str) -> List[float]:\n",
    "        return await self.embed_document(query)\n",
    "\n",
    "class RetrievalEngine:\n",
    "    def __init__(self, texts: List[dict],\n",
    "                chunk_size: int = 500,\n",
    "                overlap: int = 100,\n",
    "                tokens: bool = False,\n",
    "                embed_client = LiteLLMEmbeddingClient(model= \"text-embedding-3-small\",\n",
    "                                                      api_key=os.environ['OPENAI_API_KEY']),\n",
    "                reranker = Reranker('flashrank')):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.tokens = tokens\n",
    "        self.chunks = self.chunk_texts(texts)\n",
    "        self.bm25 = self.create_bm25()\n",
    "        self.embed_client = embed_client\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "        self.reranker = reranker\n",
    "        \n",
    "\n",
    "    def chunk_texts(self, texts: List[dict]) -> List[dict]:\n",
    "        chunks = []\n",
    "        for item in texts:\n",
    "            text = item['content']\n",
    "            url = item['url']\n",
    "            if self.tokens:\n",
    "                chunked_texts = self.chunk_text_by_tokens(text, self.chunk_size, self.overlap)\n",
    "            else:\n",
    "                chunked_texts = self.chunk_text(text, self.chunk_size, self.overlap)\n",
    "            for chunk in chunked_texts:\n",
    "                chunks.append({'content': chunk, 'url': url})\n",
    "        return chunks\n",
    "\n",
    "    def chunk_text(self, text, max_char_length=1000, overlap=0):\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        sentences = re.split(r'(\\.|\\?|!)', text.replace('\\n', ' '))\n",
    "\n",
    "        for sentence in sentences:\n",
    "            trimmed_sentence = sentence.strip()\n",
    "            if not trimmed_sentence:\n",
    "                continue\n",
    "\n",
    "            chunk_length = len(current_chunk) + len(trimmed_sentence) + 1\n",
    "            lower_bound = max_char_length - max_char_length * 0.5\n",
    "            upper_bound = max_char_length + max_char_length * 0.5\n",
    "\n",
    "            if lower_bound <= chunk_length <= upper_bound and current_chunk:\n",
    "                current_chunk = re.sub(r'^\\.\\s+', \"\", current_chunk).strip()\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "            elif chunk_length > upper_bound:\n",
    "                current_chunk = re.sub(r'^\\.\\s+', \"\", current_chunk).strip()\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = trimmed_sentence\n",
    "            else:\n",
    "                current_chunk += f\" {trimmed_sentence}\"\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "        if overlap > 0:\n",
    "            overlapped_chunks = []\n",
    "            for i in range(len(chunks)):\n",
    "                start = max(0, i - overlap)\n",
    "                end = min(len(chunks), i + 1)\n",
    "                overlapped_chunks.append(' '.join(chunks[start:end]))\n",
    "            return overlapped_chunks\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def chunk_text_by_tokens(self, text, max_token_length=100, overlap=0):\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        tokens = enc.encode(text)\n",
    "        chunks = []\n",
    "\n",
    "        for i in range(0, len(tokens), max_token_length - overlap):\n",
    "            chunk_tokens = tokens[i:i + max_token_length]\n",
    "            chunk_text = enc.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def create_bm25(self):\n",
    "        tokenized_chunks = [chunk['content'].split() for chunk in self.chunks]\n",
    "        return BM25Okapi(tokenized_chunks)\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    async def create_embeddings(self):\n",
    "        if self.embeddings is None:\n",
    "            texts = [chunk['content'] for chunk in self.chunks]\n",
    "            self.embeddings = await self.embed_client.embed_documents(texts)\n",
    "        return self.embeddings\n",
    "    \n",
    "    @lru_cache(maxsize=128)\n",
    "    async def semantic_query_run(self, query: str, top_k: int = 5) -> List[dict]:\n",
    "        query_embedding = await self.embed_client.embed_documents([query])\n",
    "        index = await self.create_faiss_index()  # Ensure this is awaited\n",
    "        distances, indices = index.search(np.array(query_embedding).astype('float32'), top_k)\n",
    "        return distances, indices\n",
    "    \n",
    "    @lru_cache(maxsize=128)\n",
    "    async def semantic_search(self, query: str, top_k: int = 5) -> List[dict]:\n",
    "        distance, indices = await self.semantic_query_run(query, top_k)\n",
    "        return [self.chunks[i] for i in indices[0]]\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    async def create_faiss_index(self):\n",
    "        if self.index is None:\n",
    "            embeddings = await self.create_embeddings()\n",
    "            dimension = len(embeddings[0])\n",
    "            embeddings = np.array(embeddings)\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "            self.index.add(embeddings)\n",
    "        return self.index\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    async def keyword_search(self, query: str, top_k: int = 5) -> List[dict]:\n",
    "        # Get BM25 scores for the query\n",
    "        scores = self.bm25.get_scores(query.split())\n",
    "        # Get indices of top_k scores in descending order\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        # Return the chunks corresponding to the top indices\n",
    "        return [self.chunks[i] for i in top_indices]\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    async def combined_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[dict]:\n",
    "        keyword_scores = self.bm25.get_scores(query.split())\n",
    "        keyword_scores = self.normalize_scores(keyword_scores)  # Normalize keyword scores\n",
    "        distances, indices = await self.semantic_query_run(query, len(self.chunks))\n",
    "        semantic_scores = np.zeros(len(self.chunks))  # Initialize semantic scores\n",
    "        semantic_scores[indices[0]] = 1 / (1 + distances[0])  # Calculate semantic scores\n",
    "        combined_scores = alpha * keyword_scores + (1 - alpha) * semantic_scores  # Combine scores\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]  # Get top indices\n",
    "        return [self.chunks[i] for i in top_indices]\n",
    "    \n",
    "    def normalize_scores(self, scores: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize an array of scores to a range between 0 and 1.\n",
    "        Args:\n",
    "            scores (np.ndarray): The array of scores to normalize.\n",
    "        Returns:\n",
    "            np.ndarray: The normalized scores.\"\"\"\n",
    "        # Find the minimum and maximum scores\n",
    "        min_score = np.min(scores)\n",
    "        max_score = np.max(scores)\n",
    "        # Normalize the scores to a range between 0 and 1\n",
    "        return (scores - min_score) / (max_score - min_score)\n",
    "    \n",
    "    async def save_faiss_index(self, file_path: str):\n",
    "        \"\"\"Save the FAISS index to a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): The path to the file where the index will be saved.\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            await self.create_faiss_index()\n",
    "        faiss.write_index(self.index, file_path)\n",
    "\n",
    "    async def save_bm25_index_avro(self, file_path: str):\n",
    "        \"\"\"Save the BM25 index to an Avro file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): The path to the file where the index will be saved.\n",
    "        \"\"\"\n",
    "        schema = {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"BM25Index\",\n",
    "            \"fields\": [\n",
    "                {\"name\": \"doc_freqs\", \"type\": {\"type\": \"array\", \"items\": {\"type\": \"map\", \"values\": \"int\"}}},\n",
    "                {\"name\": \"idf\", \"type\": {\"type\": \"array\", \"items\": \"double\"}},\n",
    "                {\"name\": \"doc_len\", \"type\": {\"type\": \"array\", \"items\": \"int\"}},\n",
    "                {\"name\": \"avgdl\", \"type\": \"double\"}\n",
    "            ]\n",
    "        }\n",
    "        bm25_data = {\n",
    "            'doc_freqs': self.bm25.doc_freqs,\n",
    "            'idf': self.bm25.idf,\n",
    "            'doc_len': self.bm25.doc_len,\n",
    "            'avgdl': self.bm25.avgdl\n",
    "        }\n",
    "        with open(file_path, 'wb') as f:\n",
    "            fastavro.writer(f, schema, [bm25_data])\n",
    "\n",
    "    async def load_bm25_index_avro(self, file_path: str):\n",
    "        \"\"\"Load the BM25 index from an Avro file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): The path to the file from which the index will be loaded.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            reader = fastavro.reader(f)\n",
    "            bm25_data = next(reader)\n",
    "        self.bm25 = BM25Okapi([])\n",
    "        self.bm25.doc_freqs = bm25_data['doc_freqs']\n",
    "        self.bm25.idf = bm25_data['idf']\n",
    "        self.bm25.doc_len = bm25_data['doc_len']\n",
    "        self.bm25.avgdl = bm25_data['avgdl']\n",
    "\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    async def rerank_chunks(self, query: str, chunks: List[dict], top_k: int = 5) -> List[dict]:\n",
    "        \"\"\"Rerank chunks of text using the Reranker library.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The query string.\n",
    "            chunks (List[dict]): The list of chunks to rerank.\n",
    "            top_k (int): The number of top results to return.\n",
    "        \n",
    "        Returns:\n",
    "            List[dict]: The top-k ranked chunks.\n",
    "        \"\"\"\n",
    "        # Extract texts from chunks\n",
    "        texts = [chunk['content'] for chunk in chunks]\n",
    "        # Perform reranking\n",
    "        results = await self.reranker.rank_async(query=query, docs=texts)\n",
    "        # Extract top-k results\n",
    "        top_results = results.top_k(top_k)\n",
    "        # Map results back to chunks\n",
    "        ranked_chunks = [chunks[result.document.doc_id] for result in top_results]\n",
    "        return ranked_chunks\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rewritten_prompt(rendered_text):\n",
    "    pattern = r'<output>\\n\\s*(.?)\\s\\n</output>'\n",
    "    match = re.search(pattern,  rendered_text.replace('\\n', ' '), re.DOTALL)\n",
    "    if match:\n",
    "        output_content = match.group(1).strip()\n",
    "        # Split the content by newlines and take the first non-empty line\n",
    "        lines = output_content.split('\\n')\n",
    "        return next((line.strip() for line in lines if line.strip()), '')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<output>\n",
    "foundation time series models examples python\n",
    "</output>\n",
    "\n",
    "This rewritten search query maintains the original intent of the prompt, focusing on the key terms \"foundation time series models\" and adding \"examples\" and \"python\" to specify the type of information being sought. This query is concise, clear, and suitable for a search API.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_rewritten_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOException",
     "evalue": "IO Error: Could not set lock on file \"/Users/jshah/Documents/GitHub/pravah/pravah.db\": Conflicting lock is held in /Users/jshah/micromamba/envs/hack/bin/python3.11 (PID 93777) by user jshah. See also https://duckdb.org/docs/connect/concurrency",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mduckdb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a connection to a new DuckDB database (in-memory)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mduckdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/jshah/Documents/GitHub/pravah/pravah.db\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIOException\u001b[0m: IO Error: Could not set lock on file \"/Users/jshah/Documents/GitHub/pravah/pravah.db\": Conflicting lock is held in /Users/jshah/micromamba/envs/hack/bin/python3.11 (PID 93777) by user jshah. See also https://duckdb.org/docs/connect/concurrency"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Create a connection to a new DuckDB database (in-memory)\n",
    "conn = duckdb.connect(database='/Users/jshah/Documents/GitHub/pravah/pravah.db', read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = conn.execute(\"SELECT * FROM retrieved_chunks\").fetchdf()\n",
    "# print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_uuid</th>\n",
       "      <th>search_type</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4410bd1d-af48-4d80-9d30-825ee0f915f5</td>\n",
       "      <td>keyword_search</td>\n",
       "      <td>{'content':  strong data validation and ML fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4410bd1d-af48-4d80-9d30-825ee0f915f5</td>\n",
       "      <td>keyword_search</td>\n",
       "      <td>{'content': Dagster vs Apache Airflow — side b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4410bd1d-af48-4d80-9d30-825ee0f915f5</td>\n",
       "      <td>keyword_search</td>\n",
       "      <td>{'content': atin Solanki307 Followers·Writer f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4410bd1d-af48-4d80-9d30-825ee0f915f5</td>\n",
       "      <td>keyword_search</td>\n",
       "      <td>{'content':  use cases.Error handling and vali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4410bd1d-af48-4d80-9d30-825ee0f915f5</td>\n",
       "      <td>keyword_search</td>\n",
       "      <td>{'content':  each platform.Provide sample code...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      conversation_uuid     search_type  \\\n",
       "0  4410bd1d-af48-4d80-9d30-825ee0f915f5  keyword_search   \n",
       "1  4410bd1d-af48-4d80-9d30-825ee0f915f5  keyword_search   \n",
       "2  4410bd1d-af48-4d80-9d30-825ee0f915f5  keyword_search   \n",
       "3  4410bd1d-af48-4d80-9d30-825ee0f915f5  keyword_search   \n",
       "4  4410bd1d-af48-4d80-9d30-825ee0f915f5  keyword_search   \n",
       "\n",
       "                                               chunk  \n",
       "0  {'content':  strong data validation and ML fra...  \n",
       "1  {'content': Dagster vs Apache Airflow — side b...  \n",
       "2  {'content': atin Solanki307 Followers·Writer f...  \n",
       "3  {'content':  use cases.Error handling and vali...  \n",
       "4  {'content':  each platform.Provide sample code...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the chat_history table with the retrieved_chunks table on conversation_uuid using SQL query\n",
    "merged_df = conn.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM chat_history\n",
    "    FULL OUTER JOIN retrieved_chunks\n",
    "    ON chat_history.conversation_uuid = retrieved_chunks.conversation_uuid\n",
    "\"\"\").fetchdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'alternatives to DuckDB Python backend open source tools'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'follow_up_questions'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'images'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'results'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are some alternatives to DuckDB? - StackShare'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://stackshare.io/duckdb/alternatives'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'MonetDB, JavaScript, Git, GitHub, and Python are the most popular alternatives and competitors to DuckDB. \"High Performance\" is the primary reason why developers choose MonetDB.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9928509</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'raw_content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Alternatives to DuckDB\\nWhat is DuckDB and what are its top alternatives?\\nDuckDB is a high-performance analytical database system designed to process complex SQL queries efficiently. It features vectorized query execution, automatic data compression, and support for various data types. However, DuckDB\\'s current limitations include lack of support for large datasets and limited scalability for concurrent workloads.\\nTop Alternatives to DuckDB\\nMonetDB innovates at all layers of a DBMS, e.g. a storage model based on vertical fragmentation, a modern CPU-tuned query execution architecture, automatic and self-tuning indexes, run-time query optimization, and a modular software architecture. ...\\nJavaScript is most known as the scripting language for Web pages, but used in many non-browser environments as well such as node.js or Apache CouchDB. It is a prototype-based, multi-paradigm scripting language that is dynamic,and supports object-oriented, imperative, and functional programming styles. ...\\nGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. ...\\nGitHub is the best place to share code with friends, co-workers, classmates, and complete strangers. Over three million people use GitHub to build amazing things together. ...\\nPython is a general purpose programming language created by Guido Van Rossum. Python is most praised for its elegant syntax and readable code, if you are just beginning your programming career python suits you best. ...\\njQuery is a cross-platform JavaScript library designed to simplify the client-side scripting of HTML. ...\\nNode.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. ...\\nBuild and debug modern web and cloud applications. Code is free and available on your favorite platform - Linux, Mac OSX, and Windows. ...\\nDuckDB alternatives &amp; related posts\\nMonetDB\\nrelated MonetDB posts\\nJavaScript\\nrelated JavaScript posts\\nOof. I have truly hated JavaScript for a long time. Like, for over twenty years now. Like, since the Clinton administration. It\\'s always been a nightmare to deal with all of the aspects of that silly language.\\nBut wowza, things have changed. Tooling is just way, way better. I\\'m primarily web-oriented, and using React and Apollo together the past few years really opened my eyes to building rich apps. And I deeply apologize for using the phrase rich apps; I don\\'t think I\\'ve ever said such Enterprisey words before.\\nBut yeah, things are different now. I still love Rails, and still use it for a lot of apps I build. But it\\'s that silly rich apps phrase that\\'s the problem. Users have way more comprehensive expectations than they did even five years ago, and the JS community does a good job at building tools and tech that tackle the problems of making heavy, complicated UI and frontend work.\\nObviously there\\'s a lot of things happening here, so just saying \"JavaScript isn\\'t terrible\" might encompass a huge amount of libraries and frameworks. But if you\\'re like me, yeah, give things another shot- I\\'m somehow not hating on JavaScript anymore and... gulp... I kinda love it.\\nHow Uber developed the open source, end-to-end distributed tracing Jaeger , now a CNCF project:\\nDistributed tracing is quickly becoming a must-have component in the tools that organizations use to monitor their complex, microservice-based architectures. At Uber, our open source distributed tracing system Jaeger saw large-scale internal adoption throughout 2016, integrated into hundreds of microservices and now recording thousands of traces every second.\\nHere is the story of how we got here, from investigating off-the-shelf solutions like Zipkin, to why we switched from pull to push architecture, and how distributed tracing will continue to evolve:\\nhttps://eng.uber.com/distributed-tracing/\\n(GitHub Pages : https://www.jaegertracing.io/, GitHub: https://github.com/jaegertracing/jaeger)\\nBindings/Operator: Python\\nJava\\nNode.js Go C++ Kubernetes JavaScript OpenShift C# Apache Spark\\nGit\\nrelated Git posts\\nOur whole DevOps stack consists of the following tools:\\nThe main reason we have chosen Kubernetes over Docker Swarm\\nis related to the following artifacts:\\nOften enough I have to explain my way of going about setting up a CI/CD pipeline with multiple deployment platforms. Since I am a bit tired of yapping the same every single time, I\\'ve decided to write it up and share with the world this way, and send people to read it instead ;). I will explain it on \"live-example\" of how the Rome got built, basing that current methodology exists only of readme.md and wishes of good luck (as it usually is ;)).\\nIt always starts with an app, whatever it may be and reading the readmes available while Vagrant and VirtualBox is installing and updating. Following that is the first hurdle to go over - convert all the instruction/scripts into Ansible playbook(s), and only stopping when doing a clear vagrant up or vagrant reload we will have a fully working environment. As our Vagrant environment is now functional, it\\'s time to break it! This is the moment to look for how things can be done better (too rigid/too lose versioning? Sloppy environment setup?) and replace them with the right way to do stuff, one that won\\'t bite us in the backside. This is the point, and the best opportunity, to upcycle the existing way of doing dev environment to produce a proper, production-grade product.\\nI should probably digress here for a moment and explain why. I firmly believe that the way you deploy production is the same way you should deploy develop, shy of few debugging-friendly setting. This way you avoid the discrepancy between how production work vs how development works, which almost always causes major pains in the back of the neck, and with use of proper tools should mean no more work for the developers. That\\'s why we start with Vagrant as developer boxes should be as easy as vagrant up, but the meat of our product lies in Ansible which will do meat of the work and can be applied to almost anything: AWS, bare metal, docker, LXC, in open net, behind vpn - you name it.\\nWe must also give proper consideration to monitoring and logging hoovering at this point. My generic answer here is to grab Elasticsearch, Kibana, and Logstash. While for different use cases there may be better solutions, this one is well battle-tested, performs reasonably and is very easy to scale both vertically (within some limits) and horizontally. Logstash rules are easy to write and are well supported in maintenance through Ansible, which as I\\'ve mentioned earlier, are at the very core of things, and creating triggers/reports and alerts based on Elastic and Kibana is generally a breeze, including some quite complex aggregations.\\nIf we are happy with the state of the Ansible it\\'s time to move on and put all those roles and playbooks to work. Namely, we need something to manage our CI/CD pipelines. For me, the choice is obvious: TeamCity. It\\'s modern, robust and unlike most of the light-weight alternatives, it\\'s transparent. What I mean by that is that it doesn\\'t tell you how to do things, doesn\\'t limit your ways to deploy, or test, or package for that matter. Instead, it provides a developer-friendly and rich playground for your pipelines. You can do most the same with Jenkins, but it has a quite dated look and feel to it, while also missing some key functionality that must be brought in via plugins (like quality REST API which comes built-in with TeamCity). It also comes with all the common-handy plugins like Slack or Apache Maven integration.\\nThe exact flow between CI and CD varies too greatly from one application to another to describe, so I will outline a few rules that guide me in it:\\n1. Make build steps as small as possible. This way when something breaks, we know exactly where, without needing to dig and root around.\\n2. All security credentials besides development environment must be sources from individual Vault instances. Keys to those containers should exist only on the CI/CD box and accessible by a few people (the less the better). This is pretty self-explanatory, as anything besides dev may contain sensitive data and, at times, be public-facing. Because of that appropriate security must be present. TeamCity shines in this department with excellent secrets-management.\\n3. Every part of the build chain shall consume and produce artifacts. If it creates nothing, it likely shouldn\\'t be its own build. This way if any issue shows up with any environment or version, all developer has to do it is grab appropriate artifacts to reproduce the issue locally.\\n4. Deployment builds should be directly tied to specific Git branches/tags.\\nThis enables much easier tracking of what caused an issue, including automated identifying and tagging the author (nothing like automated regression testing!).\\nSpeaking of deployments, I generally try to keep it simple but also with a close eye on the wallet. Because of that, I am more than happy with AWS or another cloud provider, but also constantly peeking at the loads and do we get the value of what we are paying for. Often enough the pattern of use is not constantly erratic, but rather has a firm baseline which could be migrated away from the cloud and into bare metal boxes. That is another part where this approach strongly triumphs over the common Docker and CircleCI setup, where you are very much tied in to use cloud providers and getting out is expensive. Here to embrace bare-metal hosting all you need is a help of some container-based self-hosting software, my personal preference is with Proxmox and LXC. Following that all you must write are ansible scripts to manage hardware of Proxmox, similar way as you do for Amazon EC2\\n(ansible supports both greatly) and you are good to go. One does not exclude another, quite the opposite, as they can live in great synergy and cut your costs dramatically (the heavier your base load, the bigger the savings) while providing production-grade resiliency.\\nGitHub\\nrelated GitHub posts\\nI was building a personal project that I needed to store items in a real time database. I am more comfortable with my Frontend skills than my backend so I didn\\'t want to spend time building out anything in Ruby or Go.\\nI stumbled on Firebase\\nby #Google, and it was really all I needed. It had realtime data, an area for storing file uploads and best of all for the amount of data I needed it was free!\\nI built out my application using tools I was familiar with, React for the framework, Redux.js to manage my state across components, and styled-components for the styling.\\nNow as this was a project I was just working on in my free time for fun I didn\\'t really want to pay for hosting. I did some research and I found Netlify. I had actually seen them at #ReactRally the year before and deployed a Gatsby\\nsite to Netlify already.\\nNetlify was very easy to setup and link to my GitHub account you select a repo and pretty much with very little configuration you have a live site that will deploy every time you push to master.\\nWith the selection of these tools I was able to build out my application, connect it to a realtime database, and deploy to a live environment all with $0 spent.\\nIf you\\'re looking to build out a small app I suggest giving these tools a go as you can get your idea out into the real world for absolutely no cost.\\nContext: I wanted to create an end to end IoT data pipeline simulation in Google Cloud IoT Core and other GCP services. I never touched Terraform meaningfully until working on this project, and it\\'s one of the best explorations in my development career. The documentation and syntax is incredibly human-readable and friendly. I\\'m used to building infrastructure through the google apis via Python , but I\\'m so glad past Sung did not make that decision. I was tempted to use Google Cloud Deployment Manager, but the templates were a bit convoluted by first impression. I\\'m glad past Sung did not make this decision either.\\nSolution: Leveraging Google Cloud Build Google Cloud Run Google Cloud Bigtable Google BigQuery Google Cloud Storage Google Compute Engine\\nalong with some other fun tools, I can deploy over 40 GCP resources using Terraform!\\nCheck Out My Architecture: CLICK ME\\nCheck out the GitHub repo attached\\nPython\\nrelated Python posts\\nHow Uber developed the open source, end-to-end distributed tracing Jaeger , now a CNCF project:\\nDistributed tracing is quickly becoming a must-have component in the tools that organizations use to monitor their complex, microservice-based architectures. At Uber, our open source distributed tracing system Jaeger saw large-scale internal adoption throughout 2016, integrated into hundreds of microservices and now recording thousands of traces every second.\\nHere is the story of how we got here, from investigating off-the-shelf solutions like Zipkin, to why we switched from pull to push architecture, and how distributed tracing will continue to evolve:\\nhttps://eng.uber.com/distributed-tracing/\\n(GitHub Pages : https://www.jaegertracing.io/, GitHub: https://github.com/jaegertracing/jaeger)\\nBindings/Operator: Python\\nJava\\nNode.js Go C++ Kubernetes JavaScript OpenShift C# Apache Spark\\nWinds 2.0 is an open source Podcast/RSS reader developed by Stream with a core goal to enable a wide range of developers to contribute.\\nWe chose JavaScript because nearly every developer knows or can, at the very least, read JavaScript. With ES6 and Node.js v10.x.x, itâ€™s become a very capable language. Async/Await is powerful and easy to use (Async/Await vs Promises). Babel allows us to experiment with next-generation JavaScript (features that are not in the official JavaScript spec yet). Yarn allows us to consistently install packages quickly (and is filled with tons of new tricks)\\nWeâ€™re using JavaScript for everything â€“ both front and backend. Most of our team is experienced with Go and Python, so Node was not an obvious choice for this app.\\nSure... there will be haters who refuse to acknowledge that there is anything remotely positive about JavaScript (there are even rants on Hacker News about Node.js); however, without writing completely in JavaScript, we would not have seen the results we did.\\n#FrameworksFullStack #Languages\\njQuery\\nrelated jQuery posts\\nThe client-side stack of Shopify Admin has been a long journey. It started with HTML templates, jQuery and Prototype. We moved to Batman.js, our in-house Single-Page-Application framework (SPA), in 2013. Then, we re-evaluated our approach and moved back to statically rendered HTML and vanilla JavaScript. As the front-end ecosystem matured, we felt that it was time to rethink our approach again. Last year, we started working on moving Shopify Admin to React and TypeScript.\\nMany things have changed since the days of jQuery and Batman. JavaScript execution is much faster. We can easily render our apps on the server to do less work on the client, and the resources and tooling for developers are substantially better with React than we ever had with Batman.\\n#FrameworksFullStack #Languages\\nI\\'m planning to create a web application and also a mobile application to provide a very good shopping experience to the end customers. Shortly, my application will be aggregate the product details from difference sources and giving a clear picture to the user that when and where to buy that product with best in Quality and cost.\\nI have planned to develop this in many milestones for adding N number of features and I have picked my first part to complete the core part (aggregate the product details from different sources).\\nAs per my work experience and knowledge, I have chosen the followings stacks to this mission.\\nUI:\\nI would like to develop this application using React, React Router and React Native since I\\'m a little bit familiar on this and also most importantly these will help on developing both web and mobile apps. In addition, I\\'m gonna use the stacks JavaScript, jQuery,\\njQuery UI, jQuery Mobile, Bootstrap wherever required.\\nService:\\nI have planned to use Java as the main business layer language as I have 7+ years of experience on this I believe I can do better work using Java than other languages. In addition, I\\'m thinking to use the stacks Node.js.\\nDatabase and ORM:\\nI\\'m gonna pick MySQL as DB and Hibernate as ORM since I have a piece of good knowledge and also work experience on this combination.\\nSearch Engine:\\nI need to deal with a large amount of product data and it\\'s in-detailed info to provide enough details to end user at the same time I need to focus on the performance area too. so I have decided to use Solr as a search engine for product search and suggestions. In addition, I\\'m thinking to replace Solr by Elasticsearch once explored/reviewed enough about Elasticsearch.\\nHost:\\nAs of now, my plan to complete the application with decent features first and deploy it in a free hosting environment like Docker and Heroku and then once it is stable then I have planned to use the AWS products Amazon S3, EC2, Amazon RDS and Amazon Route 53. I\\'m not sure about Microsoft Azure that what is the specialty in it than Heroku and Amazon EC2 Container Service. Anyhow, I will do explore these once again and pick the best suite one for my requirement once I reached this level.\\nBuild and Repositories:\\nI have decided to choose Apache Maven and Git as these are my favorites and also so popular on respectively build and repositories.\\nAdditional Utilities :)\\n- I would like to choose Codacy for code review as their Startup plan will be very helpful to this application. I\\'m already experienced with Google CheckStyle and SonarQube even I\\'m looking something on Codacy.\\nHappy Coding!\\nSuggestions are welcome! :)\\nThanks,\\nGanesa\\nNode.js\\nrelated Node.js posts\\nI just finished the very first version of my new hobby project: #MovieGeeks.\\nIt is a minimalist online movie catalog for you to save the movies you want to see and for rating the movies you already saw.\\nThis is just the beginning as I am planning to add more features on the lines of sharing and discovery\\nFor the #BackEnd I decided to use Node.js\\n, GraphQL\\nand MongoDB:\\nNode.js\\nhas a huge community so it will always be a safe choice in terms of libraries and finding solutions to problems you may have\\nGraphQL\\nbecause I needed to improve my skills with it and because I was never comfortable with the usual REST approach. I believe GraphQL is a better option as it feels more natural to write apis, it improves the development velocity, by definition it fixes the over-fetching and under-fetching problem that is so common on REST apis, and on top of that, the community is getting bigger and bigger.\\nMongoDB was my choice for the database as I already have a lot of experience working on it and because, despite of some bad reputation it has acquired in the last months, I still believe it is a powerful database for at least a very long list of use cases such as the one I needed for my website\\nWhen I joined NYT there was already broad dissatisfaction with the LAMP (Linux Apache HTTP Server MySQL PHP) Stack and the front end framework, in particular. So, I wasn\\'t passing judgment on it. I mean, LAMP\\'s fine, you can do good work in LAMP. It\\'s a little dated at this point, but it\\'s not ... I didn\\'t want to rip it out for its own sake, but everyone else was like, \"We don\\'t like this, it\\'s really inflexible.\" And I remember from being outside the company when that was called MIT FIVE when it had launched. And been observing it from the outside, and I was like, you guys took so long to do that and you did it so carefully, and yet you\\'re not happy with your decisions. Why is that? That was more the impetus. If we\\'re going to do this again, how are we going to do it in a way that we\\'re gonna get a better result?\\nSo we\\'re moving quickly away from LAMP, I would say. So, right now, the new front end is React based and using Apollo. And we\\'ve been in a long, protracted, gradual rollout of the core experiences.\\nReact is now talking to GraphQL as a primary API. There\\'s a Node.js back end, to the front end, which is mainly for server-side rendering, as well.\\nBehind there, the main repository for the GraphQL server is a big table repository, that we call Bodega because it\\'s a convenience store. And that reads off of a Kafka pipeline.\\nVisual Studio Code\\nrelated Visual Studio Code posts\\nI am starting to become a full-stack developer, by choosing and learning\\n.NET Core for API Development, Angular CLI / React for UI Development, MongoDB for database, as it a NoSQL DB and Flutter / React Native for Mobile App Development.\\nUsing Postman, Markdown and Visual Studio Code for development.\\nOur first experience with .NET\\ncore was when we developed our OSS feature management platform - Tweek (https://github.com/soluto/tweek).\\nWe wanted to create a solution that is able to run anywhere (super important for OSS), has excellent performance characteristics and can fit in a multi-container architecture.\\nWe decided to implement our rule engine processor in F# , our main service was implemented in C#\\nand other components were built using JavaScript / TypeScript and Go.\\nVisual Studio Code\\nworked really well for us as well, it worked well with all our polyglot services and the .Net core integration had great cross-platform developer experience (to be fair, F# was a bit trickier) - actually, each of our team members used a different OS (Ubuntu, macos, windows).\\nOur production deployment ran for a time on Docker Swarm until we\\'ve decided to adopt Kubernetes with almost seamless migration process.\\nAfter our positive experience of running .Net core workloads in containers and developing Tweek\\'s .Net services on non-windows machines, C# had gained back some of its popularity (originally lost to Node.js), and other teams have been using it for developing microservices, k8s sidecars (like https://github.com/Soluto/airbag), cli tools, serverless functions and other projects...\\nSimilar Tools\\nNew Tools\\nTop Tools\\nTrending Comparisons\\nTools'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ibis'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://ibis-project.org/'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'An open source dataframe library that works with any data system. Use the same API for 20+ backends. Fast local dataframes with embedded DuckDB (default), Polars, or DataFusion. Iterate locally and deploy remotely by changing a single line of code. Compose SQL and Python dataframe code, bridging the gap between data engineering and data science.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8008478</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'raw_content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ibis\\nAn open source dataframe library that works with any data system\\nIbis: the portable Python dataframe library\\nIbis offers a familiar local dataframe experience with outstanding performance, using DuckDB by default.\\nIterate and explore data locally:\\nOne API for 20+ backends\\nUse the same dataframe API for 20+ backends:\\nFor example:\\nThis allows you to iterate locally and deploy remotely by changing a single line of code. For instance, develop locally with DuckDB and deploy remotely to BigQuery. Or, using any combination of backends that meet your requirements.\\nPython + SQL: better together\\nIbis works by decoupling the dataframe API from the backend execution. Most backends support a SQL dialect, which Ibis compiles its expressions into using SQLGlot. You can inspect the SQL that Ibis generates for any SQL backend:\\nAnd use SQL strings directly, mixing and matching with Python dataframe code:\\nThis allows you to combine the flexibility of Python with the scale and performance of modern SQL.\\nUsers say…\\n“Ibis is amazing, there is so much bikeshedding out there that this library improves upon. I love that now we can empower any visualization with nearly any dataset! Big thanks to those who have contributed!”\\nNick Shook\\n“I now have Ibis code that runs PySpark in my Databricks environment and Polars on my laptop which is pretty slick 🔥”\\nMark Druffel\\n“I love that with Ibis, I can use SQL for the heavy lifting or aggregations and then switch to a dataframe-like API for the type of dynamic transformations that would otherwise be tedious to do in pure SQL.”\\nDaniel Kim\\nGet started with Ibis\\nWhy Ibis?\\nTutorial: getting started\\nAPI reference'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'DuckDB: In-Process Python Analytics for Not-Quite-Big Data'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://thenewstack.io/duckdb-in-process-python-analytics-for-not-quite-big-data/'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'An in-process analytics database, DuckDB can work with surprisingly large data sets without having to maintain a distributed multiserver system. Best of all? You can analyze data directly from your Python app.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7255514</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'raw_content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'We’re so glad you’re here. You can expect all the best TNS content to arrive\\nMonday through Friday to keep you on top of the news and at the top of your game.\\nCheck your inbox for a confirmation email where you can adjust your preferences\\nand even join additional groups.\\nFollow TNS on your favorite social media networks.\\nBecome a TNS follower on LinkedIn.\\nCheck out the latest featured and trending stories while you wait for your\\nfirst TNS newsletter.\\nDuckDB: In-Process Python Analytics for Not-Quite-Big Data\\nPITTSBURGH — You don’t always need a cluster to analyze even a very large data set. There is a lot you can pack into a single server running the open source DuckDB in-process analytical database system.\\nThis was one takeaway from a number of presentations comparing the performance of analytics solutions that were given at PyCon, a Python programmer’s conference held last week in Pittsburgh. There, they compared systems and asked, for instance, if a Dask system was faster at analytics than Apache Spark.\\nBut if you can avoid setting up a distributed system altogether, you can avoid a lot of headaches around upkeep.\\nAs explained in a presentation given by Kevin Kho and Han Wang, you can get a lot of mileage from a single machine, if it is optimized correctly. And this is the mission of DuckDB.\\nIn 2021, H20.ai tested DuckDB in a set of benchmarks comparing the processing speed for various database-like tools popular in open source data science.\\nThe testers ran five queries across 10 million rows and nine columns (about 0.5GB). Duck completed the task in a mere two seconds. That was surprising for a database running on a single computer. Even more surprising, it chewed through 100 million rows (5GB) in 14 seconds.\\nThese numbers were impressive, and in 2023, the DuckDB folks went back and tweaked the configuration settings and upgraded the hardware and got the 5GB workload down to two seconds and the 0.5GB in less than a second.\\nIt even tackled the 50GB workload — normally reserved for distributed systems such as Spark — in 24 seconds.\\n“This is a mind-blowing number. The improvements are amazing,” said Wang, who is the tech lead of Lyft Machine Learning Platform, in the presentation.\\nDuckDB’s benchmark of Big Data systems, 2003.\\nThe takeaway? A surprising number of self-styled “big data”-styled projects don’t need Spark or some other distributed solution: They can fit nicely onto a single server, Wang noted. Taking this approach eliminates the considerable overhead of managing a distributed system, and keeps all the data and code on the local machine.\\nIntroducing DuckDB\\nThere’s a lot happening with DuckDB, an analytical, relational in-process SQL database system created in 2018. Two things that immediately set it apart from the other data platforms.\\n1: It combines SQL with Python, giving developers/analysts an expressive query language that executes against data in the application process itself.\\n2: It is meant to run only on a single machine. This is a feature, not a bug, as it eliminates all the complexity of running a data platform on a distributed platform.\\n“As soon as a problem gets a little bit too big for Pandas, you have to throw a giant distributed system at it. It’s like cracking a nut with a sledgehammer. It’s not ergonomic,” said\\xa0said Alex Monahan, in another Pycon presentation. Monham is a forward-deployed software engineer for MotherDuck, which offers a serverless analytics service based on Duck.\\nThe two creators of DuckDB — Hannes Mühleisen (CEO) and Mark Raasveldt (CTO) — have founded DuckDB Labs, which provides commercial support for the database system, which was designed to offer a fast, easy-to-deploy mid-sized data analysis.\\nThey took considerable inspiration from the little database that could, considering DuckDB to be the SQLite of columns, rather than rows.\\nWith a Python-esque interface, Duck was also built specifically for the data science community. Data will be analyzed, modeled, and visualized. Data scientists tend not to use databases, instead relying on CSV files and other un- or semi-structured data sources. Duck allows them to embed data operations directly into their code itself.\\nThe MIT-licensed open source software is written in C++, so it is fast.\\nDuckDB is made to go fast, taking advantage of all the server’s cores and cache hierarchies. And whereas SQLite is a row-based database engine that processes one row at a time, Duck can process a whole vector, of 2,048 rows, at one time.\\nIt is a single binary install from the Python Installer It is available for multiple platforms, all pre-compiled so they can be downloaded and run through a command line, or through the client libraries. There’s even a version that runs in a browser via WebAssembly.\\nIt is an in-process application, and writes to disk, meaning it is not limited a server’s RAM, it can use the whole hard drive, opening the path to working with data sizes that are terabytes in size. Unlike a client-server database, it does not rely on a third-party transport mechanism to ship the data from the server to the client. Instead, just like SQLite, the application can pull the data as part of a Python call, in an in-process communication within the same memory space.\\n“You read it right where it sits,” Monahan said.\\nYou can write data frames natively to the database in a number of different ways, including user-defined functions, a full relational API, the Ibis library to simultaneously write data frames simultaneously across multiple back end data sources, and PySpark but with a different import statement.\\nHow DuckDB and Python Work Together\\nIn addition to the command line, it comes with clients for 15 languages. Python is the most popular, but there is also Node, JBDC, and OBDC. It can read CSV, JSON files, Apache Iceberg files. DuckDB can natively read Pandas, Polaris and Arrow files, without copying the data into another format. Unlike most SQL-only database systems, it keeps the original of the data as it is ingested.\\n“So this could fit into a lot of workflows,” Monahan said.\\nIt can also read files over the Internet, including those from GitHub (via FTP), Amazon S3, Azure Blob storage and Google Cloud Storage. It can output TensorFlow and Pytorch Tensors.\\nDuckDB uses a SQL variant that is very Python-esque, one that can ingest data frames natively.\\nMonahan produced a sample “Hello World” app to illustrate:\\nwill produce the output:\\nThe database uses PostgreSQL as the base, though some modifications were made to the SQL, both for simplifying the language and for extending its capabilities.\\nThe ways DuckDB extends and simplifies SQL (Alex Monahan presentation at Pycon)\\nIs Big Data Dead?\\nIn summary, DuckDB is a fast database with a revolutionary intent, that of making single computer analytics possible for even very large datasets. It questions the need for Big Data-based solutions.\\nIn a widely-circulated 2023 MotherDuck blog post, provocatively entitled “Big Data Is Dead,” Jordan Tigani noted that “most applications do not need to process massive amounts of data.”\\n“The amount of data processed for analytics workloads is almost certainly smaller than you think,” he wrote. So it makes sense to look at a simple single computer-based analytics software before jumping into a more expensive data warehouse or distributed analytics system.\\nCommunity created roadmaps, articles, resources and journeys for\\ndevelopers to help you choose your path and grow in your career.'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'DuckDB vs. The Titans: Spark, Elasticsearch, MongoDB - Medium'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://medium.com/walmartglobaltech/duckdb-vs-the-titans-spark-elasticsearch-mongodb-a-comparative-study-in-performance-and-cost-5366b27d5aaa'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'DuckDB as an Engine is the concept of utilizing DuckDB as a primary tool for database management and processing in an application or system.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5539276</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'raw_content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Sign up\\nSign in\\nSign up\\nSign in\\nDuckDB vs. The Titans: Spark, Elasticsearch, MongoDB — A Comparative Study in Performance and Cost\\nJiazhen Zhu\\nFollow\\nWalmart Global Tech Blog\\n--\\n6\\nListen\\nShare\\nA mini-guide about Duck as an Engine\\nDuckDB is a rising star in the realm of database management systems (DBMS), gaining prominence for its efficient columnar storage and execution design that is optimized for analytical queries. But how does it stack up against industry veterans like Apache Spark, Elasticsearch, and MongoDB? In this article, we will dive into a comparative study, examining their performance and cost implications.\\nAgenda\\nWhat is DuckDB?\\nDuckDB is an in-process SQL OLAP database management system\\nIts core strengths lie in three essential characteristics — analytical processing, in-memory operations, and a column-oriented structure, making it akin to what SQLite is for PostgreSQL, DuckDB is for Redshift, Snowflake, and others.\\nHowever, DuckDB is not without its limitations. Its scalability is relatively limited, and it is not optimized for heavy data writing workloads. Its application in production environments is also restricted due to its newness in the market. Moreover, as it is an in-memory system, DuckDB’s performance may degrade or become infeasible for datasets larger than the available memory.\\nThe Titans: Apache Spark, Elasticsearch, and MongoDB\\nThese three tools have now become industry mainstays, with numerous companies employing them to accelerate their data processing and searching capabilities.\\nApache Spark is an open-source distributed computing system known for its speed, versatility, and sophisticated analytics.\\nElasticsearch is a robust search and analytics engine.\\nMongoDB is a source-available, NoSQL database program offering a flexible, JSON-like document model.\\nPerformance Comparison\\nTo gain a deeper understanding of DuckDB, we’ll conduct a performance analysis involving three types of data processing tasks.\\nFor Apache Spark, our test setup involves a ‘n1-standard-16’ master with ten ‘n1-standard-32’ workers. For DuckDB, we’ll utilize a ‘n2d-highmem-16’.\\nOur testing scenario involves two tables:\\nThe three operations we’ll analyze are as follows:\\nYou can find all the detailed information in the table below.\\nPost completion of our tests, we’ve gathered performance metrics for Apache Spark, MongoDB, BigQuery (BQ), Elasticsearch, DuckDB with external tables (DuckDB ext.), and DuckDB with internal tables (DuckDB int.) across all three types of data processing. The details of these metrics can be found in Table 2.\\nIn summary, DuckDB outperforms the other database systems in these three operations, especially in its internal configuration. This might be due to its in-memory and columnar architecture, making it a strong candidate for aggregation, join, and search operations tasks.\\nCost Analysis\\nBeyond performance, cost-effectiveness is a crucial factor that warrants consideration.\\nThe cost for running Apache Spark, MongoDB, Elasticsearch, and DuckDB, predicated on the selected Virtual Machine (VM), can be viewed in Table 3. A more detailed examination in Table 4 reveals that DuckDB yields substantial cost savings: around 90% compared to Spark, approximately 63% to MongoDB, and about 81% to Elasticsearch.\\nIn conclusion, while each of these DBMS has unique strengths in terms of capabilities and performance, DuckDB emerges as the most cost-effective option, according to this table.\\nCase Studies and Limitations\\nDuckDB as an Engine is the concept of utilizing DuckDB as a primary tool for database management and processing in an application or system.\\nFigure 1 illustrates an end-to-end workflow leveraging DuckDB as an Engine. We can utilize DuckDB as an Engine (DaaE) in three specific areas: conventional ETL transformation, the speed layer for search and aggregation, and backend service-side API processing. We will delve into each of these areas in the subsequent sections.\\nConventional ETL Transformation:\\nIn the realm of ETL processes, tools like Spark or BigQuery are commonly employed for data handling. However, they can encounter certain constraints. For instance, Spark requires careful tuning to deliver effective data processing, while BigQuery can be costly when dealing with high-volume and complex queries.\\nDuckDB as an Engine (DaaE) offers notable solutions for performance tuning and cost-saving. As seen in Table 5, DaaE can significantly enhance performance — in most cases offering over a 90% increase — without necessitating any specific tuning. As highlighted in Table 4, DaaE also excels in facilitating cost savings.\\nThe speed layer for search and aggregation:\\nUsers anticipate swift responses to search requests or intricate SQL queries as data reaches the final speed layer. The table below offers a comparative analysis between DuckDB with external tables (DuckDB ext.) and MongoDB, Elasticsearch (ES), as well as DuckDB with internal tables (DuckDB int.) and Elasticsearch (ES).\\nBackend service-side API processing:\\nIn data-intensive applications that offer user-facing analytics, developers often utilize backend services for data transformations, such as formatting or computations. If developers are primarily performing SQL-style processing, incorporating DuckDB as an Engine (DaaE) can significantly enhance the performance of these transformations.\\nPotential Constraints of DuckDB as an Engine (DaaE):\\nDuck as an Engine with DataBathing\\nI plan to integrate DuckDB as an Engine (DaaE) into the DataBathing framework. This way, users can leverage the advantages of both Spark and DaaE.\\nIf you’re unfamiliar with DataBathing, additional information can be found in the blogs listed below:\\nDataBathing — A Framework for Transferring the Query to Spark Code\\nA mini-guide about Query-Config Driven Coding\\nmedium.com\\nModularization Using Auto-Generated Pipeline With DataBathing\\nA mini-guide about MicroService design for Data Pipeline: Part III\\nmedium.com\\nSummary\\nDuckDB as an Engine (DaaE) is a strong choice for certain scenarios. However, when selecting a database management system (DBMS) for your specific needs, balancing cost, performance requirements, and specific needs is critical. The optimal system will hinge on factors such as the tasks' nature, data volume, and budget limitations.\\n--\\n--\\n6\\nWritten by Jiazhen Zhu\\nWalmart Global Tech Blog\\nData @ Walmart\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\"</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Pandas 2.0 and its Ecosystem (Arrow, Polars, DuckDB) - Airbyte'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://airbyte.com/blog/pandas-2-0-ecosystem-arrow-polars-duckdb'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Pandas recently got an update, which is version 2.0. This article takes a closer look at what Pandas is, its success, and what the new version brings, including its ecosystem around Arrow, Polars, and DuckDB. Pandas has established itself as the standard tool for in-memory data processing in Python, and it offers an extensive range of data ...'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4698243</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'raw_content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Fully-managed, get started in minutes\\nSecure data movement for your entire org\\nUsed by 40k+\\xa0companies\\nEmbed 100s integrations at once in your app\\nReliable database and API replication at any scale\\nEmbeddings from unstructured data\\nBuild a new connector \\xa0in 10 min\\nThe power of Airbyte to every Python developer\\nMake sense of unstructured data with LLMs\\nHigh-volume DBs with low latency\\nMarketing, sales, product, finance, eng &amp; more\\nEasily collect credentials from your end-users\\nLearn from other members’ success\\nChoose the right solutions for you\\nEvaluate your costs in both scenarios\\nOur guides to help you in your journey\\nBecome a technology or consulting partner\\nOur next community call (WED JUL 31)\\nHow to use and contribute to Airbyte\\nData engineering thought leadership\\nImprove your data replication game\\nDeploy your use case in minutes\\nLearn how to use Airbyte with Airflow, dbt, etc.\\nPlace for all data knowledge\\nGet a glimpse in the future\\nStay up to date. 30k+ subscribers\\nAccess our knowledge base\\nJoin our 20k+ data \\xa0community\\nLeave your mark in the OSS community\\nLive events by the Airbyte team\\nCommunity's knowledge base\\nGitHub Discussions for questions, ideas\\n20,000+ \\xa0share tips and get support\\nLearn more about Airbyte and data engineering\\nPandas 2.0 and its Ecosystem (Arrow, Polars, DuckDB)\\nData manipulation and analysis can be challenging and involve working with large datasets. Thankfully, a widely used Python library known as Pandas has become the go-to tool for processing and manipulating data. Pandas recently got an update, which is version 2.0. This article takes a closer look at what Pandas is, its success, and what the new version brings, including its ecosystem around Arrow, Polars, and DuckDB.\\nPandas has established itself as the standard tool for in-memory data processing in Python, and it offers an extensive range of data manipulation capabilities. As such, it is unsurprising that data engineers or those just starting with data have come across it at some point in their work.\\nIn the recent update, Pandas 2.0 has adopted Apache Arrow as a backend. This article explores why this is the most significant change in the update and how Arrow has become a fundamental part of many recently released frameworks and hot topics in data engineering.\\nThat makes it an excellent time to reflect on what Pandas is and why it’s successful.\\nWhat is Pandas\\nBut let’s start with what Pandas is? Pandas is an library to process data in memory with a feature-rich Python API. Compared to many tools, Pandas is the python library to work with smaller data and sets the standard for wrangling data in memory.\\nPandas is generally suited for:\\nSome key features you might have used it for:\\nHow Does Pandas Work?\\nTo understand what is new and better with the latest version, let’s briefly discuss how Pandas works.\\nBefore doing anything with Pandas, the general idea is to load data in-memory into a Pandas DataFrame, usually using functions like read_csv, read_sql, read_parquet, etc. When loading data into memory, it must decide how it will be stored in memory.\\nFor simple data like integers of floats, this is standard and straightforward. But some decisions must be made for other types, such as strings, dates and times, and categories.\\nPython is versatile and can represent mostly anything, but Python data structures (lists, dictionaries, tuples, etc.) are very slow and can’t be used. So the data representation is not Python and is not standard, and implementation needs to happen via Python extensions, usually implemented in C (also in C++, Rust, and others).\\nFor many years, the main extension to represent arrays and perform operations on them quickly has been NumPy. Which is also what Pandas was initially built on.\\nWhile NumPy has been good enough to make pandas the popular library it is, according to Marc Garcia (pandas core developer), it was never built as a backend for DataFrame libraries, and it has some critical limitations.\\nWhat Are the Highlights of Version 2.0\\nThat brings us to version 2.0. Let’s look at the key improvements: what are the highlights, and why do we have them?\\nAll of these are achieved through using Apache Arrow as the backend. By moving from NumPy (C++) to Apache Arrow as a backend (especially In Pandas 1.5 and 2.0 added Arrow support for all data types), Arrow gives it better storage and speed. A significant milestone was implementing a string data type based on Arrow that started in 2020.\\nBefore we dive into a code example and why Arrow helped across the board, here is a short comparison of how the speed improved:\\nWhat Changes Code-Wise?\\nThe change of the Arrow type results in an extension of the type pyarrow. The pyarrow-addition was chosen to avoid breaking changes to existing code.\\nArrow also defines a type to encode categories:\\nWhat is Apache Arrow?\\nApache Arrow sets the open standard to exchange in a heterogeneous data pipeline, which needs to read and share data among different steps.\\nInstead of the default, to persist copies along the way and lose costly time and resources to write and read that data, Apache Arrow allows sharing data in an in-memory representation without persisting.\\nWhy Apache Arrow?\\nApache Arrow solves most discussed problems, such as improving speed, interoperability, and data types, especially for strings. For example, the new string[pyarrow] column type is around 3.5 times more efficient.\\nThese efficiency gains also greatly influence how much data you can load with the same amount of RAM, which is essential when most Pandas DataFrames run on single laptops.\\nOne of the biggest problems of Pandas was data interchange, particularly moving large tabular datasets from one process’s memory space to another’s.\\nThe significant achievement here is zero-copy data access, mapping complex tables to memory to make accessing one terabyte of data on disk as fast and easy as one megabyte.\\nAnother big one is above standardization saves. Apache Arrow has a vast ecosystem, and you can share it among all other libraries that integrate with Arrow. Moreover, they can implement custom connectors for new libraries and systems such as Polars. On top of these savings, a standardized memory format facilitates the reuse of libraries of algorithms, even across languages.\\nInteroperability\\nArrow is a program-independent format. Similar to Data Lake File Formats but less obvious, as it does not have a file extension attached and only exists in memory.\\nIts interoperability makes it relatively easy to share the data among different programs and is speedy and memory-efficient because two programs can share the same data, in this case, the same memory, without copying each program. Which is the dream of every data engineer, right?\\nWhen you have a data pipeline that loads some data from your data lake, transforms it, computes some analytics, and then exports and automatically creates a Data Asset that can be used for analytics, you usually have different tools, abstractions, or even teams in more prominent companies. If you do not need to persist the data set at each step, that’s a tremendous boost in performance and cost, as you do not need to store so much data immediately.\\nWhen Not to Use Pandas\\nSo when is Pandas not the right choice?\\nPeople who are used to SQL usually prefer something other than the API of Pandas. Too bloat and too many “gotchas” is what people say.\\nThe main problem is that you know how to do it in SQL, but you need to fiddle that SQL into a strange syntax you must look up most of the time.\\nBut with the latest 2.0 release, many of these issues are handled, and with the integration of Arrow, future-proof and the cost of switching to another tool for a dedicated process is no problem anymore.\\nThe Alternatives\\nThe data ecosystem is growing every day, giving us plenty of alternatives to choose from. Let’s look at these alternatives and try to understand why they were created and when they should be used.\\nPolars: Riding the Fast Train of Rust\\nThe one-to-one replacement that provides a rich Python API would be Polars, implemented in Rust.\\nSome say Polars has a less confusing API and better ergonomics, especially from SQL. Polars is more performant out of the box but less stable and mature. It is growing the fastest of all mentioned in this chapter.\\nPolars has superpowers as it comes with a query optimizer that can make the pipeline run faster by analyzing all operations together before executing them.\\nAn example of how that could look “switching” from one to another format:\\nDaniel Beach says that Polars is the more accessible version of Spark and easier to understand than Pandas.\\nDuckDB: The SQL Version\\nDuckDB would be for SQL lovers. Sure, it’s a database format, but in case you didn’t know, DuckDB is also super strong as a zero-copy layer approach. For example, you can use DuckDB as a thin SQL wrapper on top of your S3 files across your Data Lake. DuckDB will do a great job of providing you with fast analytical queries.\\nDuckDB can efficiently run SQL queries directly on Pandas DataFrames. If you want to use SQL and have a fast interface, use DuckDB.\\nDuckDB can also query Arrow datasets directly and stream query results back to Arrow. This streaming allows users to query Arrow data using DuckDB’s SQL Interface and API while taking advantage of DuckDB’s parallel vectorized execution engine without requiring extra data copying. Additionally, this integration takes full advantage of Arrow’s predicate and filter pushdown while scanning datasets.\\nMore (how it works, benchmarks, example projects) about DuckDB on the Data Glossary.\\nWhat about Dask?\\nMany people may think of Dask as helping with Pandas performance and scalability. It optimizes speed by parallelizing large datasets into pieces and working with them in separate threads or processes or rescuing Pandas from the RAM limit.\\nOne problem with the Dask is that it uses Pandas as a black box. dask.dataframe does not solve Pandas inherent performance and memory use issues. Still, it spreads them out across multiple processes. It helps mitigate them by being careful not to work simultaneously with too large pieces of data, which can result in an unpleasant MemoryError.\\nThey make Dask jobs slower than possible with a more efficient in-memory runtime.\\nOthers: Koalas, Vaex, VertiPaq\\nFor completion, here are some pointers to other valid alternatives without explaining much more about them.\\nKoalas is another one, built on top of Spark, re-implemented 70%+ of the Pandas API by Databricks, and now officially included in PySpark since Apache Spark 3.2. Interesting to see that Spark has a similar growth rate of GitHub stars as Pandas, especially as Spark DataFrames are very similar to Pandas, supporting a rich Python API and SQL. But instead of stored locally, it’s persisted inside the Spark cluster.\\nVaex is a high-performance Python library for lazy out-of-core DataFrames (similar to Pandas) to visualize and explore big tabular datasets.\\nVertiPaq is the closed-source version from Microsoft. An engine is an in-memory columnar database behind Excel Power Pivot, SQL Server Analysis Services (SSAS) Tabular, and Power BI. When you load data into a data model, it is loaded, compressed, and stored in RAM using the VertiPaq engine.\\nData Lake Table Formats\\nAnother way to solve this problem is using a Data Lake Table Format. Hence, you only read a fraction of the data to the pandas DataFrame, similar to what DuckDB is doing, but these formats are for large-scale and distributed files.\\nThese have other advantages, such as predicate filtering and Z-ORDERING, which is more straightforward than adding a new tech like Polars, DuckDB, or others to the stack.\\nConclusion\\nWe have seen a brief on what Pandas is and how it works, highlighting its key features and capabilities. It has also touched on the limitations of its initial backend, NumPy, and how the move to Arrow in Pandas 2.0 addresses these limitations. Overall, this article provides insights into the benefits of using Pandas, particularly with its 2.0 version, and the exciting changes in its ecosystem around Arrow, Polars, and DuckDB.\\nLuckily whatever tool you use, once Pandas 2.0 is used, if you use Arrows types, converting between Pandas, Polars, and others will be almost immediate, with little metadata overhead.\\n\\u200d\\n&lt;hr&gt;\\n\\u200d\\nIf you liked this content, subscribe to our Newsletter, or sign up for Slack to join 10k+ data engineers in our community.\\nAbout the Author\\nSimon is a Data Engineer and Technical Author at Airbyte. He is dedicated, empathetic, and entrepreneurial with 15+ years of experience in the data ecosystem. He enjoys maintaining awareness of new innovative and emerging open-source technologies.\\nAbout the Author\\nTable of contents\\nJoin our newsletter to get all the insights on the data stack\\nRelated posts\\nFollow the largest open-source data movement repo\\nHi there! Keep yourself up to date with our progress, our events, and the latest news on data &amp; AI engineering.\"</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'response_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.22</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'alternatives to DuckDB Python backend open source tools'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'follow_up_questions'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'answer'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'images'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'results'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'What are some alternatives to DuckDB? - StackShare'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://stackshare.io/duckdb/alternatives'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'MonetDB, JavaScript, Git, GitHub, and Python are the most popular alternatives and competitors to DuckDB. \"High Performance\" is the primary reason why developers choose MonetDB.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m0.9928509\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'raw_content'\u001b[0m: \u001b[32m'Alternatives to DuckDB\\nWhat is DuckDB and what are its top alternatives?\\nDuckDB is a high-performance analytical database system designed to process complex SQL queries efficiently. It features vectorized query execution, automatic data compression, and support for various data types. However, DuckDB\\'s current limitations include lack of support for large datasets and limited scalability for concurrent workloads.\\nTop Alternatives to DuckDB\\nMonetDB innovates at all layers of a DBMS, e.g. a storage model based on vertical fragmentation, a modern CPU-tuned query execution architecture, automatic and self-tuning indexes, run-time query optimization, and a modular software architecture. ...\\nJavaScript is most known as the scripting language for Web pages, but used in many non-browser environments as well such as node.js or Apache CouchDB. It is a prototype-based, multi-paradigm scripting language that is dynamic,and supports object-oriented, imperative, and functional programming styles. ...\\nGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. ...\\nGitHub is the best place to share code with friends, co-workers, classmates, and complete strangers. Over three million people use GitHub to build amazing things together. ...\\nPython is a general purpose programming language created by Guido Van Rossum. Python is most praised for its elegant syntax and readable code, if you are just beginning your programming career python suits you best. ...\\njQuery is a cross-platform JavaScript library designed to simplify the client-side scripting of HTML. ...\\nNode.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. ...\\nBuild and debug modern web and cloud applications. Code is free and available on your favorite platform - Linux, Mac OSX, and Windows. ...\\nDuckDB alternatives & related posts\\nMonetDB\\nrelated MonetDB posts\\nJavaScript\\nrelated JavaScript posts\\nOof. I have truly hated JavaScript for a long time. Like, for over twenty years now. Like, since the Clinton administration. It\\'s always been a nightmare to deal with all of the aspects of that silly language.\\nBut wowza, things have changed. Tooling is just way, way better. I\\'m primarily web-oriented, and using React and Apollo together the past few years really opened my eyes to building rich apps. And I deeply apologize for using the phrase rich apps; I don\\'t think I\\'ve ever said such Enterprisey words before.\\nBut yeah, things are different now. I still love Rails, and still use it for a lot of apps I build. But it\\'s that silly rich apps phrase that\\'s the problem. Users have way more comprehensive expectations than they did even five years ago, and the JS community does a good job at building tools and tech that tackle the problems of making heavy, complicated UI and frontend work.\\nObviously there\\'s a lot of things happening here, so just saying \"JavaScript isn\\'t terrible\" might encompass a huge amount of libraries and frameworks. But if you\\'re like me, yeah, give things another shot- I\\'m somehow not hating on JavaScript anymore and... gulp... I kinda love it.\\nHow Uber developed the open source, end-to-end distributed tracing Jaeger , now a CNCF project:\\nDistributed tracing is quickly becoming a must-have component in the tools that organizations use to monitor their complex, microservice-based architectures. At Uber, our open source distributed tracing system Jaeger saw large-scale internal adoption throughout 2016, integrated into hundreds of microservices and now recording thousands of traces every second.\\nHere is the story of how we got here, from investigating off-the-shelf solutions like Zipkin, to why we switched from pull to push architecture, and how distributed tracing will continue to evolve:\\nhttps://eng.uber.com/distributed-tracing/\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mGitHub Pages : https://www.jaegertracing.io/, GitHub: https://github.com/jaegertracing/jaeger\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nBindings/Operator: Python\\nJava\\nNode.js Go C++ Kubernetes JavaScript OpenShift C# Apache Spark\\nGit\\nrelated Git posts\\nOur whole DevOps stack consists of the following tools:\\nThe main reason we have chosen Kubernetes over Docker Swarm\\nis related to the following artifacts:\\nOften enough I have to explain my way of going about setting up a CI/CD pipeline with multiple deployment platforms. Since I am a bit tired of yapping the same every single time, I\\'ve decided to write it up and share with the world this way, and send people to read it instead ;\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. I will explain it on \"live-example\" of how the Rome got built, basing that current methodology exists only of readme.md and wishes of good luck \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas it usually is ;\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nIt always starts with an app, whatever it may be and reading the readmes available while Vagrant and VirtualBox is installing and updating. Following that is the first hurdle to go over - convert all the instruction/scripts into Ansible playbook\u001b[0m\u001b[32m(\u001b[0m\u001b[32ms\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and only stopping when doing a clear vagrant up or vagrant reload we will have a fully working environment. As our Vagrant environment is now functional, it\\'s time to break it! This is the moment to look for how things can be done better \u001b[0m\u001b[32m(\u001b[0m\u001b[32mtoo rigid/too lose versioning? Sloppy environment setup?\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and replace them with the right way to do stuff, one that won\\'t bite us in the backside. This is the point, and the best opportunity, to upcycle the existing way of doing dev environment to produce a proper, production-grade product.\\nI should probably digress here for a moment and explain why. I firmly believe that the way you deploy production is the same way you should deploy develop, shy of few debugging-friendly setting. This way you avoid the discrepancy between how production work vs how development works, which almost always causes major pains in the back of the neck, and with use of proper tools should mean no more work for the developers. That\\'s why we start with Vagrant as developer boxes should be as easy as vagrant up, but the meat of our product lies in Ansible which will do meat of the work and can be applied to almost anything: AWS, bare metal, docker, LXC, in open net, behind vpn - you name it.\\nWe must also give proper consideration to monitoring and logging hoovering at this point. My generic answer here is to grab Elasticsearch, Kibana, and Logstash. While for different use cases there may be better solutions, this one is well battle-tested, performs reasonably and is very easy to scale both vertically \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithin some limits\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and horizontally. Logstash rules are easy to write and are well supported in maintenance through Ansible, which as I\\'ve mentioned earlier, are at the very core of things, and creating triggers/reports and alerts based on Elastic and Kibana is generally a breeze, including some quite complex aggregations.\\nIf we are happy with the state of the Ansible it\\'s time to move on and put all those roles and playbooks to work. Namely, we need something to manage our CI/CD pipelines. For me, the choice is obvious: TeamCity. It\\'s modern, robust and unlike most of the light-weight alternatives, it\\'s transparent. What I mean by that is that it doesn\\'t tell you how to do things, doesn\\'t limit your ways to deploy, or test, or package for that matter. Instead, it provides a developer-friendly and rich playground for your pipelines. You can do most the same with Jenkins, but it has a quite dated look and feel to it, while also missing some key functionality that must be brought in via plugins \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike quality REST API which comes built-in with TeamCity\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. It also comes with all the common-handy plugins like Slack or Apache Maven integration.\\nThe exact flow between CI and CD varies too greatly from one application to another to describe, so I will outline a few rules that guide me in it:\\n1. Make build steps as small as possible. This way when something breaks, we know exactly where, without needing to dig and root around.\\n2. All security credentials besides development environment must be sources from individual Vault instances. Keys to those containers should exist only on the CI/CD box and accessible by a few people \u001b[0m\u001b[32m(\u001b[0m\u001b[32mthe less the better\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This is pretty self-explanatory, as anything besides dev may contain sensitive data and, at times, be public-facing. Because of that appropriate security must be present. TeamCity shines in this department with excellent secrets-management.\\n3. Every part of the build chain shall consume and produce artifacts. If it creates nothing, it likely shouldn\\'t be its own build. This way if any issue shows up with any environment or version, all developer has to do it is grab appropriate artifacts to reproduce the issue locally.\\n4. Deployment builds should be directly tied to specific Git branches/tags.\\nThis enables much easier tracking of what caused an issue, including automated identifying and tagging the author \u001b[0m\u001b[32m(\u001b[0m\u001b[32mnothing like automated regression testing!\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nSpeaking of deployments, I generally try to keep it simple but also with a close eye on the wallet. Because of that, I am more than happy with AWS or another cloud provider, but also constantly peeking at the loads and do we get the value of what we are paying for. Often enough the pattern of use is not constantly erratic, but rather has a firm baseline which could be migrated away from the cloud and into bare metal boxes. That is another part where this approach strongly triumphs over the common Docker and CircleCI setup, where you are very much tied in to use cloud providers and getting out is expensive. Here to embrace bare-metal hosting all you need is a help of some container-based self-hosting software, my personal preference is with Proxmox and LXC. Following that all you must write are ansible scripts to manage hardware of Proxmox, similar way as you do for Amazon EC2\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mansible supports both greatly\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and you are good to go. One does not exclude another, quite the opposite, as they can live in great synergy and cut your costs dramatically \u001b[0m\u001b[32m(\u001b[0m\u001b[32mthe heavier your base load, the bigger the savings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m while providing production-grade resiliency.\\nGitHub\\nrelated GitHub posts\\nI was building a personal project that I needed to store items in a real time database. I am more comfortable with my Frontend skills than my backend so I didn\\'t want to spend time building out anything in Ruby or Go.\\nI stumbled on Firebase\\nby #Google, and it was really all I needed. It had realtime data, an area for storing file uploads and best of all for the amount of data I needed it was free!\\nI built out my application using tools I was familiar with, React for the framework, Redux.js to manage my state across components, and styled-components for the styling.\\nNow as this was a project I was just working on in my free time for fun I didn\\'t really want to pay for hosting. I did some research and I found Netlify. I had actually seen them at #ReactRally the year before and deployed a Gatsby\\nsite to Netlify already.\\nNetlify was very easy to setup and link to my GitHub account you select a repo and pretty much with very little configuration you have a live site that will deploy every time you push to master.\\nWith the selection of these tools I was able to build out my application, connect it to a realtime database, and deploy to a live environment all with $0 spent.\\nIf you\\'re looking to build out a small app I suggest giving these tools a go as you can get your idea out into the real world for absolutely no cost.\\nContext: I wanted to create an end to end IoT data pipeline simulation in Google Cloud IoT Core and other GCP services. I never touched Terraform meaningfully until working on this project, and it\\'s one of the best explorations in my development career. The documentation and syntax is incredibly human-readable and friendly. I\\'m used to building infrastructure through the google apis via Python , but I\\'m so glad past Sung did not make that decision. I was tempted to use Google Cloud Deployment Manager, but the templates were a bit convoluted by first impression. I\\'m glad past Sung did not make this decision either.\\nSolution: Leveraging Google Cloud Build Google Cloud Run Google Cloud Bigtable Google BigQuery Google Cloud Storage Google Compute Engine\\nalong with some other fun tools, I can deploy over 40 GCP resources using Terraform!\\nCheck Out My Architecture: CLICK ME\\nCheck out the GitHub repo attached\\nPython\\nrelated Python posts\\nHow Uber developed the open source, end-to-end distributed tracing Jaeger , now a CNCF project:\\nDistributed tracing is quickly becoming a must-have component in the tools that organizations use to monitor their complex, microservice-based architectures. At Uber, our open source distributed tracing system Jaeger saw large-scale internal adoption throughout 2016, integrated into hundreds of microservices and now recording thousands of traces every second.\\nHere is the story of how we got here, from investigating off-the-shelf solutions like Zipkin, to why we switched from pull to push architecture, and how distributed tracing will continue to evolve:\\nhttps://eng.uber.com/distributed-tracing/\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mGitHub Pages : https://www.jaegertracing.io/, GitHub: https://github.com/jaegertracing/jaeger\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nBindings/Operator: Python\\nJava\\nNode.js Go C++ Kubernetes JavaScript OpenShift C# Apache Spark\\nWinds 2.0 is an open source Podcast/RSS reader developed by Stream with a core goal to enable a wide range of developers to contribute.\\nWe chose JavaScript because nearly every developer knows or can, at the very least, read JavaScript. With ES6 and Node.js v10.x.x, itâ€™s become a very capable language. Async/Await is powerful and easy to use \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAsync/Await vs Promises\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Babel allows us to experiment with next-generation JavaScript \u001b[0m\u001b[32m(\u001b[0m\u001b[32mfeatures that are not in the official JavaScript spec yet\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Yarn allows us to consistently install packages quickly \u001b[0m\u001b[32m(\u001b[0m\u001b[32mand is filled with tons of new tricks\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nWeâ€™re using JavaScript for everything â€“ both front and backend. Most of our team is experienced with Go and Python, so Node was not an obvious choice for this app.\\nSure... there will be haters who refuse to acknowledge that there is anything remotely positive about JavaScript \u001b[0m\u001b[32m(\u001b[0m\u001b[32mthere are even rants on Hacker News about Node.js\u001b[0m\u001b[32m)\u001b[0m\u001b[32m; however, without writing completely in JavaScript, we would not have seen the results we did.\\n#FrameworksFullStack #Languages\\njQuery\\nrelated jQuery posts\\nThe client-side stack of Shopify Admin has been a long journey. It started with HTML templates, jQuery and Prototype. We moved to Batman.js, our in-house Single-Page-Application framework \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSPA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, in 2013. Then, we re-evaluated our approach and moved back to statically rendered HTML and vanilla JavaScript. As the front-end ecosystem matured, we felt that it was time to rethink our approach again. Last year, we started working on moving Shopify Admin to React and TypeScript.\\nMany things have changed since the days of jQuery and Batman. JavaScript execution is much faster. We can easily render our apps on the server to do less work on the client, and the resources and tooling for developers are substantially better with React than we ever had with Batman.\\n#FrameworksFullStack #Languages\\nI\\'m planning to create a web application and also a mobile application to provide a very good shopping experience to the end customers. Shortly, my application will be aggregate the product details from difference sources and giving a clear picture to the user that when and where to buy that product with best in Quality and cost.\\nI have planned to develop this in many milestones for adding N number of features and I have picked my first part to complete the core part \u001b[0m\u001b[32m(\u001b[0m\u001b[32maggregate the product details from different sources\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nAs per my work experience and knowledge, I have chosen the followings stacks to this mission.\\nUI:\\nI would like to develop this application using React, React Router and React Native since I\\'m a little bit familiar on this and also most importantly these will help on developing both web and mobile apps. In addition, I\\'m gonna use the stacks JavaScript, jQuery,\\njQuery UI, jQuery Mobile, Bootstrap wherever required.\\nService:\\nI have planned to use Java as the main business layer language as I have 7+ years of experience on this I believe I can do better work using Java than other languages. In addition, I\\'m thinking to use the stacks Node.js.\\nDatabase and ORM:\\nI\\'m gonna pick MySQL as DB and Hibernate as ORM since I have a piece of good knowledge and also work experience on this combination.\\nSearch Engine:\\nI need to deal with a large amount of product data and it\\'s in-detailed info to provide enough details to end user at the same time I need to focus on the performance area too. so I have decided to use Solr as a search engine for product search and suggestions. In addition, I\\'m thinking to replace Solr by Elasticsearch once explored/reviewed enough about Elasticsearch.\\nHost:\\nAs of now, my plan to complete the application with decent features first and deploy it in a free hosting environment like Docker and Heroku and then once it is stable then I have planned to use the AWS products Amazon S3, EC2, Amazon RDS and Amazon Route 53. I\\'m not sure about Microsoft Azure that what is the specialty in it than Heroku and Amazon EC2 Container Service. Anyhow, I will do explore these once again and pick the best suite one for my requirement once I reached this level.\\nBuild and Repositories:\\nI have decided to choose Apache Maven and Git as these are my favorites and also so popular on respectively build and repositories.\\nAdditional Utilities :\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n- I would like to choose Codacy for code review as their Startup plan will be very helpful to this application. I\\'m already experienced with Google CheckStyle and SonarQube even I\\'m looking something on Codacy.\\nHappy Coding!\\nSuggestions are welcome! :\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nThanks,\\nGanesa\\nNode.js\\nrelated Node.js posts\\nI just finished the very first version of my new hobby project: #MovieGeeks.\\nIt is a minimalist online movie catalog for you to save the movies you want to see and for rating the movies you already saw.\\nThis is just the beginning as I am planning to add more features on the lines of sharing and discovery\\nFor the #BackEnd I decided to use Node.js\\n, GraphQL\\nand MongoDB:\\nNode.js\\nhas a huge community so it will always be a safe choice in terms of libraries and finding solutions to problems you may have\\nGraphQL\\nbecause I needed to improve my skills with it and because I was never comfortable with the usual REST approach. I believe GraphQL is a better option as it feels more natural to write apis, it improves the development velocity, by definition it fixes the over-fetching and under-fetching problem that is so common on REST apis, and on top of that, the community is getting bigger and bigger.\\nMongoDB was my choice for the database as I already have a lot of experience working on it and because, despite of some bad reputation it has acquired in the last months, I still believe it is a powerful database for at least a very long list of use cases such as the one I needed for my website\\nWhen I joined NYT there was already broad dissatisfaction with the LAMP \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLinux Apache HTTP Server MySQL PHP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Stack and the front end framework, in particular. So, I wasn\\'t passing judgment on it. I mean, LAMP\\'s fine, you can do good work in LAMP. It\\'s a little dated at this point, but it\\'s not ... I didn\\'t want to rip it out for its own sake, but everyone else was like, \"We don\\'t like this, it\\'s really inflexible.\" And I remember from being outside the company when that was called MIT FIVE when it had launched. And been observing it from the outside, and I was like, you guys took so long to do that and you did it so carefully, and yet you\\'re not happy with your decisions. Why is that? That was more the impetus. If we\\'re going to do this again, how are we going to do it in a way that we\\'re gonna get a better result?\\nSo we\\'re moving quickly away from LAMP, I would say. So, right now, the new front end is React based and using Apollo. And we\\'ve been in a long, protracted, gradual rollout of the core experiences.\\nReact is now talking to GraphQL as a primary API. There\\'s a Node.js back end, to the front end, which is mainly for server-side rendering, as well.\\nBehind there, the main repository for the GraphQL server is a big table repository, that we call Bodega because it\\'s a convenience store. And that reads off of a Kafka pipeline.\\nVisual Studio Code\\nrelated Visual Studio Code posts\\nI am starting to become a full-stack developer, by choosing and learning\\n.NET Core for API Development, Angular CLI / React for UI Development, MongoDB for database, as it a NoSQL DB and Flutter / React Native for Mobile App Development.\\nUsing Postman, Markdown and Visual Studio Code for development.\\nOur first experience with .NET\\ncore was when we developed our OSS feature management platform - Tweek \u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://github.com/soluto/tweek\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nWe wanted to create a solution that is able to run anywhere \u001b[0m\u001b[32m(\u001b[0m\u001b[32msuper important for OSS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, has excellent performance characteristics and can fit in a multi-container architecture.\\nWe decided to implement our rule engine processor in F# , our main service was implemented in C#\\nand other components were built using JavaScript / TypeScript and Go.\\nVisual Studio Code\\nworked really well for us as well, it worked well with all our polyglot services and the .Net core integration had great cross-platform developer experience \u001b[0m\u001b[32m(\u001b[0m\u001b[32mto be fair, F# was a bit trickier\u001b[0m\u001b[32m)\u001b[0m\u001b[32m - actually, each of our team members used a different OS \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUbuntu, macos, windows\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nOur production deployment ran for a time on Docker Swarm until we\\'ve decided to adopt Kubernetes with almost seamless migration process.\\nAfter our positive experience of running .Net core workloads in containers and developing Tweek\\'s .Net services on non-windows machines, C# had gained back some of its popularity \u001b[0m\u001b[32m(\u001b[0m\u001b[32moriginally lost to Node.js\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and other teams have been using it for developing microservices, k8s sidecars \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike https://github.com/Soluto/airbag\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, cli tools, serverless functions and other projects...\\nSimilar Tools\\nNew Tools\\nTop Tools\\nTrending Comparisons\\nTools'\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Ibis'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://ibis-project.org/'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'An open source dataframe library that works with any data system. Use the same API for 20+ backends. Fast local dataframes with embedded DuckDB \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdefault\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Polars, or DataFusion. Iterate locally and deploy remotely by changing a single line of code. Compose SQL and Python dataframe code, bridging the gap between data engineering and data science.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m0.8008478\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'raw_content'\u001b[0m: \u001b[32m'Ibis\\nAn open source dataframe library that works with any data system\\nIbis: the portable Python dataframe library\\nIbis offers a familiar local dataframe experience with outstanding performance, using DuckDB by default.\\nIterate and explore data locally:\\nOne API for 20+ backends\\nUse the same dataframe API for 20+ backends:\\nFor example:\\nThis allows you to iterate locally and deploy remotely by changing a single line of code. For instance, develop locally with DuckDB and deploy remotely to BigQuery. Or, using any combination of backends that meet your requirements.\\nPython + SQL: better together\\nIbis works by decoupling the dataframe API from the backend execution. Most backends support a SQL dialect, which Ibis compiles its expressions into using SQLGlot. You can inspect the SQL that Ibis generates for any SQL backend:\\nAnd use SQL strings directly, mixing and matching with Python dataframe code:\\nThis allows you to combine the flexibility of Python with the scale and performance of modern SQL.\\nUsers say…\\n“Ibis is amazing, there is so much bikeshedding out there that this library improves upon. I love that now we can empower any visualization with nearly any dataset! Big thanks to those who have contributed!”\\nNick Shook\\n“I now have Ibis code that runs PySpark in my Databricks environment and Polars on my laptop which is pretty slick 🔥”\\nMark Druffel\\n“I love that with Ibis, I can use SQL for the heavy lifting or aggregations and then switch to a dataframe-like API for the type of dynamic transformations that would otherwise be tedious to do in pure SQL.”\\nDaniel Kim\\nGet started with Ibis\\nWhy Ibis?\\nTutorial: getting started\\nAPI reference'\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'DuckDB: In-Process Python Analytics for Not-Quite-Big Data'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://thenewstack.io/duckdb-in-process-python-analytics-for-not-quite-big-data/'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'An in-process analytics database, DuckDB can work with surprisingly large data sets without having to maintain a distributed multiserver system. Best of all? You can analyze data directly from your Python app.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m0.7255514\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'raw_content'\u001b[0m: \u001b[32m'We’re so glad you’re here. You can expect all the best TNS content to arrive\\nMonday through Friday to keep you on top of the news and at the top of your game.\\nCheck your inbox for a confirmation email where you can adjust your preferences\\nand even join additional groups.\\nFollow TNS on your favorite social media networks.\\nBecome a TNS follower on LinkedIn.\\nCheck out the latest featured and trending stories while you wait for your\\nfirst TNS newsletter.\\nDuckDB: In-Process Python Analytics for Not-Quite-Big Data\\nPITTSBURGH — You don’t always need a cluster to analyze even a very large data set. There is a lot you can pack into a single server running the open source DuckDB in-process analytical database system.\\nThis was one takeaway from a number of presentations comparing the performance of analytics solutions that were given at PyCon, a Python programmer’s conference held last week in Pittsburgh. There, they compared systems and asked, for instance, if a Dask system was faster at analytics than Apache Spark.\\nBut if you can avoid setting up a distributed system altogether, you can avoid a lot of headaches around upkeep.\\nAs explained in a presentation given by Kevin Kho and Han Wang, you can get a lot of mileage from a single machine, if it is optimized correctly. And this is the mission of DuckDB.\\nIn 2021, H20.ai tested DuckDB in a set of benchmarks comparing the processing speed for various database-like tools popular in open source data science.\\nThe testers ran five queries across 10 million rows and nine columns \u001b[0m\u001b[32m(\u001b[0m\u001b[32mabout 0.5GB\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Duck completed the task in a mere two seconds. That was surprising for a database running on a single computer. Even more surprising, it chewed through 100 million rows \u001b[0m\u001b[32m(\u001b[0m\u001b[32m5GB\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in 14 seconds.\\nThese numbers were impressive, and in 2023, the DuckDB folks went back and tweaked the configuration settings and upgraded the hardware and got the 5GB workload down to two seconds and the 0.5GB in less than a second.\\nIt even tackled the 50GB workload — normally reserved for distributed systems such as Spark — in 24 seconds.\\n“This is a mind-blowing number. The improvements are amazing,” said Wang, who is the tech lead of Lyft Machine Learning Platform, in the presentation.\\nDuckDB’s benchmark of Big Data systems, 2003.\\nThe takeaway? A surprising number of self-styled “big data”-styled projects don’t need Spark or some other distributed solution: They can fit nicely onto a single server, Wang noted. Taking this approach eliminates the considerable overhead of managing a distributed system, and keeps all the data and code on the local machine.\\nIntroducing DuckDB\\nThere’s a lot happening with DuckDB, an analytical, relational in-process SQL database system created in 2018. Two things that immediately set it apart from the other data platforms.\\n1: It combines SQL with Python, giving developers/analysts an expressive query language that executes against data in the application process itself.\\n2: It is meant to run only on a single machine. This is a feature, not a bug, as it eliminates all the complexity of running a data platform on a distributed platform.\\n“As soon as a problem gets a little bit too big for Pandas, you have to throw a giant distributed system at it. It’s like cracking a nut with a sledgehammer. It’s not ergonomic,” said\\xa0said Alex Monahan, in another Pycon presentation. Monham is a forward-deployed software engineer for MotherDuck, which offers a serverless analytics service based on Duck.\\nThe two creators of DuckDB — Hannes Mühleisen \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCEO\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and Mark Raasveldt \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCTO\u001b[0m\u001b[32m)\u001b[0m\u001b[32m — have founded DuckDB Labs, which provides commercial support for the database system, which was designed to offer a fast, easy-to-deploy mid-sized data analysis.\\nThey took considerable inspiration from the little database that could, considering DuckDB to be the SQLite of columns, rather than rows.\\nWith a Python-esque interface, Duck was also built specifically for the data science community. Data will be analyzed, modeled, and visualized. Data scientists tend not to use databases, instead relying on CSV files and other un- or semi-structured data sources. Duck allows them to embed data operations directly into their code itself.\\nThe MIT-licensed open source software is written in C++, so it is fast.\\nDuckDB is made to go fast, taking advantage of all the server’s cores and cache hierarchies. And whereas SQLite is a row-based database engine that processes one row at a time, Duck can process a whole vector, of 2,048 rows, at one time.\\nIt is a single binary install from the Python Installer It is available for multiple platforms, all pre-compiled so they can be downloaded and run through a command line, or through the client libraries. There’s even a version that runs in a browser via WebAssembly.\\nIt is an in-process application, and writes to disk, meaning it is not limited a server’s RAM, it can use the whole hard drive, opening the path to working with data sizes that are terabytes in size. Unlike a client-server database, it does not rely on a third-party transport mechanism to ship the data from the server to the client. Instead, just like SQLite, the application can pull the data as part of a Python call, in an in-process communication within the same memory space.\\n“You read it right where it sits,” Monahan said.\\nYou can write data frames natively to the database in a number of different ways, including user-defined functions, a full relational API, the Ibis library to simultaneously write data frames simultaneously across multiple back end data sources, and PySpark but with a different import statement.\\nHow DuckDB and Python Work Together\\nIn addition to the command line, it comes with clients for 15 languages. Python is the most popular, but there is also Node, JBDC, and OBDC. It can read CSV, JSON files, Apache Iceberg files. DuckDB can natively read Pandas, Polaris and Arrow files, without copying the data into another format. Unlike most SQL-only database systems, it keeps the original of the data as it is ingested.\\n“So this could fit into a lot of workflows,” Monahan said.\\nIt can also read files over the Internet, including those from GitHub \u001b[0m\u001b[32m(\u001b[0m\u001b[32mvia FTP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Amazon S3, Azure Blob storage and Google Cloud Storage. It can output TensorFlow and Pytorch Tensors.\\nDuckDB uses a SQL variant that is very Python-esque, one that can ingest data frames natively.\\nMonahan produced a sample “Hello World” app to illustrate:\\nwill produce the output:\\nThe database uses PostgreSQL as the base, though some modifications were made to the SQL, both for simplifying the language and for extending its capabilities.\\nThe ways DuckDB extends and simplifies SQL \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAlex Monahan presentation at Pycon\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nIs Big Data Dead?\\nIn summary, DuckDB is a fast database with a revolutionary intent, that of making single computer analytics possible for even very large datasets. It questions the need for Big Data-based solutions.\\nIn a widely-circulated 2023 MotherDuck blog post, provocatively entitled “Big Data Is Dead,” Jordan Tigani noted that “most applications do not need to process massive amounts of data.”\\n“The amount of data processed for analytics workloads is almost certainly smaller than you think,” he wrote. So it makes sense to look at a simple single computer-based analytics software before jumping into a more expensive data warehouse or distributed analytics system.\\nCommunity created roadmaps, articles, resources and journeys for\\ndevelopers to help you choose your path and grow in your career.'\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'DuckDB vs. The Titans: Spark, Elasticsearch, MongoDB - Medium'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://medium.com/walmartglobaltech/duckdb-vs-the-titans-spark-elasticsearch-mongodb-a-comparative-study-in-performance-and-cost-5366b27d5aaa'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'DuckDB as an Engine is the concept of utilizing DuckDB as a primary tool for database management and processing in an application or system.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m0.5539276\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'raw_content'\u001b[0m: \u001b[32m\"Sign up\\nSign in\\nSign up\\nSign in\\nDuckDB vs. The Titans: Spark, Elasticsearch, MongoDB — A Comparative Study in Performance and Cost\\nJiazhen Zhu\\nFollow\\nWalmart Global Tech Blog\\n--\\n6\\nListen\\nShare\\nA mini-guide about Duck as an Engine\\nDuckDB is a rising star in the realm of database management systems \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDBMS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, gaining prominence for its efficient columnar storage and execution design that is optimized for analytical queries. But how does it stack up against industry veterans like Apache Spark, Elasticsearch, and MongoDB? In this article, we will dive into a comparative study, examining their performance and cost implications.\\nAgenda\\nWhat is DuckDB?\\nDuckDB is an in-process SQL OLAP database management system\\nIts core strengths lie in three essential characteristics — analytical processing, in-memory operations, and a column-oriented structure, making it akin to what SQLite is for PostgreSQL, DuckDB is for Redshift, Snowflake, and others.\\nHowever, DuckDB is not without its limitations. Its scalability is relatively limited, and it is not optimized for heavy data writing workloads. Its application in production environments is also restricted due to its newness in the market. Moreover, as it is an in-memory system, DuckDB’s performance may degrade or become infeasible for datasets larger than the available memory.\\nThe Titans: Apache Spark, Elasticsearch, and MongoDB\\nThese three tools have now become industry mainstays, with numerous companies employing them to accelerate their data processing and searching capabilities.\\nApache Spark is an open-source distributed computing system known for its speed, versatility, and sophisticated analytics.\\nElasticsearch is a robust search and analytics engine.\\nMongoDB is a source-available, NoSQL database program offering a flexible, JSON-like document model.\\nPerformance Comparison\\nTo gain a deeper understanding of DuckDB, we’ll conduct a performance analysis involving three types of data processing tasks.\\nFor Apache Spark, our test setup involves a ‘n1-standard-16’ master with ten ‘n1-standard-32’ workers. For DuckDB, we’ll utilize a ‘n2d-highmem-16’.\\nOur testing scenario involves two tables:\\nThe three operations we’ll analyze are as follows:\\nYou can find all the detailed information in the table below.\\nPost completion of our tests, we’ve gathered performance metrics for Apache Spark, MongoDB, BigQuery \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBQ\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Elasticsearch, DuckDB with external tables \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDuckDB ext.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and DuckDB with internal tables \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDuckDB int.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m across all three types of data processing. The details of these metrics can be found in Table 2.\\nIn summary, DuckDB outperforms the other database systems in these three operations, especially in its internal configuration. This might be due to its in-memory and columnar architecture, making it a strong candidate for aggregation, join, and search operations tasks.\\nCost Analysis\\nBeyond performance, cost-effectiveness is a crucial factor that warrants consideration.\\nThe cost for running Apache Spark, MongoDB, Elasticsearch, and DuckDB, predicated on the selected Virtual Machine \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, can be viewed in Table 3. A more detailed examination in Table 4 reveals that DuckDB yields substantial cost savings: around 90% compared to Spark, approximately 63% to MongoDB, and about 81% to Elasticsearch.\\nIn conclusion, while each of these DBMS has unique strengths in terms of capabilities and performance, DuckDB emerges as the most cost-effective option, according to this table.\\nCase Studies and Limitations\\nDuckDB as an Engine is the concept of utilizing DuckDB as a primary tool for database management and processing in an application or system.\\nFigure 1 illustrates an end-to-end workflow leveraging DuckDB as an Engine. We can utilize DuckDB as an Engine \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDaaE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in three specific areas: conventional ETL transformation, the speed layer for search and aggregation, and backend service-side API processing. We will delve into each of these areas in the subsequent sections.\\nConventional ETL Transformation:\\nIn the realm of ETL processes, tools like Spark or BigQuery are commonly employed for data handling. However, they can encounter certain constraints. For instance, Spark requires careful tuning to deliver effective data processing, while BigQuery can be costly when dealing with high-volume and complex queries.\\nDuckDB as an Engine \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDaaE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m offers notable solutions for performance tuning and cost-saving. As seen in Table 5, DaaE can significantly enhance performance — in most cases offering over a 90% increase — without necessitating any specific tuning. As highlighted in Table 4, DaaE also excels in facilitating cost savings.\\nThe speed layer for search and aggregation:\\nUsers anticipate swift responses to search requests or intricate SQL queries as data reaches the final speed layer. The table below offers a comparative analysis between DuckDB with external tables \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDuckDB ext.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and MongoDB, Elasticsearch \u001b[0m\u001b[32m(\u001b[0m\u001b[32mES\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, as well as DuckDB with internal tables \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDuckDB int.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and Elasticsearch \u001b[0m\u001b[32m(\u001b[0m\u001b[32mES\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nBackend service-side API processing:\\nIn data-intensive applications that offer user-facing analytics, developers often utilize backend services for data transformations, such as formatting or computations. If developers are primarily performing SQL-style processing, incorporating DuckDB as an Engine \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDaaE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m can significantly enhance the performance of these transformations.\\nPotential Constraints of DuckDB as an Engine \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDaaE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\nDuck as an Engine with DataBathing\\nI plan to integrate DuckDB as an Engine \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDaaE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m into the DataBathing framework. This way, users can leverage the advantages of both Spark and DaaE.\\nIf you’re unfamiliar with DataBathing, additional information can be found in the blogs listed below:\\nDataBathing — A Framework for Transferring the Query to Spark Code\\nA mini-guide about Query-Config Driven Coding\\nmedium.com\\nModularization Using Auto-Generated Pipeline With DataBathing\\nA mini-guide about MicroService design for Data Pipeline: Part III\\nmedium.com\\nSummary\\nDuckDB as an Engine \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDaaE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is a strong choice for certain scenarios. However, when selecting a database management system \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDBMS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for your specific needs, balancing cost, performance requirements, and specific needs is critical. The optimal system will hinge on factors such as the tasks' nature, data volume, and budget limitations.\\n--\\n--\\n6\\nWritten by Jiazhen Zhu\\nWalmart Global Tech Blog\\nData @ Walmart\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\"\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Pandas 2.0 and its Ecosystem \u001b[0m\u001b[32m(\u001b[0m\u001b[32mArrow, Polars, DuckDB\u001b[0m\u001b[32m)\u001b[0m\u001b[32m - Airbyte'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://airbyte.com/blog/pandas-2-0-ecosystem-arrow-polars-duckdb'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'Pandas recently got an update, which is version 2.0. This article takes a closer look at what Pandas is, its success, and what the new version brings, including its ecosystem around Arrow, Polars, and DuckDB. Pandas has established itself as the standard tool for in-memory data processing in Python, and it offers an extensive range of data ...'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m0.4698243\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'raw_content'\u001b[0m: \u001b[32m\"Fully-managed, get started in minutes\\nSecure data movement for your entire org\\nUsed by 40k+\\xa0companies\\nEmbed 100s integrations at once in your app\\nReliable database and API replication at any scale\\nEmbeddings from unstructured data\\nBuild a new connector \\xa0in 10 min\\nThe power of Airbyte to every Python developer\\nMake sense of unstructured data with LLMs\\nHigh-volume DBs with low latency\\nMarketing, sales, product, finance, eng & more\\nEasily collect credentials from your end-users\\nLearn from other members’ success\\nChoose the right solutions for you\\nEvaluate your costs in both scenarios\\nOur guides to help you in your journey\\nBecome a technology or consulting partner\\nOur next community call \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWED JUL 31\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nHow to use and contribute to Airbyte\\nData engineering thought leadership\\nImprove your data replication game\\nDeploy your use case in minutes\\nLearn how to use Airbyte with Airflow, dbt, etc.\\nPlace for all data knowledge\\nGet a glimpse in the future\\nStay up to date. 30k+ subscribers\\nAccess our knowledge base\\nJoin our 20k+ data \\xa0community\\nLeave your mark in the OSS community\\nLive events by the Airbyte team\\nCommunity's knowledge base\\nGitHub Discussions for questions, ideas\\n20,000+ \\xa0share tips and get support\\nLearn more about Airbyte and data engineering\\nPandas 2.0 and its Ecosystem \u001b[0m\u001b[32m(\u001b[0m\u001b[32mArrow, Polars, DuckDB\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nData manipulation and analysis can be challenging and involve working with large datasets. Thankfully, a widely used Python library known as Pandas has become the go-to tool for processing and manipulating data. Pandas recently got an update, which is version 2.0. This article takes a closer look at what Pandas is, its success, and what the new version brings, including its ecosystem around Arrow, Polars, and DuckDB.\\nPandas has established itself as the standard tool for in-memory data processing in Python, and it offers an extensive range of data manipulation capabilities. As such, it is unsurprising that data engineers or those just starting with data have come across it at some point in their work.\\nIn the recent update, Pandas 2.0 has adopted Apache Arrow as a backend. This article explores why this is the most significant change in the update and how Arrow has become a fundamental part of many recently released frameworks and hot topics in data engineering.\\nThat makes it an excellent time to reflect on what Pandas is and why it’s successful.\\nWhat is Pandas\\nBut let’s start with what Pandas is? Pandas is an library to process data in memory with a feature-rich Python API. Compared to many tools, Pandas is the python library to work with smaller data and sets the standard for wrangling data in memory.\\nPandas is generally suited for:\\nSome key features you might have used it for:\\nHow Does Pandas Work?\\nTo understand what is new and better with the latest version, let’s briefly discuss how Pandas works.\\nBefore doing anything with Pandas, the general idea is to load data in-memory into a Pandas DataFrame, usually using functions like read_csv, read_sql, read_parquet, etc. When loading data into memory, it must decide how it will be stored in memory.\\nFor simple data like integers of floats, this is standard and straightforward. But some decisions must be made for other types, such as strings, dates and times, and categories.\\nPython is versatile and can represent mostly anything, but Python data structures \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlists, dictionaries, tuples, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are very slow and can’t be used. So the data representation is not Python and is not standard, and implementation needs to happen via Python extensions, usually implemented in C \u001b[0m\u001b[32m(\u001b[0m\u001b[32malso in C++, Rust, and others\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nFor many years, the main extension to represent arrays and perform operations on them quickly has been NumPy. Which is also what Pandas was initially built on.\\nWhile NumPy has been good enough to make pandas the popular library it is, according to Marc Garcia \u001b[0m\u001b[32m(\u001b[0m\u001b[32mpandas core developer\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, it was never built as a backend for DataFrame libraries, and it has some critical limitations.\\nWhat Are the Highlights of Version 2.0\\nThat brings us to version 2.0. Let’s look at the key improvements: what are the highlights, and why do we have them?\\nAll of these are achieved through using Apache Arrow as the backend. By moving from NumPy \u001b[0m\u001b[32m(\u001b[0m\u001b[32mC++\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to Apache Arrow as a backend \u001b[0m\u001b[32m(\u001b[0m\u001b[32mespecially In Pandas 1.5 and 2.0 added Arrow support for all data types\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Arrow gives it better storage and speed. A significant milestone was implementing a string data type based on Arrow that started in 2020.\\nBefore we dive into a code example and why Arrow helped across the board, here is a short comparison of how the speed improved:\\nWhat Changes Code-Wise?\\nThe change of the Arrow type results in an extension of the type pyarrow. The pyarrow-addition was chosen to avoid breaking changes to existing code.\\nArrow also defines a type to encode categories:\\nWhat is Apache Arrow?\\nApache Arrow sets the open standard to exchange in a heterogeneous data pipeline, which needs to read and share data among different steps.\\nInstead of the default, to persist copies along the way and lose costly time and resources to write and read that data, Apache Arrow allows sharing data in an in-memory representation without persisting.\\nWhy Apache Arrow?\\nApache Arrow solves most discussed problems, such as improving speed, interoperability, and data types, especially for strings. For example, the new string\u001b[0m\u001b[32m[\u001b[0m\u001b[32mpyarrow\u001b[0m\u001b[32m]\u001b[0m\u001b[32m column type is around 3.5 times more efficient.\\nThese efficiency gains also greatly influence how much data you can load with the same amount of RAM, which is essential when most Pandas DataFrames run on single laptops.\\nOne of the biggest problems of Pandas was data interchange, particularly moving large tabular datasets from one process’s memory space to another’s.\\nThe significant achievement here is zero-copy data access, mapping complex tables to memory to make accessing one terabyte of data on disk as fast and easy as one megabyte.\\nAnother big one is above standardization saves. Apache Arrow has a vast ecosystem, and you can share it among all other libraries that integrate with Arrow. Moreover, they can implement custom connectors for new libraries and systems such as Polars. On top of these savings, a standardized memory format facilitates the reuse of libraries of algorithms, even across languages.\\nInteroperability\\nArrow is a program-independent format. Similar to Data Lake File Formats but less obvious, as it does not have a file extension attached and only exists in memory.\\nIts interoperability makes it relatively easy to share the data among different programs and is speedy and memory-efficient because two programs can share the same data, in this case, the same memory, without copying each program. Which is the dream of every data engineer, right?\\nWhen you have a data pipeline that loads some data from your data lake, transforms it, computes some analytics, and then exports and automatically creates a Data Asset that can be used for analytics, you usually have different tools, abstractions, or even teams in more prominent companies. If you do not need to persist the data set at each step, that’s a tremendous boost in performance and cost, as you do not need to store so much data immediately.\\nWhen Not to Use Pandas\\nSo when is Pandas not the right choice?\\nPeople who are used to SQL usually prefer something other than the API of Pandas. Too bloat and too many “gotchas” is what people say.\\nThe main problem is that you know how to do it in SQL, but you need to fiddle that SQL into a strange syntax you must look up most of the time.\\nBut with the latest 2.0 release, many of these issues are handled, and with the integration of Arrow, future-proof and the cost of switching to another tool for a dedicated process is no problem anymore.\\nThe Alternatives\\nThe data ecosystem is growing every day, giving us plenty of alternatives to choose from. Let’s look at these alternatives and try to understand why they were created and when they should be used.\\nPolars: Riding the Fast Train of Rust\\nThe one-to-one replacement that provides a rich Python API would be Polars, implemented in Rust.\\nSome say Polars has a less confusing API and better ergonomics, especially from SQL. Polars is more performant out of the box but less stable and mature. It is growing the fastest of all mentioned in this chapter.\\nPolars has superpowers as it comes with a query optimizer that can make the pipeline run faster by analyzing all operations together before executing them.\\nAn example of how that could look “switching” from one to another format:\\nDaniel Beach says that Polars is the more accessible version of Spark and easier to understand than Pandas.\\nDuckDB: The SQL Version\\nDuckDB would be for SQL lovers. Sure, it’s a database format, but in case you didn’t know, DuckDB is also super strong as a zero-copy layer approach. For example, you can use DuckDB as a thin SQL wrapper on top of your S3 files across your Data Lake. DuckDB will do a great job of providing you with fast analytical queries.\\nDuckDB can efficiently run SQL queries directly on Pandas DataFrames. If you want to use SQL and have a fast interface, use DuckDB.\\nDuckDB can also query Arrow datasets directly and stream query results back to Arrow. This streaming allows users to query Arrow data using DuckDB’s SQL Interface and API while taking advantage of DuckDB’s parallel vectorized execution engine without requiring extra data copying. Additionally, this integration takes full advantage of Arrow’s predicate and filter pushdown while scanning datasets.\\nMore \u001b[0m\u001b[32m(\u001b[0m\u001b[32mhow it works, benchmarks, example projects\u001b[0m\u001b[32m)\u001b[0m\u001b[32m about DuckDB on the Data Glossary.\\nWhat about Dask?\\nMany people may think of Dask as helping with Pandas performance and scalability. It optimizes speed by parallelizing large datasets into pieces and working with them in separate threads or processes or rescuing Pandas from the RAM limit.\\nOne problem with the Dask is that it uses Pandas as a black box. dask.dataframe does not solve Pandas inherent performance and memory use issues. Still, it spreads them out across multiple processes. It helps mitigate them by being careful not to work simultaneously with too large pieces of data, which can result in an unpleasant MemoryError.\\nThey make Dask jobs slower than possible with a more efficient in-memory runtime.\\nOthers: Koalas, Vaex, VertiPaq\\nFor completion, here are some pointers to other valid alternatives without explaining much more about them.\\nKoalas is another one, built on top of Spark, re-implemented 70%+ of the Pandas API by Databricks, and now officially included in PySpark since Apache Spark 3.2. Interesting to see that Spark has a similar growth rate of GitHub stars as Pandas, especially as Spark DataFrames are very similar to Pandas, supporting a rich Python API and SQL. But instead of stored locally, it’s persisted inside the Spark cluster.\\nVaex is a high-performance Python library for lazy out-of-core DataFrames \u001b[0m\u001b[32m(\u001b[0m\u001b[32msimilar to Pandas\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to visualize and explore big tabular datasets.\\nVertiPaq is the closed-source version from Microsoft. An engine is an in-memory columnar database behind Excel Power Pivot, SQL Server Analysis Services \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSSAS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Tabular, and Power BI. When you load data into a data model, it is loaded, compressed, and stored in RAM using the VertiPaq engine.\\nData Lake Table Formats\\nAnother way to solve this problem is using a Data Lake Table Format. Hence, you only read a fraction of the data to the pandas DataFrame, similar to what DuckDB is doing, but these formats are for large-scale and distributed files.\\nThese have other advantages, such as predicate filtering and Z-ORDERING, which is more straightforward than adding a new tech like Polars, DuckDB, or others to the stack.\\nConclusion\\nWe have seen a brief on what Pandas is and how it works, highlighting its key features and capabilities. It has also touched on the limitations of its initial backend, NumPy, and how the move to Arrow in Pandas 2.0 addresses these limitations. Overall, this article provides insights into the benefits of using Pandas, particularly with its 2.0 version, and the exciting changes in its ecosystem around Arrow, Polars, and DuckDB.\\nLuckily whatever tool you use, once Pandas 2.0 is used, if you use Arrows types, converting between Pandas, Polars, and others will be almost immediate, with little metadata overhead.\\n\\u200d\\n\u001b[0m\u001b[32m<\u001b[0m\u001b[32mhr\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\\n\\u200d\\nIf you liked this content, subscribe to our Newsletter, or sign up for Slack to join 10k+ data engineers in our community.\\nAbout the Author\\nSimon is a Data Engineer and Technical Author at Airbyte. He is dedicated, empathetic, and entrepreneurial with 15+ years of experience in the data ecosystem. He enjoys maintaining awareness of new innovative and emerging open-source technologies.\\nAbout the Author\\nTable of contents\\nJoin our newsletter to get all the insights on the data stack\\nRelated posts\\nFollow the largest open-source data movement repo\\nHi there! Keep yourself up to date with our progress, our events, and the latest news on data & AI engineering.\"\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'response_time'\u001b[0m: \u001b[1;36m5.22\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.pretty import pprint\n",
    "import json\n",
    "pprint(json.loads(result_df.search_result[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matches = re.findall(r\"(?:^|\\r?\\n)(?: {4}|\\t)[^\\r\\n]{0,200}(?:\\r?\\n(?: {4}|\\t)[^\\r\\n]{0,200}){0,20}\\r?\\n?\" , text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Item 1\n",
      "    - Subitem 1.1\n",
      "    - Subitem 1.2\n",
      "- Item 2\n",
      "    1. Subitem 2.1\n",
      "        - Sub-subitem 2.1.1\n",
      "    2. Subitem 2.2\n"
     ]
    }
   ],
   "source": [
    "matches = re.findall(pattern_all, text, re.MULTILINE)\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches = re.findall(quoted_text_pattern, text, re.MULTILINE)\n",
    "# for match in matches:\n",
    "#     print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Web Scraping Python Tutorial - How to Scrape Data From A Website', 'href': 'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/', 'body': 'Note that this is only one of the solutions. You can attempt this in a different way too. In this solution: First of all you select all the div.thumbnail elements which gives you a list of individual products; Then you iterate over them; Because select allows you to chain over itself, you can use select again to get the title.'}, {'title': 'How To Scrape Data from Any Website: 5 Code and No-Code Methods', 'href': 'https://www.scrapin.io/blog/web-scraping', 'body': 'To help you with this, here are some of the methods that you can use depending on your data extraction needs: \\u200d. 1. Manual Scraping with Upwork and Fiverr. If you are interested in manual data scraping, you can hire a freelancer via popular freelancing platforms like Upwork and Fiverr.'}, {'title': 'My ultimate guide to web scraping : r/datascience - Reddit', 'href': 'https://www.reddit.com/r/datascience/comments/a116l5/my_ultimate_guide_to_web_scraping/', 'body': \"A space for data science professionals to engage in discussions and debates on the subject of data science. My ultimate guide to web scraping. I've been doing some freelance web scraping for a few years now and thought it might be interesting to create a multi-part tutorial on building a scraping project with a data science end goal.\"}, {'title': 'Python Web Scraping: Full Tutorial With Examples (2024)', 'href': 'https://www.scrapingbee.com/blog/web-scraping-101-with-python/', 'body': 'Copying Our Target XPath from Chrome Dev Tools. Open Chrome Dev Tools (press F12 key or right-click on the webpage and select \"Inspect\") Use the element selector tool to highlight the element you want to scrape. Right-click the highlighted element in the Dev Tools panel. Select \"Copy\" and then \"Copy XPath\".'}, {'title': 'Web Scraping with Python: A Complete Step-by-Step Guide + Code', 'href': 'https://medium.com/geekculture/web-scraping-with-python-a-complete-step-by-step-guide-code-5174e52340ea', 'body': 'Keep to GoLogin recommended fingerprint settings for best results: even one option changed at your wish may affect your browser anonymity. Step 3: Configure proxies In the profile settings, click ...'}]\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "results = DDGS().text(\"Best way to webscrape\", max_results=5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Web Scraping Python Tutorial - How to Scrape Data From A Website'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'href'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Note that this is only one of the solutions. You can attempt this in a different way too. In this solution: First of all you select all the div.thumbnail elements which gives you a list of individual products; Then you iterate over them; Because select allows you to chain over itself, you can use select again to get the title.'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'How To Scrape Data from Any Website: 5 Code and No-Code Methods'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'href'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://www.scrapin.io/blog/web-scraping'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'To help you with this, here are some of the methods that you can use depending on your data extraction needs: \\u200d. 1. Manual Scraping with Upwork and Fiverr. If you are interested in manual data scraping, you can hire a freelancer via popular freelancing platforms like Upwork and Fiverr.'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'My ultimate guide to web scraping : r/datascience - Reddit'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'href'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://www.reddit.com/r/datascience/comments/a116l5/my_ultimate_guide_to_web_scraping/'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"A space for data science professionals to engage in discussions and debates on the subject of data science. My ultimate guide to web scraping. I've been doing some freelance web scraping for a few years now and thought it might be interesting to create a multi-part tutorial on building a scraping project with a data science end goal.\"</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Python Web Scraping: Full Tutorial With Examples (2024)'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'href'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://www.scrapingbee.com/blog/web-scraping-101-with-python/'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Copying Our Target XPath from Chrome Dev Tools. Open Chrome Dev Tools (press F12 key or right-click on the webpage and select \"Inspect\") Use the element selector tool to highlight the element you want to scrape. Right-click the highlighted element in the Dev Tools panel. Select \"Copy\" and then \"Copy XPath\".'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Web Scraping with Python: A Complete Step-by-Step Guide + Code'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'href'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://medium.com/geekculture/web-scraping-with-python-a-complete-step-by-step-guide-code-5174e52340ea'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Keep to GoLogin recommended fingerprint settings for best results: even one option changed at your wish may affect your browser anonymity. Step 3: Configure proxies In the profile settings, click ...'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Web Scraping Python Tutorial - How to Scrape Data From A Website'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'href'\u001b[0m: \u001b[32m'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'body'\u001b[0m: \u001b[32m'Note that this is only one of the solutions. You can attempt this in a different way too. In this solution: First of all you select all the div.thumbnail elements which gives you a list of individual products; Then you iterate over them; Because select allows you to chain over itself, you can use select again to get the title.'\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'How To Scrape Data from Any Website: 5 Code and No-Code Methods'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'href'\u001b[0m: \u001b[32m'https://www.scrapin.io/blog/web-scraping'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'body'\u001b[0m: \u001b[32m'To help you with this, here are some of the methods that you can use depending on your data extraction needs: \\u200d. 1. Manual Scraping with Upwork and Fiverr. If you are interested in manual data scraping, you can hire a freelancer via popular freelancing platforms like Upwork and Fiverr.'\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'My ultimate guide to web scraping : r/datascience - Reddit'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'href'\u001b[0m: \u001b[32m'https://www.reddit.com/r/datascience/comments/a116l5/my_ultimate_guide_to_web_scraping/'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'body'\u001b[0m: \u001b[32m\"A space for data science professionals to engage in discussions and debates on the subject of data science. My ultimate guide to web scraping. I've been doing some freelance web scraping for a few years now and thought it might be interesting to create a multi-part tutorial on building a scraping project with a data science end goal.\"\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Python Web Scraping: Full Tutorial With Examples \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'href'\u001b[0m: \u001b[32m'https://www.scrapingbee.com/blog/web-scraping-101-with-python/'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'body'\u001b[0m: \u001b[32m'Copying Our Target XPath from Chrome Dev Tools. Open Chrome Dev Tools \u001b[0m\u001b[32m(\u001b[0m\u001b[32mpress F12 key or right-click on the webpage and select \"Inspect\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Use the element selector tool to highlight the element you want to scrape. Right-click the highlighted element in the Dev Tools panel. Select \"Copy\" and then \"Copy XPath\".'\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Web Scraping with Python: A Complete Step-by-Step Guide + Code'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'href'\u001b[0m: \u001b[32m'https://medium.com/geekculture/web-scraping-with-python-a-complete-step-by-step-guide-code-5174e52340ea'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'body'\u001b[0m: \u001b[32m'Keep to GoLogin recommended fingerprint settings for best results: even one option changed at your wish may affect your browser anonymity. Step 3: Configure proxies In the profile settings, click ...'\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import warnings\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "import PyPDF2\n",
    "import io\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "# load_dotenv()\n",
    "# api_key=os.environ['TVLY_API_KEY']\n",
    "\n",
    "def search_query(query:str, api_key):\n",
    "    tavily_client = TavilyClient(api_key=api_key)\n",
    "    results = tavily_client.search(query,\n",
    "                               include_raw_content=False)\n",
    "    return results\n",
    "\n",
    "\n",
    "# List of user-agents\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.3',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1',\n",
    "    'Mozilla/5.0 (Linux; Android 8.0.0; SM-G960F Build/R16NW) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.84 Mobile Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko'\n",
    "]\n",
    "\n",
    "# Proxy details\n",
    "PROXIES = {\n",
    "    'http': 'http://your_proxy_server:port',\n",
    "    'https': 'https://your_proxy_server:port'\n",
    "}\n",
    "\n",
    "async def fetch_content(url: str) -> str:\n",
    "    \"\"\"Fetches the content from a given URL asynchronously.\n",
    "\n",
    "    Args:\n",
    "        url: The URL to fetch the content from.\n",
    "    Returns:\n",
    "        The content of the page, or an empty string if an error occurs.\n",
    "    \"\"\"\n",
    "    # await asyncio.sleep(random.randint(2, 5))  # Simulate human-like behavior with random sleep durations\n",
    "    try:\n",
    "        headers = {'User-Agent': random.choice(USER_AGENTS)}\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url, headers=headers) as response:\n",
    "                response.raise_for_status()\n",
    "                if url.endswith(\".pdf\"):\n",
    "                    # Handle PDF content\n",
    "                    content = await response.read()\n",
    "                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))\n",
    "                    text = \"\"\n",
    "                    for page in range(len(pdf_reader.pages)):\n",
    "                        text += pdf_reader.pages[page].extract_text()\n",
    "                    return text\n",
    "                else:\n",
    "                    # Handle other content types as before\n",
    "                    return await response.text()\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"An error occurred while fetching content from {url}: {e}\")\n",
    "        return ''\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"An error occurred while decoding content from {url}: {e}\")\n",
    "        return ''\n",
    "\n",
    "def parse_content(content: str) -> str:\n",
    "    \"\"\"Parses the HTML content to extract text.\n",
    "\n",
    "    Args:\n",
    "        content: The raw HTML content.\n",
    "    Returns:\n",
    "        The text content of the page.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = '\\n'.join([s.get_text(strip=True) for s in soup.find_all()])\n",
    "    return text\n",
    "\n",
    "async def get_text_from_url(url: str) -> str:\n",
    "    \"\"\"Fetches and parses the text content from a given URL asynchronously.\n",
    "\n",
    "    Args:\n",
    "        url: The URL to fetch the content from.\n",
    "    Returns:\n",
    "        The text content of the page, or an empty string if an error occurs.\n",
    "    \"\"\"\n",
    "    content = await fetch_content(url)\n",
    "    if content:\n",
    "        return parse_content(content)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_html= await fetch_content(results[0]['href'])\n",
    "if text_html:\n",
    "    text_output = parse_content(text_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Web Scraping Python Tutorial – How to Scrape Data From A WebsiteSearchSubmit your search queryForumDonateSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)Passing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)Let\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)Part 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)Let\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}Note that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)In this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}Make sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)Here, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)You have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)Theforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENTfreeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)Our mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile AppOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nWeb Scraping Python Tutorial – How to Scrape Data From A Website\\n\\n\\nWeb Scraping Python Tutorial – How to Scrape Data From A Website\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlet client,index;document.addEventListener(\"DOMContentLoaded\",(()=&gt;{client=algoliasearch(\"QMJYL5WYTI\",\"89770b24481654192d7a5c402c6ad9a0\"),index=client.initIndex(\"news\")})),document.addEventListener(\"DOMContentLoaded\",(()=&gt;{const e=window.screen.width,t=window.screen.height,n=e&gt;=767&amp;&amp;t&gt;=768?8:5,o=document.getElementById(\"search-form\"),s=document.getElementById(\"search-input\"),a=document.getElementById(\"dropdown-container\");let i,d,c;s.addEventListener(\"input\",(e=&gt;{i=e.target.value})),o.addEventListener(\"submit\",(e=&gt;{e.preventDefault(),function(){if(d=document.getElementsByClassName(\"aa-cursor\")[0],d&amp;&amp;i){const e=d.querySelector(\"a\").href;window.location.assign(e)}else!d&amp;&amp;i&amp;&amp;c&amp;&amp;window.location.assign(`https://www.freecodecamp.org/news/search?query=${i}`)}()}));const l=autocomplete(\"#search-input\",{hint:!1,keyboardShortcuts:[\"s\",191],openOnFocus:!0,appendTo:a,debug:!0},[{source:autocomplete.sources.hits(index,{hitsPerPage:n}),debounce:250,templates:{suggestion:e=&gt;(c=!0,`\\\\n            &lt;a href=\"${e.url}\"&gt;\\\\n              &lt;div class=\"algolia-result\"&gt;\\\\n                &lt;span&gt;${e._highlightResult.title.value}&lt;/span&gt;\\\\n              &lt;/div&gt;\\\\n            &lt;/a&gt;\\\\n          `),empty:()=&gt;(c=!1,\\'\\\\n            &lt;div class=\"aa-suggestion footer-suggestion no-hits-footer\"&gt;\\\\n              &lt;div class=\"algolia-result\"&gt;\\\\n                &lt;span&gt;\\\\n                  No tutorials found\\\\n                &lt;/span&gt;\\\\n              &lt;/div&gt;\\\\n            &lt;/div&gt;\\\\n          \\'),footer:e=&gt;{if(!e.isEmpty)return`\\\\n              &lt;div class=\"aa-suggestion footer-suggestion\"&gt;\\\\n                &lt;a id=\"algolia-footer-selector\" href=\"https://www.freecodecamp.org/news/search?query=${i}\"&gt;\\\\n                  &lt;div class=\"algolia-result algolia-footer\"&gt;\\\\n                    See all results for ${i}\\\\n                  &lt;/div&gt;\\\\n                &lt;/a&gt;\\\\n              &lt;/div&gt;\\\\n            `}}}]).on(\"autocomplete:selected\",((e,t,n,o)=&gt;{d=t?t.url:`https://www.freecodecamp.org/news/search?query=${i}`,\"click\"!==o.selectionMethod&amp;&amp;\"tabKey\"!==o.selectionMethod&amp;&amp;c&amp;&amp;window.location.assign(d)}));document.addEventListener(\"click\",(e=&gt;{e.target!==s&amp;&amp;l.autocomplete.close()}))})),document.addEventListener(\"DOMContentLoaded\",(()=&gt;{dayjs.extend(dayjs_plugin_localizedFormat),dayjs.extend(dayjs_plugin_relativeTime),dayjs.locale(\"en\")}));const isAuthenticated=document.cookie.split(\";\").some((e=&gt;e.trim().startsWith(\"jwt_access_token=\"))),isDonor=document.cookie.split(\";\").some((e=&gt;e.trim().startsWith(\"isDonor=true\")));\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n{\\n\\t\"@context\": \"https://schema.org\",\\n\\t\"@type\": \"Article\",\\n\\t\"publisher\": {\\n\\t\\t\"@type\": \"Organization\",\\n\\t\\t\"name\": \"freeCodeCamp.org\",\\n\\t\\t\"url\": \"https://www.freecodecamp.org/news/\",\\n\\t\\t\"logo\": {\\n\\t\\t\\t\"@type\": \"ImageObject\",\\n\\t\\t\\t\"url\": \"https://cdn.freecodecamp.org/platform/universal/fcc_primary.svg\",\\n\\t\\t\\t\"width\": 2100,\\n\\t\\t\\t\"height\": 240\\n\\t\\t}\\n\\t},\\n\\t\"image\": {\\n\\t\\t\"@type\": \"ImageObject\",\\n\\t\\t\"url\": \"https://www.freecodecamp.org/news/content/images/2020/09/webscrapingposter.jpg\",\\n\\t\\t\"width\": 1920,\\n\\t\\t\"height\": 1080\\n\\t},\\n\\t\"url\": \"https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\",\\n\\t\"mainEntityOfPage\": {\\n\\t\\t\"@type\": \"WebPage\",\\n\\t\\t\"@id\": \"https://www.freecodecamp.org/news/\"\\n\\t},\\n\\t\"datePublished\": \"2020-09-25T20:24:10.000Z\",\\n\\t\"dateModified\": \"2020-10-26T23:54:33.000Z\",\\n\\t\"keywords\": \"Web Scraping, Python\",\\n\\t\"description\": \"Python is a beautiful language to code in. It has a great package ecosystem,\\\\nthere&amp;#x27;s much less noise than you&amp;#x27;ll find in other languages, and it is super\\\\neasy to use.\\\\n\\\\nPython is used for a number of things, from data analysis to server programming.\\\\nAnd one exciting use-case of Python is Web Scraping. \\\\n\\\\nIn this article, we will cover how to use Python for web scraping. We&amp;#x27;ll also\\\\nwork through a complete hands-on classroom guide as we proceed.\\\\n\\\\nNote: We will be scraping a webpage that I host, so w\",\\n\\t\"headline\": \"Web Scraping Python Tutorial – How to Scrape Data From A Website\",\\n\\t\"author\": {\\n\\t\\t\"@type\": \"Person\",\\n\\t\\t\"name\": \"Mehul Mohan\",\\n\\t\\t\"url\": \"https://www.freecodecamp.org/news/author/mehulmpt/\",\\n\\t\\t\"sameAs\": [\\n\\t\\t\\t\"https://codedamn.com\",\\n\\t\\t\\t\"https://www.facebook.com/mehulmpt\",\\n\\t\\t\\t\"https://twitter.com/mehulmpt\"\\n\\t\\t],\\n\\t\\t\"image\": {\\n\\t\\t\\t\"@type\": \"ImageObject\",\\n\\t\\t\\t\"url\": \"https://www.freecodecamp.org/news/content/images/2021/05/mehul-mohan-gravatar.jpeg\",\\n\\t\\t\\t\"width\": 250,\\n\\t\\t\\t\"height\": 250\\n\\t\\t}\\n\\t}\\n}\\n\\n\\n\\nwindow.dataLayer = window.dataLayer || [];\\n(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':\\nnew Date().getTime(),event:\\'gtm.js\\'});var f=d.getElementsByTagName(s)[0],\\nj=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&amp;l=\\'+l:\\'\\';j.async=true;j.src=\\n\\'https://www.googletagmanager.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);\\n})(window,document,\\'script\\',\\'dataLayer\\',\\'GTM-5D6RKKP\\');\\n\\n\\nSearchSubmit your search queryForumDonateSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)Passing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)Let\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)Part 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)Let\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}Note that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)In this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}Make sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)Here, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)You have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)Theforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENTfreeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)Our mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile AppOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nSearchSubmit your search queryForumDonateSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)Passing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)Let\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)Part 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)Let\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}Note that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)In this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}Make sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)Here, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)You have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)Theforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENTfreeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)Our mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile AppOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nSearchSubmit your search queryForumDonate\\nSearchSubmit your search query\\nSearchSubmit your search query\\nSearchSubmit your search query\\nSearch\\n\\nSubmit your search query\\n\\n\\nSubmit your search query\\n\\n\\n\\n\\nForumDonate\\nForumDonate\\nForum\\nDonate\\n\\n\\ndocument.addEventListener(\"DOMContentLoaded\",(()=&gt;{const e=document.getElementById(\"banner\"),t=document.getElementById(\"banner-text\");isAuthenticated?(t.innerHTML=\"Support our charity and our mission. &lt;span&gt;Donate to freeCodeCamp.org&lt;/span&gt;.\",e.href=\"https://www.freecodecamp.org/donate\",e.setAttribute(\"text-variation\",\"authenticated\")):isDonor?(t.innerHTML=\"Thank you for supporting freeCodeCamp through &lt;span&gt;your donations&lt;/span&gt;.\",e.href=\"https://www.freecodecamp.org/news/how-to-donate-to-free-code-camp/\",e.setAttribute(\"text-variation\",\"donor\")):(t.innerHTML=\"Learn to code — &lt;span&gt;free 3,000-hour curriculum&lt;/span&gt;\",e.href=\"https://www.freecodecamp.org/\",e.setAttribute(\"text-variation\",\"default\"))}));\\n\\nSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)Passing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)Let\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)Part 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)Let\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}Note that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)In this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}Make sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)Here, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)You have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)Theforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENT\\nSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)Passing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)Let\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)Part 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)Let\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}Note that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)In this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}Make sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)Here, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)You have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)Theforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENT\\nSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)Passing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)Let\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)Part 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)Let\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}Note that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)In this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}Make sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)Here, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)You have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)Theforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENT\\nSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A Website\\nSeptember 25, 2020/#Web Scraping\\nSeptember 25, 2020\\n/\\n#Web Scraping\\nWeb Scraping Python Tutorial – How to Scrape Data From A Website\\nMehul Mohan\\nMehul Mohan\\n\\nMehul Mohan\\nMehul Mohan\\nMehul Mohan\\n\\n\\n\\n\\n\\nPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)Passing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)Let\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)Part 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)Let\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}Note that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)In this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}Make sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)Here, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)You have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)Theforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get started\\nPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)Passing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)Let\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)Part 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)Let\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}Note that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)In this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}Make sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)Here, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)You have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)Theforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENT\\nPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)Passing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)Let\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)Part 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)Let\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}Note that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)In this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}Make sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)Here, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)You have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)Theforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!\\nPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.\\nPython is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.\\nIn this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.\\nNote: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.\\nNote: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.\\nIntroduction to Web Scraping classroom\\nPreview of codedamn classroom\\n\\nPreview of codedamn classroom\\nIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.\\nthis free codedamn classroom\\n\\nIn this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nhttps://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nThis classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.\\nPart 1: Loading Web Pages with \\'request\\'\\nThis is thelink to this lab.\\nlink to this lab\\nTherequestsmodule allows you to send HTTP requests using Python.\\nrequests\\nThe HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:\\nimport requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)\\nimport requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)\\nPassing requirements:\\nGet the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response (as shown above) in a variable calledtxtStore the status code (as shown above) in a variable calledstatusPrinttxtandstatususingprintfunction\\nGet the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nrequests\\nhttps://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nStore the text response (as shown above) in a variable calledtxt\\ntxt\\nStore the status code (as shown above) in a variable calledstatus\\nstatus\\nPrinttxtandstatususingprintfunction\\ntxt\\nstatus\\nprint\\nOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:\\nimport requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the result\\nimport requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the result\\nLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.\\nPart 2: Extracting title with BeautifulSoup\\nThis is thelink to this lab.\\nlink to this lab\\nIn this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:\\nBeautifulSoup\\nIt provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.\\nIt provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an application\\nBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.\\nBasically, BeautifulSoup can parse anything on the web you give it.\\nHere’s a simple example of BeautifulSoup:\\nfrom bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;\\nfrom bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;\\nPassing requirements:\\nUse therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_title\\nUse therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nrequests\\nUse BeautifulSoup to store the title of this page into a variable calledpage_title\\npage_title\\nLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:\\npage.content\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)\\nThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.\\nPart 3: Soup-ed body and head\\nThis is thelink to this lab.\\nlink to this lab\\nIn the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.\\ntitle\\nYou also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:\\n.text\\n.text\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)\\nLet\\'s take a look at how you can extract outbodyandheadsections from your pages.\\nbody\\nhead\\nPassing requirements:\\nRepeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title (without calling .text) of URL inpage_titleStore body content (without calling .text) of URL inpage_bodyStore head content (without calling .text) of URL inpage_head\\nRepeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nhttps://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nStore page title (without calling .text) of URL inpage_title\\npage_title\\nStore body content (without calling .text) of URL inpage_body\\npage_body\\nStore head content (without calling .text) of URL inpage_head\\npage_head\\nWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint(type page_body)you\\'ll see it is not a string but it works fine.\\npage_body\\npage_head\\nstrings\\nprint(type page_body)\\nThe solution of this example would be simple, based on the code above:\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)\\nPart 4: select with BeautifulSoup\\nThis is thelink to this lab.\\nlink to this lab\\nNow that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.\\nOnce you have thesoupvariable (like previous labs), you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:\\nsoup\\n.select\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text\\n.selectreturns a Python list of all the elements. This is why you selected only the first element here with the[0]index.\\n.select\\n[0]\\nPassing requirements:\\nCreate a variableall_h1_tags. Set it to empty list.Use.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.\\nCreate a variableall_h1_tags. Set it to empty list.\\nall_h1_tags\\nUse.selectto select all the&lt;h1&gt;tags and store the text of those h1 insideall_h1_tagslist.\\n.select\\n&lt;h1&gt;\\nall_h1_tags\\nCreate a variableseventh_p_textand store the text of the 7thpelement (index 6) inside.\\nseventh_p_text\\np\\nThe solution for this lab is:\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)\\nLet\\'s keep going.\\nPart 5: Top items being scraped right now\\nThis is thelink to this lab.\\nlink to this lab\\nLet\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nIf you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.\\ntop_items\\nTo pass this challenge, take care of the following things:\\nUse.selectto extract the titles. (Hint: one selector for product titles could bea.title)Use.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.Create a new dictionary in the format:\\nUse.selectto extract the titles. (Hint: one selector for product titles could bea.title)\\n.select\\na.title\\nUse.selectto extract the review count label for those product titles. (Hint: one selector for reviews could bediv.ratings) Note: this is a complete label (i.e.2 reviews) and not just a number.\\n.select\\ndiv.ratings\\n2 reviews\\nCreate a new dictionary in the format:\\ninfo = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}\\ninfo = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}\\nNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the end\\nNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.\\nstrip\\nimportant\\nAppend this dictionary in a list calledtop_items\\ntop_items\\nPrint this list at the end\\nThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)\\nNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:\\nFirst of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.\\nFirst of all you select all thediv.thumbnailelements which gives you a list of individual products\\ndiv.thumbnail\\nThen you iterate over them\\nBecauseselectallows you to chain over itself, you can use select again to get the title.\\nselect\\nNote that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 &gt; a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.\\ndiv.thumbnail\\nh4 &gt; a.title\\nFinally you strip any extra whitespace and append it to your list.\\nStraightforward right?\\nPart 6: Extracting Links\\nThis is thelink to this lab.\\nlink to this lab\\nSo far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.\\nHere’s an example of how to extract out all the image information from the page:\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)\\nIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:\\nhref\\ntext\\nYou have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:\\nYou have to create a list calledall_links\\nall_links\\nIn this list, store all link dict information. It should be in the following format:\\ninfo = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}\\ninfo = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}\\nMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip()on it.Store all these dicts in theall_linksPrint this list at the end\\nMake sure yourtextis stripped of any whitespace\\ntext\\nMake sure you check if your.textis None before you call.strip()on it.\\n.text\\n.strip()\\nStore all these dicts in theall_links\\nall_links\\nPrint this list at the end\\nYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:\\nget\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)\\nHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.\\nhref\\nPart 7: Generating CSV from data\\nThis is thelink to this lab.\\nlink to this lab\\nFinally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:\\nProduct NamePriceDescriptionReviewsProduct Image\\nProduct Name\\nPrice\\nDescription\\nReviews\\nProduct Image\\nThese products are located in thediv.thumbnail. The CSV boilerplate is given below:\\ndiv.thumbnail\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)\\nYou have to extract data from the website and generate this CSV for the three products.\\nPassing Requirements:\\nProduct Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)Price is the whitespace trimmed but full price label of the product (example - $1101.83)The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)Reviews are the whitespace trimmed version of the product (example - 7 reviews)Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)The name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfile\\nProduct Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)\\nPrice is the whitespace trimmed but full price label of the product (example - $1101.83)\\nThe description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)\\nReviews are the whitespace trimmed version of the product (example - 7 reviews)\\nProduct image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)\\nThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfile\\nproducts.csv\\nscript.py\\nLet\\'s see the solution to this lab:\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)\\nTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.\\nfor\\nWhen you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!\\nConclusion\\nI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.\\ncodedamn\\nIf you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!\\ntwitter\\nInstagram\\nADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENT\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\n\\nMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3\\nMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3\\n\\nMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3\\nMehul Mohan\\nMehul Mohan\\nIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3\\n\\nIf you read this far, thank the author to show them you care.Say Thanks\\nSay Thanks\\ndocument.addEventListener(\"DOMContentLoaded\",(()=&gt;{const t=document.getElementById(\"tweet-btn\"),e=window.location,n=\"Web%20Scraping%20Python%20Tutorial%20%E2%80%93%20How%20to%20Scrape%20Data%20From%20A%20Website\".replace(/&amp;#39;/g,\"%27\"),o=\"\",i=\"@mehulmpt\",r=Boolean(\"\");let a;if(r&amp;&amp;(o||i)){const t={originalPostAuthor:\"\",currentPostAuthor:\"Mehul Mohan\"};a=encodeURIComponent(`Thank you ${o||t.originalPostAuthor} for writing this helpful article, and ${i||t.currentPostAuthor} for translating it.`)}else!r&amp;&amp;i&amp;&amp;(a=encodeURIComponent(`Thank you ${i} for writing this helpful article.`));const h=`window.open(\\\\n    \\'${a?`https://twitter.com/intent/tweet?text=${a}%0A%0A${n}%0A%0A${e}`:`https://twitter.com/intent/tweet?text=${n}%0A%0A${e}`}\\',\\\\n    \\'share-twitter\\',\\\\n    \\'width=550, height=235\\'\\\\n  ); return false;`;t.setAttribute(\"onclick\",h)}));\\nLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get started\\nLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get started\\nGet started\\nADVERTISEMENT\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n(adsbygoogle = window.adsbygoogle || []).push({});\\nfreeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)Our mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile AppOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nfreeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)Our mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile App\\nfreeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)Our mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.\\nfreeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)\\nOur mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.\\nDonations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.\\nYou canmake a tax-deductible donation here.\\nmake a tax-deductible donation here\\nTrending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile App\\nTrending Guides\\nLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture Patterns\\nLearn CSS Transform\\nLearn CSS Transform\\nBuild a Static Blog\\nBuild a Static Blog\\nBuild an AI Chatbot\\nBuild an AI Chatbot\\nWhat is Programming?\\nWhat is Programming?\\nPython Code Examples\\nPython Code Examples\\nOpen Source for Devs\\nOpen Source for Devs\\nHTTP Networking in JS\\nHTTP Networking in JS\\nWrite React Unit Tests\\nWrite React Unit Tests\\nLearn Algorithms in JS\\nLearn Algorithms in JS\\nHow to Write Clean Code\\nHow to Write Clean Code\\nLearn PHP\\nLearn PHP\\nLearn Java\\nLearn Java\\nLearn Swift\\nLearn Swift\\nLearn Golang\\nLearn Golang\\nLearn Node.js\\nLearn Node.js\\nLearn CSS Grid\\nLearn CSS Grid\\nLearn Solidity\\nLearn Solidity\\nLearn Express.js\\nLearn Express.js\\nLearn JS Modules\\nLearn JS Modules\\nLearn Apache Kafka\\nLearn Apache Kafka\\nREST API Best Practices\\nREST API Best Practices\\nFront-End JS Development\\nFront-End JS Development\\nLearn to Build REST APIs\\nLearn to Build REST APIs\\nIntermediate TS and React\\nIntermediate TS and React\\nCommand Line for Beginners\\nCommand Line for Beginners\\nIntro to Operating Systems\\nIntro to Operating Systems\\nLearn to Build GraphQL APIs\\nLearn to Build GraphQL APIs\\nOSS Security Best Practices\\nOSS Security Best Practices\\nDistributed Systems Patterns\\nDistributed Systems Patterns\\nSoftware Architecture Patterns\\nSoftware Architecture Patterns\\n\\nMobile App\\nMobile App\\n\\n\\n\\n\\n\\n\\n\\n\\nOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nOur Charity\\nAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nAbout\\nAlumni Network\\nOpen Source\\nShop\\nSupport\\nSponsors\\nAcademic Honesty\\nCode of Conduct\\nPrivacy Policy\\nTerms of Service\\nCopyright Policy\\n\\n'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Web Scraping Python Tutorial – How to Scrape Data From A WebsiteSearchSubmit your search queryForumDonateSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:import requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPassing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the \u001b[0m\u001b[32m<\u001b[0m\u001b[32mtitle\u001b[0m\u001b[32m>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPart 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32mNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mYou have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENTfreeCodeCamp is a donor-supported tax-exempt 501\u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m charity organization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUnited States Federal Tax Identification Number: 82-0779546\u001b[0m\u001b[32m)\u001b[0m\u001b[32mOur mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile AppOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nWeb Scraping Python Tutorial – How to Scrape Data From A Website\\n\\n\\nWeb Scraping Python Tutorial – How to Scrape Data From A Website\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlet client,index;document.addEventListener\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"DOMContentLoaded\",\u001b[0m\u001b[32m(\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m{\u001b[0m\u001b[32mclient\u001b[0m\u001b[32m=\u001b[0m\u001b[32malgoliasearch\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"QMJYL5WYTI\",\"89770b24481654192d7a5c402c6ad9a0\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32mindex\u001b[0m\u001b[32m=\u001b[0m\u001b[32mclient\u001b[0m\u001b[32m.initIndex\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"news\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,document.addEventListener\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"DOMContentLoaded\",\u001b[0m\u001b[32m(\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m{\u001b[0m\u001b[32mconst \u001b[0m\u001b[32me\u001b[0m\u001b[32m=\u001b[0m\u001b[32mwindow\u001b[0m\u001b[32m.screen.width,\u001b[0m\u001b[32mt\u001b[0m\u001b[32m=\u001b[0m\u001b[32mwindow\u001b[0m\u001b[32m.screen.height,\u001b[0m\u001b[32mn\u001b[0m\u001b[32m=\u001b[0m\u001b[32me\u001b[0m\u001b[32m>=767&&t>=768?8:5,\u001b[0m\u001b[32mo\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocument\u001b[0m\u001b[32m.getElementById\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"search-form\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32ms\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocument\u001b[0m\u001b[32m.getElementById\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"search-input\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32ma\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocument\u001b[0m\u001b[32m.getElementById\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"dropdown-container\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;let i,d,c;s.addEventListener\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"input\",\u001b[0m\u001b[32m(\u001b[0m\u001b[32me\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m{\u001b[0m\u001b[32mi\u001b[0m\u001b[32m=\u001b[0m\u001b[32me\u001b[0m\u001b[32m.target.value\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,o.addEventListener\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"submit\",\u001b[0m\u001b[32m(\u001b[0m\u001b[32me\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m{\u001b[0m\u001b[32me.preventDefault\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,function\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m{\u001b[0m\u001b[32mif\u001b[0m\u001b[32m(\u001b[0m\u001b[32md\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocument\u001b[0m\u001b[32m.getElementsByClassName\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"aa-cursor\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,d&&i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m{\u001b[0m\u001b[32mconst \u001b[0m\u001b[32me\u001b[0m\u001b[32m=\u001b[0m\u001b[32md\u001b[0m\u001b[32m.querySelector\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"a\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.href;window.location.assign\u001b[0m\u001b[32m(\u001b[0m\u001b[32me\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32melse!d&&i&&c&&window.location.assign\u001b[0m\u001b[32m(\u001b[0m\u001b[32m`https://www.freecodecamp.org/news/search?\u001b[0m\u001b[32mquery\u001b[0m\u001b[32m=$\u001b[0m\u001b[32m{\u001b[0m\u001b[32mi\u001b[0m\u001b[32m}\u001b[0m\u001b[32m`\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;const \u001b[0m\u001b[32ml\u001b[0m\u001b[32m=\u001b[0m\u001b[32mautocomplete\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"#search-input\",\u001b[0m\u001b[32m{\u001b[0m\u001b[32mhint:!1,keyboardShortcuts:\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"s\",191\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,openOnFocus:!0,appendTo:a,debug:!0\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32msource:autocomplete.sources.hits\u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex,\u001b[0m\u001b[32m{\u001b[0m\u001b[32mhitsPerPage:n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,debounce:250,templates:\u001b[0m\u001b[32m{\u001b[0m\u001b[32msuggestion:\u001b[0m\u001b[32me\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m=!0,`\\\\n            <a \u001b[0m\u001b[32mhref\u001b[0m\u001b[32m=\"$\u001b[0m\u001b[32m{\u001b[0m\u001b[32me.url\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\">\\\\n              <div \u001b[0m\u001b[32mclass\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"algolia\u001b[0m\u001b[32m-result\">\\\\n                <span>$\u001b[0m\u001b[32m{\u001b[0m\u001b[32me._highlightResult.title.value\u001b[0m\u001b[32m}\u001b[0m\u001b[32m</span>\\\\n              </div>\\\\n            </a>\\\\n          `\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,empty:\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m=!1,\\'\\\\n            <div \u001b[0m\u001b[32mclass\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"aa\u001b[0m\u001b[32m-suggestion footer-suggestion no-hits-footer\">\\\\n              <div \u001b[0m\u001b[32mclass\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"algolia\u001b[0m\u001b[32m-result\">\\\\n                <span>\\\\n                  No tutorials found\\\\n                </span>\\\\n              </div>\\\\n            </div>\\\\n          \\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,footer:\u001b[0m\u001b[32me\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m{\u001b[0m\u001b[32mif\u001b[0m\u001b[32m(\u001b[0m\u001b[32m!e.isEmpty\u001b[0m\u001b[32m)\u001b[0m\u001b[32mreturn`\\\\n              <div \u001b[0m\u001b[32mclass\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"aa\u001b[0m\u001b[32m-suggestion footer-suggestion\">\\\\n                <a \u001b[0m\u001b[32mid\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"algolia\u001b[0m\u001b[32m-footer-selector\" \u001b[0m\u001b[32mhref\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://www.freecodecamp.org/news/search?\u001b[0m\u001b[32mquery\u001b[0m\u001b[32m=$\u001b[0m\u001b[32m{\u001b[0m\u001b[32mi\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\">\\\\n                  <div \u001b[0m\u001b[32mclass\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"algolia\u001b[0m\u001b[32m-result algolia-footer\">\\\\n                    See all results for $\u001b[0m\u001b[32m{\u001b[0m\u001b[32mi\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\\\n                  </div>\\\\n                </a>\\\\n              </div>\\\\n            `\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.on\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"autocomplete:selected\",\u001b[0m\u001b[32m(\u001b[0m\u001b[32m(\u001b[0m\u001b[32me,t,n,o\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m{\u001b[0m\u001b[32md\u001b[0m\u001b[32m=\u001b[0m\u001b[32mt\u001b[0m\u001b[32m?t.url:`https://www.freecodecamp.org/news/search?\u001b[0m\u001b[32mquery\u001b[0m\u001b[32m=$\u001b[0m\u001b[32m{\u001b[0m\u001b[32mi\u001b[0m\u001b[32m}\u001b[0m\u001b[32m`,\"click\"!==o.selectionMethod&&\"tabKey\"!==o.selectionMethod&&c&&window.location.assign\u001b[0m\u001b[32m(\u001b[0m\u001b[32md\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;document.addEventListener\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"click\",\u001b[0m\u001b[32m(\u001b[0m\u001b[32me\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m{\u001b[0m\u001b[32me.target!==s&&l.autocomplete.close\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,document.addEventListener\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"DOMContentLoaded\",\u001b[0m\u001b[32m(\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m{\u001b[0m\u001b[32mdayjs.extend\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdayjs_plugin_localizedFormat\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,dayjs.extend\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdayjs_plugin_relativeTime\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,dayjs.locale\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"en\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;const \u001b[0m\u001b[32misAuthenticated\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocument\u001b[0m\u001b[32m.cookie.split\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\";\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.some\u001b[0m\u001b[32m(\u001b[0m\u001b[32m(\u001b[0m\u001b[32me\u001b[0m\u001b[32m=>e.trim\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.startsWith\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"\u001b[0m\u001b[32mjwt_access_token\u001b[0m\u001b[32m=\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32misDonor\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocument\u001b[0m\u001b[32m.cookie.split\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\";\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.some\u001b[0m\u001b[32m(\u001b[0m\u001b[32m(\u001b[0m\u001b[32me\u001b[0m\u001b[32m=>e.trim\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.startsWith\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"\u001b[0m\u001b[32misDonor\u001b[0m\u001b[32m=\u001b[0m\u001b[32mtrue\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n\\t\"@context\": \"https://schema.org\",\\n\\t\"@type\": \"Article\",\\n\\t\"publisher\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n\\t\\t\"@type\": \"Organization\",\\n\\t\\t\"name\": \"freeCodeCamp.org\",\\n\\t\\t\"url\": \"https://www.freecodecamp.org/news/\",\\n\\t\\t\"logo\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n\\t\\t\\t\"@type\": \"ImageObject\",\\n\\t\\t\\t\"url\": \"https://cdn.freecodecamp.org/platform/universal/fcc_primary.svg\",\\n\\t\\t\\t\"width\": 2100,\\n\\t\\t\\t\"height\": 240\\n\\t\\t\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\t\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n\\t\"image\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n\\t\\t\"@type\": \"ImageObject\",\\n\\t\\t\"url\": \"https://www.freecodecamp.org/news/content/images/2020/09/webscrapingposter.jpg\",\\n\\t\\t\"width\": 1920,\\n\\t\\t\"height\": 1080\\n\\t\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n\\t\"url\": \"https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\",\\n\\t\"mainEntityOfPage\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n\\t\\t\"@type\": \"WebPage\",\\n\\t\\t\"@id\": \"https://www.freecodecamp.org/news/\"\\n\\t\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n\\t\"datePublished\": \"2020-09-25T20:24:10.000Z\",\\n\\t\"dateModified\": \"2020-10-26T23:54:33.000Z\",\\n\\t\"keywords\": \"Web Scraping, Python\",\\n\\t\"description\": \"Python is a beautiful language to code in. It has a great package ecosystem,\\\\nthere&#x27;s much less noise than you&#x27;ll find in other languages, and it is super\\\\neasy to use.\\\\n\\\\nPython is used for a number of things, from data analysis to server programming.\\\\nAnd one exciting use-case of Python is Web Scraping. \\\\n\\\\nIn this article, we will cover how to use Python for web scraping. We&#x27;ll also\\\\nwork through a complete hands-on classroom guide as we proceed.\\\\n\\\\nNote: We will be scraping a webpage that I host, so w\",\\n\\t\"headline\": \"Web Scraping Python Tutorial – How to Scrape Data From A Website\",\\n\\t\"author\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n\\t\\t\"@type\": \"Person\",\\n\\t\\t\"name\": \"Mehul Mohan\",\\n\\t\\t\"url\": \"https://www.freecodecamp.org/news/author/mehulmpt/\",\\n\\t\\t\"sameAs\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n\\t\\t\\t\"https://codedamn.com\",\\n\\t\\t\\t\"https://www.facebook.com/mehulmpt\",\\n\\t\\t\\t\"https://twitter.com/mehulmpt\"\\n\\t\\t\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\n\\t\\t\"image\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n\\t\\t\\t\"@type\": \"ImageObject\",\\n\\t\\t\\t\"url\": \"https://www.freecodecamp.org/news/content/images/2021/05/mehul-mohan-gravatar.jpeg\",\\n\\t\\t\\t\"width\": 250,\\n\\t\\t\\t\"height\": 250\\n\\t\\t\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\t\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n\\n\\nwindow.dataLayer = window.dataLayer || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m;\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mfunction\u001b[0m\u001b[32m(\u001b[0m\u001b[32mw,d,s,l,i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m{\u001b[0m\u001b[32mw\u001b[0m\u001b[32m[\u001b[0m\u001b[32ml\u001b[0m\u001b[32m]\u001b[0m\u001b[32m=w\u001b[0m\u001b[32m[\u001b[0m\u001b[32ml\u001b[0m\u001b[32m]\u001b[0m\u001b[32m||\u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m;w\u001b[0m\u001b[32m[\u001b[0m\u001b[32ml\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'gtm.start\\':\\nnew Date\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.getTime\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,event:\\'gtm.js\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;var \u001b[0m\u001b[32mf\u001b[0m\u001b[32m=\u001b[0m\u001b[32md\u001b[0m\u001b[32m.getElementsByTagName\u001b[0m\u001b[32m(\u001b[0m\u001b[32ms\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnj\u001b[0m\u001b[32m=\u001b[0m\u001b[32md\u001b[0m\u001b[32m.createElement\u001b[0m\u001b[32m(\u001b[0m\u001b[32ms\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32mdl\u001b[0m\u001b[32m=\u001b[0m\u001b[32ml\u001b[0m\u001b[32m!=\\'dataLayer\\'?\\'&\u001b[0m\u001b[32ml\u001b[0m\u001b[32m=\\'+l:\\'\\';j.\u001b[0m\u001b[32masync\u001b[0m\u001b[32m=\u001b[0m\u001b[32mtrue\u001b[0m\u001b[32m;j.\u001b[0m\u001b[32msrc\u001b[0m\u001b[32m=\\n\\'https://www.googletagmanager.com/gtm.js?\u001b[0m\u001b[32mid\u001b[0m\u001b[32m=\\'+i+dl;f.parentNode.insertBefore\u001b[0m\u001b[32m(\u001b[0m\u001b[32mj,f\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32mwindow,document,\\'script\\',\\'dataLayer\\',\\'GTM-5D6RKKP\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\n\\n\\nSearchSubmit your search queryForumDonateSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:import requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPassing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPart 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32mNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mYou have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENTfreeCodeCamp is a donor-supported tax-exempt 501\u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m charity organization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUnited States Federal Tax Identification Number: 82-0779546\u001b[0m\u001b[32m)\u001b[0m\u001b[32mOur mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile AppOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nSearchSubmit your search queryForumDonateSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:import requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPassing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPart 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32mNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mYou have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENTfreeCodeCamp is a donor-supported tax-exempt 501\u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m charity organization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUnited States Federal Tax Identification Number: 82-0779546\u001b[0m\u001b[32m)\u001b[0m\u001b[32mOur mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile AppOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nSearchSubmit your search queryForumDonate\\nSearchSubmit your search query\\nSearchSubmit your search query\\nSearchSubmit your search query\\nSearch\\n\\nSubmit your search query\\n\\n\\nSubmit your search query\\n\\n\\n\\n\\nForumDonate\\nForumDonate\\nForum\\nDonate\\n\\n\\ndocument.addEventListener\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"DOMContentLoaded\",\u001b[0m\u001b[32m(\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=>\u001b[0m\u001b[32m{\u001b[0m\u001b[32mconst \u001b[0m\u001b[32me\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocument\u001b[0m\u001b[32m.getElementById\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"banner\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32mt\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocument\u001b[0m\u001b[32m.getElementById\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"banner-text\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;isAuthenticated?\u001b[0m\u001b[32m(\u001b[0m\u001b[32mt.\u001b[0m\u001b[32minnerHTML\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"Support\u001b[0m\u001b[32m our charity and our mission. <span>Donate to freeCodeCamp.org</span>.\",e.\u001b[0m\u001b[32mhref\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://www.freecodecamp.org/donate\",e.setAttribute\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"text-variation\",\"authenticated\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:isDonor?\u001b[0m\u001b[32m(\u001b[0m\u001b[32mt.\u001b[0m\u001b[32minnerHTML\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"Thank\u001b[0m\u001b[32m you for supporting freeCodeCamp through <span>your donations</span>.\",e.\u001b[0m\u001b[32mhref\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://www.freecodecamp.org/news/how-to-donate-to-free-code-camp/\",e.setAttribute\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"text-variation\",\"donor\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\u001b[0m\u001b[32m(\u001b[0m\u001b[32mt.\u001b[0m\u001b[32minnerHTML\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"Learn\u001b[0m\u001b[32m to code — <span>free 3,000-hour curriculum</span>\",e.\u001b[0m\u001b[32mhref\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://www.freecodecamp.org/\",e.setAttribute\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"text-variation\",\"default\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\n\\nSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:import requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPassing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPart 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32mNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mYou have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENT\\nSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:import requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPassing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPart 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32mNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mYou have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENT\\nSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A WebsiteMehul MohanPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:import requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPassing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPart 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32mNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mYou have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get startedADVERTISEMENT\\nSeptember 25, 2020/#Web ScrapingWeb Scraping Python Tutorial – How to Scrape Data From A Website\\nSeptember 25, 2020/#Web Scraping\\nSeptember 25, 2020\\n/\\n#Web Scraping\\nWeb Scraping Python Tutorial – How to Scrape Data From A Website\\nMehul Mohan\\nMehul Mohan\\n\\nMehul Mohan\\nMehul Mohan\\nMehul Mohan\\n\\n\\n\\n\\n\\nPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:import requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPassing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPart 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32mNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mYou have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3If you read this far, thank the author to show them you care.Say ThanksLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get started\\nPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:import requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPassing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPart 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32mNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mYou have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!ADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENT\\nPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.Introduction to Web Scraping classroomPreview of codedamn classroomIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.In this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.Part 1: Loading Web Pages with \\'request\\'This is thelink to this lab.Therequestsmodule allows you to send HTTP requests using Python.The HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:import requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPassing requirements:Get the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunctionOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the resultLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.Part 2: Extracting title with BeautifulSoupThis is thelink to this lab.In this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.Basically, BeautifulSoup can parse anything on the web you give it.Here’s a simple example of BeautifulSoup:from bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>Passing requirements:Use therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_titleLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.Part 3: Soup-ed body and headThis is thelink to this lab.In the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.You also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s take a look at how you can extract outbodyandheadsections from your pages.Passing requirements:Repeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_headWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.The solution of this example would be simple, based on the code above:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPart 4: select with BeautifulSoupThis is thelink to this lab.Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.Once you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.Passing requirements:Create a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.The solution for this lab is:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32mLet\\'s keep going.Part 5: Top items being scraped right nowThis is thelink to this lab.Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.To pass this challenge, take care of the following things:Use.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the endThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32mNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:First of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.Straightforward right?Part 6: Extracting LinksThis is thelink to this lab.So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.Here’s an example of how to extract out all the image information from the page:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:You have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the endYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.Part 7: Generating CSV from dataThis is thelink to this lab.Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:Product NamePriceDescriptionReviewsProduct ImageThese products are located in thediv.thumbnail. The CSV boilerplate is given below:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mYou have to extract data from the website and generate this CSV for the three products.Passing Requirements:Product Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfileLet\\'s see the solution to this lab:import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32mTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!ConclusionI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.If you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!\\nPython is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.\\nPython is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.\\nIn this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.\\nNote: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.\\nNote: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.\\nIntroduction to Web Scraping classroom\\nPreview of codedamn classroom\\n\\nPreview of codedamn classroom\\nIf you want to code along, you can usethis free codedamn classroomthat consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.\\nthis free codedamn classroom\\n\\nIn this classroom, you\\'ll be using this page to test web scraping:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nhttps://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nThis classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.\\nPart 1: Loading Web Pages with \\'request\\'\\nThis is thelink to this lab.\\nlink to this lab\\nTherequestsmodule allows you to send HTTP requests using Python.\\nrequests\\nThe HTTP request returns a Response Object with all the response data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcontent, encoding, status, and so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One example of getting the HTML of a page:\\nimport requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\n\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'https://codedamn.com\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mres.status_code\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nPassing requirements:\\nGet the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxtStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatusPrinttxtandstatususingprintfunction\\nGet the contents of the following URL usingrequestsmodule:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nrequests\\nhttps://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nStore the text response \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledtxt\\ntxt\\nStore the status code \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas shown above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in a variable calledstatus\\nstatus\\nPrinttxtandstatususingprintfunction\\ntxt\\nstatus\\nprint\\nOnce you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:\\nimport requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the result\\nimport requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtxt, status\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# print the result\\nLet\\'s move on to part 2 now where you\\'ll build more on top of your existing code.\\nPart 2: Extracting title with BeautifulSoup\\nThis is thelink to this lab.\\nlink to this lab\\nIn this whole classroom, you’ll be using a library calledBeautifulSoupin Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:\\nBeautifulSoup\\nIt provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an applicationBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.\\nIt provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an application\\nBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.\\nBasically, BeautifulSoup can parse anything on the web you give it.\\nHere’s a simple example of BeautifulSoup:\\nfrom bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>\\nfrom bs4 import BeautifulSoup\\n\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ntitle = soup.title.text # gets you the text of the <title>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</title>\\nPassing requirements:\\nUse therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Use BeautifulSoup to store the title of this page into a variable calledpage_title\\nUse therequestspackage to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nrequests\\nUse BeautifulSoup to store the title of this page into a variable calledpage_title\\npage_title\\nLooking at the example above, you can see once we feed thepage.contentinside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:\\npage.content\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nThis was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.\\nPart 3: Soup-ed body and head\\nThis is thelink to this lab.\\nlink to this lab\\nIn the last lab, you saw how you can extract thetitlefrom the page. It is equally easy to extract out certain sections too.\\ntitle\\nYou also saw that you have to call.texton these to get the string, but you can print them without calling.texttoo, and it will give you the full markup. Try to run the example below:\\n.text\\n.text\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn.com\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_body, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nLet\\'s take a look at how you can extract outbodyandheadsections from your pages.\\nbody\\nhead\\nPassing requirements:\\nRepeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/Store page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_titleStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_bodyStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_head\\nRepeat the experiment with URL:https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nhttps://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nStore page title \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_title\\npage_title\\nStore body content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_body\\npage_body\\nStore head content \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithout calling .text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of URL inpage_head\\npage_head\\nWhen you try to print thepage_bodyorpage_headyou\\'ll see that those are printed asstrings. But in reality, when youprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32myou\\'ll see it is not a string but it works fine.\\npage_body\\npage_head\\nstrings\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtype page_body\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nThe solution of this example would be simple, based on the code above:\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage_title, page_head\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nPart 4: select with BeautifulSoup\\nThis is thelink to this lab.\\nlink to this lab\\nNow that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.\\nOnce you have thesoupvariable \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike previous labs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, you can work with.selecton it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:\\nsoup\\n.select\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Extract first <h1>\u001b[0m\u001b[32m(\u001b[0m\u001b[32m...\u001b[0m\u001b[32m)\u001b[0m\u001b[32m</h1> text\\nfirst_h1 = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n.selectreturns a Python list of all the elements. This is why you selected only the first element here with the\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32mindex.\\n.select\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\nPassing requirements:\\nCreate a variableall_h1_tags. Set it to empty list.Use.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.Create a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.\\nCreate a variableall_h1_tags. Set it to empty list.\\nall_h1_tags\\nUse.selectto select all the<h1>tags and store the text of those h1 insideall_h1_tagslist.\\n.select\\n<h1>\\nall_h1_tags\\nCreate a variableseventh_p_textand store the text of the 7thpelement \u001b[0m\u001b[32m(\u001b[0m\u001b[32mindex 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m inside.\\nseventh_p_text\\np\\nThe solution for this lab is:\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h1\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    all_h1_tags.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32melement.text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_h1_tags, seventh_p_text\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nLet\\'s keep going.\\nPart 5: Top items being scraped right now\\nThis is thelink to this lab.\\nlink to this lab\\nLet\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\nIf you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list calledtop_items. You will also extract out the reviews for these items as well.\\ntop_items\\nTo pass this challenge, take care of the following things:\\nUse.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32mUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.Create a new dictionary in the format:\\nUse.selectto extract the titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for product titles could bea.title\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n.select\\na.title\\nUse.selectto extract the review count label for those product titles. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHint: one selector for reviews could bediv.ratings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Note: this is a complete label \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.2 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and not just a number.\\n.select\\ndiv.ratings\\n2 reviews\\nCreate a new dictionary in the format:\\ninfo = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\ninfo = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.Append this dictionary in a list calledtop_itemsPrint this list at the end\\nNote that you are using thestripmethod to remove any extra newlines/whitespaces you might have in the output. This isimportantto pass this lab.\\nstrip\\nimportant\\nAppend this dictionary in a list calledtop_items\\ntop_items\\nPrint this list at the end\\nThere are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\ntop_items = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor elem in products:\\n    title = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a.title\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    review_label = elem.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text\\n    info = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"title\": title.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n        \"review\": review_label.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    top_items.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32minfo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop_items\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nNote that this is only one of the solutions. You can attempt this in a different way too. In this solution:\\nFirst of all you select all thediv.thumbnailelements which gives you a list of individual productsThen you iterate over themBecauseselectallows you to chain over itself, you can use select again to get the title.Note that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.Finally you strip any extra whitespace and append it to your list.\\nFirst of all you select all thediv.thumbnailelements which gives you a list of individual products\\ndiv.thumbnail\\nThen you iterate over them\\nBecauseselectallows you to chain over itself, you can use select again to get the title.\\nselect\\nNote that because you\\'re running inside a loop fordiv.thumbnailalready, theh4 > a.titleselector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.\\ndiv.thumbnail\\nh4 > a.title\\nFinally you strip any extra whitespace and append it to your list.\\nStraightforward right?\\nPart 6: Extracting Links\\nThis is thelink to this lab.\\nlink to this lab\\nSo far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.\\nHere’s an example of how to extract out all the image information from the page:\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nimage_data = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor image in images:\\n    src = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    alt = image.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'alt\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image_data.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"src\": src, \"alt\": alt\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage_data\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nIn this lab, your task is to extract thehrefattribute of links with theirtextas well. Make sure of the following things:\\nhref\\ntext\\nYou have to create a list calledall_linksIn this list, store all link dict information. It should be in the following format:\\nYou have to create a list calledall_links\\nall_links\\nIn this list, store all link dict information. It should be in the following format:\\ninfo = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\ninfo = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n   \"href\": \"<link here>\",\\n   \"text\": \"<link text here>\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nMake sure yourtextis stripped of any whitespaceMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.Store all these dicts in theall_linksPrint this list at the end\\nMake sure yourtextis stripped of any whitespace\\ntext\\nMake sure you check if your.textis None before you call.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32mon it.\\n.text\\n.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nStore all these dicts in theall_links\\nall_links\\nPrint this list at the end\\nYou are extracting the attribute values just like you extract values from a dict, using thegetfunction. Let\\'s take a look at the solution for this lab:\\nget\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_links = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if text is not None else \\'\\'\\n\\n    href = ahref.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'href\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    href = href.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if href is not None else \\'\\'\\n    all_links.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"href\": href, \"text\": text\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nprint\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_links\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nHere, you extract thehrefattribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.\\nhref\\nPart 7: Generating CSV from data\\nThis is thelink to this lab.\\nlink to this lab\\nFinally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:\\nProduct NamePriceDescriptionReviewsProduct Image\\nProduct Name\\nPrice\\nDescription\\nReviews\\nProduct Image\\nThese products are located in thediv.thumbnail. The CSV boilerplate is given below:\\ndiv.thumbnail\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    # TODO: Work\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"Work on product here\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nYou have to extract data from the website and generate this CSV for the three products.\\nPassing Requirements:\\nProduct Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32mPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32mReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32mProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfile\\nProduct Name is the whitespace trimmed version of the name of the item \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Adv..\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nPrice is the whitespace trimmed but full price label of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - $1101.83\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nThe description is the whitespace trimmed version of the product description \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nReviews are the whitespace trimmed version of the product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - 7 reviews\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nProduct image is the URL \u001b[0m\u001b[32m(\u001b[0m\u001b[32msrc attribute\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of the image for a product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexample - /webscraper-python-codedamn-classroom-website/cart2.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nThe name of the CSV file should beproducts.csvand should be stored in the same directory as yourscript.pyfile\\nproducts.csv\\nscript.py\\nLet\\'s see the solution to this lab:\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nsoup = BeautifulSoup\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpage.content, \\'html.parser\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# Create top_items as empty list\\nall_products = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.thumbnail\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor product in products:\\n    name = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4 > a\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    description = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'p.description\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    price = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'h4.price\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    reviews = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'div.ratings\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.text.strip\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    image = product.select\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'img\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.get\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'src\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n    all_products.append\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    \u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nkeys = all_products\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.keys\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nwith open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'products.csv\\', \\'w\\', \u001b[0m\u001b[32mnewline\u001b[0m\u001b[32m=\\'\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output_file:\\n    dict_writer = csv.DictWriter\u001b[0m\u001b[32m(\u001b[0m\u001b[32moutput_file, keys\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writeheader\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    dict_writer.writerows\u001b[0m\u001b[32m(\u001b[0m\u001b[32mall_products\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nTheforblock is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs.\\nfor\\nWhen you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!\\nConclusion\\nI hope this interactive classroom fromcodedamnhelped you understand the basics of web scraping with Python.\\ncodedamn\\nIf you liked this classroom and this blog, tell me about it on mytwitterandInstagram. Would love to hear feedback!\\ntwitter\\nInstagram\\nADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENTADVERTISEMENT\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\n\\nMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3\\nMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3\\n\\nMehul MohanIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3\\nMehul Mohan\\nMehul Mohan\\nIndependent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript <3\\n\\nIf you read this far, thank the author to show them you care.Say Thanks\\nSay Thanks\\ndocument.addEventListener\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"DOMContentLoaded\",\u001b[0m\u001b[32m(\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=\u001b[0m\u001b[32m>\u001b[0m\u001b[32m{\u001b[0m\u001b[32mconst \u001b[0m\u001b[32mt\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocument\u001b[0m\u001b[32m.getElementById\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"tweet-btn\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32me\u001b[0m\u001b[32m=\u001b[0m\u001b[32mwindow\u001b[0m\u001b[32m.location,\u001b[0m\u001b[32mn\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"Web\u001b[0m\u001b[32m%20Scraping%20Python%20Tutorial%20%E2%80%93%20How%20to%20Scrape%20Data%20From%20A%20Website\".replace\u001b[0m\u001b[32m(\u001b[0m\u001b[32m/&#39;/g,\"%27\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32mo\u001b[0m\u001b[32m=\"\",\u001b[0m\u001b[32mi\u001b[0m\u001b[32m=\"@mehulmpt\",\u001b[0m\u001b[32mr\u001b[0m\u001b[32m=\u001b[0m\u001b[32mBoolean\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;let a;if\u001b[0m\u001b[32m(\u001b[0m\u001b[32mr&&\u001b[0m\u001b[32m(\u001b[0m\u001b[32mo||i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m{\u001b[0m\u001b[32mconst \u001b[0m\u001b[32mt\u001b[0m\u001b[32m=\u001b[0m\u001b[32m{\u001b[0m\u001b[32moriginalPostAuthor:\"\",currentPostAuthor:\"Mehul Mohan\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m;\u001b[0m\u001b[32ma\u001b[0m\u001b[32m=\u001b[0m\u001b[32mencodeURIComponent\u001b[0m\u001b[32m(\u001b[0m\u001b[32m`Thank you $\u001b[0m\u001b[32m{\u001b[0m\u001b[32mo||t.originalPostAuthor\u001b[0m\u001b[32m}\u001b[0m\u001b[32m for writing this helpful article, and $\u001b[0m\u001b[32m{\u001b[0m\u001b[32mi||t.currentPostAuthor\u001b[0m\u001b[32m}\u001b[0m\u001b[32m for translating it.`\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32melse!r&&i&&\u001b[0m\u001b[32m(\u001b[0m\u001b[32ma\u001b[0m\u001b[32m=\u001b[0m\u001b[32mencodeURIComponent\u001b[0m\u001b[32m(\u001b[0m\u001b[32m`Thank you $\u001b[0m\u001b[32m{\u001b[0m\u001b[32mi\u001b[0m\u001b[32m}\u001b[0m\u001b[32m for writing this helpful article.`\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;const \u001b[0m\u001b[32mh\u001b[0m\u001b[32m=`window.open\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\n    \\'$\u001b[0m\u001b[32m{\u001b[0m\u001b[32ma?`https://twitter.com/intent/tweet?\u001b[0m\u001b[32mtext\u001b[0m\u001b[32m=$\u001b[0m\u001b[32m{\u001b[0m\u001b[32ma\u001b[0m\u001b[32m}\u001b[0m\u001b[32m%0A%0A$\u001b[0m\u001b[32m{\u001b[0m\u001b[32mn\u001b[0m\u001b[32m}\u001b[0m\u001b[32m%0A%0A$\u001b[0m\u001b[32m{\u001b[0m\u001b[32me\u001b[0m\u001b[32m}\u001b[0m\u001b[32m`:`https://twitter.com/intent/tweet?\u001b[0m\u001b[32mtext\u001b[0m\u001b[32m=$\u001b[0m\u001b[32m{\u001b[0m\u001b[32mn\u001b[0m\u001b[32m}\u001b[0m\u001b[32m%0A%0A$\u001b[0m\u001b[32m{\u001b[0m\u001b[32me\u001b[0m\u001b[32m}\u001b[0m\u001b[32m`\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\',\\\\n    \\'share-twitter\\',\\\\n    \\'\u001b[0m\u001b[32mwidth\u001b[0m\u001b[32m=\u001b[0m\u001b[32m550\u001b[0m\u001b[32m, \u001b[0m\u001b[32mheight\u001b[0m\u001b[32m=\u001b[0m\u001b[32m235\u001b[0m\u001b[32m\\'\\\\n  \u001b[0m\u001b[32m)\u001b[0m\u001b[32m; return false;`;t.setAttribute\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"onclick\",h\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get started\\nLearn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers.Get started\\nGet started\\nADVERTISEMENT\\nADVERTISEMENT\\nADVERTISEMENT\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32madsbygoogle = window.adsbygoogle || \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.push\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nfreeCodeCamp is a donor-supported tax-exempt 501\u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m charity organization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUnited States Federal Tax Identification Number: 82-0779546\u001b[0m\u001b[32m)\u001b[0m\u001b[32mOur mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile AppOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nfreeCodeCamp is a donor-supported tax-exempt 501\u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m charity organization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUnited States Federal Tax Identification Number: 82-0779546\u001b[0m\u001b[32m)\u001b[0m\u001b[32mOur mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.Trending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile App\\nfreeCodeCamp is a donor-supported tax-exempt 501\u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m charity organization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUnited States Federal Tax Identification Number: 82-0779546\u001b[0m\u001b[32m)\u001b[0m\u001b[32mOur mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.You canmake a tax-deductible donation here.\\nfreeCodeCamp is a donor-supported tax-exempt 501\u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m charity organization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUnited States Federal Tax Identification Number: 82-0779546\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nOur mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.\\nDonations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.\\nYou canmake a tax-deductible donation here.\\nmake a tax-deductible donation here\\nTrending GuidesLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture PatternsMobile App\\nTrending Guides\\nLearn CSS TransformBuild a Static BlogBuild an AI ChatbotWhat is Programming?Python Code ExamplesOpen Source for DevsHTTP Networking in JSWrite React Unit TestsLearn Algorithms in JSHow to Write Clean CodeLearn PHPLearn JavaLearn SwiftLearn GolangLearn Node.jsLearn CSS GridLearn SolidityLearn Express.jsLearn JS ModulesLearn Apache KafkaREST API Best PracticesFront-End JS DevelopmentLearn to Build REST APIsIntermediate TS and ReactCommand Line for BeginnersIntro to Operating SystemsLearn to Build GraphQL APIsOSS Security Best PracticesDistributed Systems PatternsSoftware Architecture Patterns\\nLearn CSS Transform\\nLearn CSS Transform\\nBuild a Static Blog\\nBuild a Static Blog\\nBuild an AI Chatbot\\nBuild an AI Chatbot\\nWhat is Programming?\\nWhat is Programming?\\nPython Code Examples\\nPython Code Examples\\nOpen Source for Devs\\nOpen Source for Devs\\nHTTP Networking in JS\\nHTTP Networking in JS\\nWrite React Unit Tests\\nWrite React Unit Tests\\nLearn Algorithms in JS\\nLearn Algorithms in JS\\nHow to Write Clean Code\\nHow to Write Clean Code\\nLearn PHP\\nLearn PHP\\nLearn Java\\nLearn Java\\nLearn Swift\\nLearn Swift\\nLearn Golang\\nLearn Golang\\nLearn Node.js\\nLearn Node.js\\nLearn CSS Grid\\nLearn CSS Grid\\nLearn Solidity\\nLearn Solidity\\nLearn Express.js\\nLearn Express.js\\nLearn JS Modules\\nLearn JS Modules\\nLearn Apache Kafka\\nLearn Apache Kafka\\nREST API Best Practices\\nREST API Best Practices\\nFront-End JS Development\\nFront-End JS Development\\nLearn to Build REST APIs\\nLearn to Build REST APIs\\nIntermediate TS and React\\nIntermediate TS and React\\nCommand Line for Beginners\\nCommand Line for Beginners\\nIntro to Operating Systems\\nIntro to Operating Systems\\nLearn to Build GraphQL APIs\\nLearn to Build GraphQL APIs\\nOSS Security Best Practices\\nOSS Security Best Practices\\nDistributed Systems Patterns\\nDistributed Systems Patterns\\nSoftware Architecture Patterns\\nSoftware Architecture Patterns\\n\\nMobile App\\nMobile App\\n\\n\\n\\n\\n\\n\\n\\n\\nOur CharityAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nOur Charity\\nAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of ConductPrivacy PolicyTerms of ServiceCopyright Policy\\nAbout\\nAlumni Network\\nOpen Source\\nShop\\nSupport\\nSponsors\\nAcademic Honesty\\nCode of Conduct\\nPrivacy Policy\\nTerms of Service\\nCopyright Policy\\n\\n'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text in a text file\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    file.write(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "jina_api_key ='jina_2e52c403cf164e98b4b33932e010a5b4Pu-b7mFMs178dqaBxy38TPA3f12o'\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "url = 'https://r.jina.ai/https://example.com'\n",
    "headers = {\n",
    "    'Authorization': 'Bearer {}'.format(jina_api_key),\n",
    "    'X-Return-Format': 'markdown'\n",
    "}\n",
    "\n",
    "async def fetch_jina_content(url: str, headers: dict) -> str:\n",
    "    url = f'https://r.jina.ai/{url}'\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url, headers=headers) as response:\n",
    "            response.raise_for_status()\n",
    "            return await response.text()\n",
    "\n",
    "# To call the function, you would use:\n",
    "# content = await fetch_jina_content(url, headers)\n",
    "# print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://r.jina.ai/' + results[0]['href']\n",
    "content = await fetch_jina_content(url, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://r.jina.ai/https://www.python.org/\n"
     ]
    }
   ],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text in a text file\n",
    "with open(\"output_jina.txt\", \"w\") as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://r.jina.ai/' + results[0]['href']\n",
    "content2 = await fetch_jina_content(url, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_content_markdown(content: str) -> str:\n",
    "    \"\"\"Parses the HTML content and converts it to markdown format.\n",
    "\n",
    "    Args:\n",
    "        content: The raw HTML content.\n",
    "    Returns:\n",
    "        The markdown formatted text content of the page.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Handle headings\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        heading.string = f\"{'#' * int(heading.name[1:])} {heading.get_text(strip=True)}\\n\"\n",
    "\n",
    "    # Handle paragraphs\n",
    "    for p in soup.find_all('p'):\n",
    "        p.string = f\"{p.get_text(strip=True)}\\n\\n\"\n",
    "\n",
    "    # Handle links\n",
    "    for a in soup.find_all('a'):\n",
    "        if 'href' in a.attrs:  # Check if 'href' exists\n",
    "            a.string = f\"[{a.get_text(strip=True)}]({a['href']})\"\n",
    "        \n",
    "\n",
    "    # Handle bold and italic text\n",
    "    for strong in soup.find_all('strong'):\n",
    "        strong.string = f\"**{strong.get_text(strip=True)}**\"\n",
    "    for em in soup.find_all('em'):\n",
    "        em.string = f\"*{em.get_text(strip=True)}*\"\n",
    "\n",
    "    # Handle unordered lists\n",
    "    for ul in soup.find_all('ul'):\n",
    "        for li in ul.find_all('li'):\n",
    "            li.string = f\"- {li.get_text(strip=True)}\\n\"\n",
    "\n",
    "    # Handle ordered lists\n",
    "    for ol in soup.find_all('ol'):\n",
    "        for i, li in enumerate(ol.find_all('li')):\n",
    "            li.string = f\"{i+1}. {li.get_text(strip=True)}\\n\"\n",
    "\n",
    "    # Handle code blocks\n",
    "    for pre in soup.find_all('pre'):\n",
    "        pre.string = f\"```\\n{pre.get_text()}\\n```\"\n",
    "\n",
    "    # Handle images\n",
    "    for img in soup.find_all('img'):\n",
    "        alt_text = img.get('alt', '')\n",
    "        img.replace_with(f\"![{alt_text}]({img['src']})\")\n",
    "\n",
    "    # Remove empty tags\n",
    "    for tag in soup.find_all():\n",
    "        if not tag.get_text(strip=True):\n",
    "            tag.decompose()\n",
    "\n",
    "    # Get the final markdown text\n",
    "    markdown_text = soup.get_text()\n",
    "\n",
    "    # Clean up extra newlines\n",
    "    markdown_text = re.sub(r'\\n{3,}', '\\n\\n', markdown_text)\n",
    "\n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\">\\n    <head>\\n        <meta charset=\"utf-8\">\\n        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n        \\n        \\n            <title>Web Scraping Python Tutorial – How to Scrape Data From A Website</title>\\n        \\n        <meta name=\"HandheldFriendly\" content=\"True\">\\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n\\n        <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\\n        <link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin=\"\">\\n        \\n            <link rel=\"preload\" as=\"style\" onload=\"this.onload=null;this.rel=\\'stylesheet\\'\" href=\"https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;0,700;1,400&family=Roboto+Mono:wght@400;700&display=swap\">\\n        \\n\\n        \\n        \\n    <link rel=\"preload\" as=\"style\" onload=\"this.onload=null;this.rel=\\'stylesheet\\'\" href=\"https://cdn.freecodecamp.org/news-assets/prism/1.29.0/themes/prism.min.css\">\\n<noscript>\\n  <link rel=\"stylesheet\" href=\"https://cdn.freecodecamp.org/news-assets/prism/1.29.0/themes/prism.min.css\">\\n</noscript>\\n<link rel=\"preload\" as=\"style\" onload=\"this.onload=null;this.rel=\\'stylesheet\\'\" href=\"https://cdn.freecodecamp.org/news-assets/prism/1.29.0/plugins/unescaped-markup/prism-unescaped-markup.min.css\">\\n<noscript>\\n  <link rel=\"stylesheet\" href=\"https://cdn.freecodecamp.org/news-assets/prism/1.29.0/plugins/unescaped-markup/prism-unescaped-markup.min.css\">\\n</noscript>\\n\\n<script defer=\"\" src=\"https://cdn.freecodecamp.org/news-assets/prism/1.29.0/components/prism-core.min.js\"></script>\\n<script defer=\"\" src=\"https://cdn.freecodecamp.org/news-assets/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js\"></script>\\n\\n\\n\\n        \\n        <link rel=\"preload\" as=\"style\" onload=\"this.onload=null;this.rel=\\'stylesheet\\'\" href=\"/news/assets/css/global-26e2f3cde9.css\">\\n        <link rel=\"stylesheet\" type=\"text/css\" href=\"/news/assets/css/screen-926fddfac2.css\">\\n        <link rel=\"stylesheet\" type=\"text/css\" href=\"/news/assets/css/search-bar-124f5f949c.css\">\\n\\n        \\n        <script defer=\"\" src=\"https://cdn.freecodecamp.org/news-assets/algolia/algoliasearch-3-33-0/algoliasearch.min.js\"></script>\\n        <script defer=\"\" src=\"https://cdn.freecodecamp.org/news-assets/algolia/autocomplete-0-36-0/autocomplete.min.js\"></script>\\n\\n        \\n        <script defer=\"\" src=\"https://cdn.freecodecamp.org/news-assets/dayjs/1.10.4/dayjs.min.js\"></script>\\n        <script defer=\"\" src=\"https://cdn.freecodecamp.org/news-assets/dayjs/1.10.4/localizedFormat.min.js\"></script>\\n        <script defer=\"\" src=\"https://cdn.freecodecamp.org/news-assets/dayjs/1.10.4/relativeTime.min.js\"></script>\\n\\n        \\n        \\n            <script defer=\"\" src=\"https://cdn.freecodecamp.org/news-assets/dayjs/1.10.4/locale/en.min.js\"></script>\\n        \\n\\n        \\n        <script>let client,index;document.addEventListener(\"DOMContentLoaded\",(()=>{client=algoliasearch(\"QMJYL5WYTI\",\"89770b24481654192d7a5c402c6ad9a0\"),index=client.initIndex(\"news\")})),document.addEventListener(\"DOMContentLoaded\",(()=>{const e=window.screen.width,t=window.screen.height,n=e>=767&&t>=768?8:5,o=document.getElementById(\"search-form\"),s=document.getElementById(\"search-input\"),a=document.getElementById(\"dropdown-container\");let i,d,c;s.addEventListener(\"input\",(e=>{i=e.target.value})),o.addEventListener(\"submit\",(e=>{e.preventDefault(),function(){if(d=document.getElementsByClassName(\"aa-cursor\")[0],d&&i){const e=d.querySelector(\"a\").href;window.location.assign(e)}else!d&&i&&c&&window.location.assign(`https://www.freecodecamp.org/news/search?query=${i}`)}()}));const l=autocomplete(\"#search-input\",{hint:!1,keyboardShortcuts:[\"s\",191],openOnFocus:!0,appendTo:a,debug:!0},[{source:autocomplete.sources.hits(index,{hitsPerPage:n}),debounce:250,templates:{suggestion:e=>(c=!0,`\\\\n            <a href=\"${e.url}\">\\\\n              <div class=\"algolia-result\">\\\\n                <span>${e._highlightResult.title.value}</span>\\\\n              </div>\\\\n            </a>\\\\n          `),empty:()=>(c=!1,\\'\\\\n            <div class=\"aa-suggestion footer-suggestion no-hits-footer\">\\\\n              <div class=\"algolia-result\">\\\\n                <span>\\\\n                  No tutorials found\\\\n                </span>\\\\n              </div>\\\\n            </div>\\\\n          \\'),footer:e=>{if(!e.isEmpty)return`\\\\n              <div class=\"aa-suggestion footer-suggestion\">\\\\n                <a id=\"algolia-footer-selector\" href=\"https://www.freecodecamp.org/news/search?query=${i}\">\\\\n                  <div class=\"algolia-result algolia-footer\">\\\\n                    See all results for ${i}\\\\n                  </div>\\\\n                </a>\\\\n              </div>\\\\n            `}}}]).on(\"autocomplete:selected\",((e,t,n,o)=>{d=t?t.url:`https://www.freecodecamp.org/news/search?query=${i}`,\"click\"!==o.selectionMethod&&\"tabKey\"!==o.selectionMethod&&c&&window.location.assign(d)}));document.addEventListener(\"click\",(e=>{e.target!==s&&l.autocomplete.close()}))})),document.addEventListener(\"DOMContentLoaded\",(()=>{dayjs.extend(dayjs_plugin_localizedFormat),dayjs.extend(dayjs_plugin_relativeTime),dayjs.locale(\"en\")}));const isAuthenticated=document.cookie.split(\";\").some((e=>e.trim().startsWith(\"jwt_access_token=\"))),isDonor=document.cookie.split(\";\").some((e=>e.trim().startsWith(\"isDonor=true\")));</script>\\n\\n        \\n        \\n    \\n        \\n            <script data-ad-client=\"ca-pub-9482786369113753\" src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\" crossorigin=\"anonymous\" async=\"\"></script>\\n        \\n    \\n\\n\\n        \\n        \\n    \\n        \\n    \\n        \\n    \\n\\n\\n        \\n        \\n        \\n\\n        \\n        \\n\\n        <link rel=\"icon\" href=\"https://cdn.freecodecamp.org/universal/favicons/favicon.ico\" type=\"image/png\">\\n        \\n        \\n            <link rel=\"canonical\" href=\"https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\">\\n        \\n        <meta name=\"referrer\" content=\"no-referrer-when-downgrade\">\\n\\n        \\n\\n        \\n    <meta name=\"description\" content=\"Python is a beautiful language to code in. It has a great package ecosystem, there&#39;s much less noise than you&#39;ll find in other languages, and it is super easy to use. Python is used for a number of things, from data analysis to server programming. And one exciting use-case of\">\\n\\n    \\n    <meta property=\"og:site_name\" content=\"freeCodeCamp.org\">\\n    <meta property=\"og:type\" content=\"article\">\\n    <meta property=\"og:title\" content=\"Web Scraping Python Tutorial – How to Scrape Data From A Website\">\\n    \\n        <meta property=\"og:description\" content=\"Python is a beautiful language to code in. It has a great package ecosystem, there&#39;s much less noise than you&#39;ll find in other languages, and it is super easy to use. Python is used for a number of things, from data analysis to server programming. And one exciting use-case of\">\\n    \\n    <meta property=\"og:url\" content=\"https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\">\\n    <meta property=\"og:image\" content=\"https://www.freecodecamp.org/news/content/images/2020/09/webscrapingposter.jpg\">\\n    <meta property=\"article:published_time\" content=\"2020-09-25T20:24:10.000Z\">\\n    <meta property=\"article:modified_time\" content=\"2020-10-26T23:54:33.000Z\">\\n    \\n        <meta property=\"article:tag\" content=\"Web Scraping\">\\n    \\n        <meta property=\"article:tag\" content=\"Python\">\\n    \\n    <meta property=\"article:publisher\" content=\"https://www.facebook.com/freecodecamp\">\\n    \\n        <meta property=\"article:author\" content=\"mehulmpt\">\\n    \\n\\n    \\n    <meta name=\"twitter:card\" content=\"summary_large_image\">\\n    <meta name=\"twitter:title\" content=\"Web Scraping Python Tutorial – How to Scrape Data From A Website\">\\n    \\n        <meta name=\"twitter:description\" content=\"Python is a beautiful language to code in. It has a great package ecosystem, there&#39;s much less noise than you&#39;ll find in other languages, and it is super easy to use. Python is used for a number of things, from data analysis to server programming. And one exciting use-case of\">\\n    \\n    <meta name=\"twitter:url\" content=\"https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\">\\n    <meta name=\"twitter:image\" content=\"https://www.freecodecamp.org/news/content/images/2020/09/webscrapingposter.jpg\">\\n    <meta name=\"twitter:label1\" content=\"Written by\">\\n    <meta name=\"twitter:data1\" content=\"Mehul Mohan\">\\n    <meta name=\"twitter:label2\" content=\"Filed under\">\\n    <meta name=\"twitter:data2\" content=\"Web Scraping, Python\">\\n    <meta name=\"twitter:site\" content=\"@freecodecamp\">\\n    \\n        <meta name=\"twitter:creator\" content=\"@mehulmpt\">\\n    \\n\\n    <meta property=\"og:image:width\" content=\"1920\">\\n    <meta property=\"og:image:height\" content=\"1080\">\\n\\n\\n        \\n    <script type=\"application/ld+json\">{\\n\\t\"@context\": \"https://schema.org\",\\n\\t\"@type\": \"Article\",\\n\\t\"publisher\": {\\n\\t\\t\"@type\": \"Organization\",\\n\\t\\t\"name\": \"freeCodeCamp.org\",\\n\\t\\t\"url\": \"https://www.freecodecamp.org/news/\",\\n\\t\\t\"logo\": {\\n\\t\\t\\t\"@type\": \"ImageObject\",\\n\\t\\t\\t\"url\": \"https://cdn.freecodecamp.org/platform/universal/fcc_primary.svg\",\\n\\t\\t\\t\"width\": 2100,\\n\\t\\t\\t\"height\": 240\\n\\t\\t}\\n\\t},\\n\\t\"image\": {\\n\\t\\t\"@type\": \"ImageObject\",\\n\\t\\t\"url\": \"https://www.freecodecamp.org/news/content/images/2020/09/webscrapingposter.jpg\",\\n\\t\\t\"width\": 1920,\\n\\t\\t\"height\": 1080\\n\\t},\\n\\t\"url\": \"https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\",\\n\\t\"mainEntityOfPage\": {\\n\\t\\t\"@type\": \"WebPage\",\\n\\t\\t\"@id\": \"https://www.freecodecamp.org/news/\"\\n\\t},\\n\\t\"datePublished\": \"2020-09-25T20:24:10.000Z\",\\n\\t\"dateModified\": \"2020-10-26T23:54:33.000Z\",\\n\\t\"keywords\": \"Web Scraping, Python\",\\n\\t\"description\": \"Python is a beautiful language to code in. It has a great package ecosystem,\\\\nthere&#x27;s much less noise than you&#x27;ll find in other languages, and it is super\\\\neasy to use.\\\\n\\\\nPython is used for a number of things, from data analysis to server programming.\\\\nAnd one exciting use-case of Python is Web Scraping. \\\\n\\\\nIn this article, we will cover how to use Python for web scraping. We&#x27;ll also\\\\nwork through a complete hands-on classroom guide as we proceed.\\\\n\\\\nNote: We will be scraping a webpage that I host, so w\",\\n\\t\"headline\": \"Web Scraping Python Tutorial – How to Scrape Data From A Website\",\\n\\t\"author\": {\\n\\t\\t\"@type\": \"Person\",\\n\\t\\t\"name\": \"Mehul Mohan\",\\n\\t\\t\"url\": \"https://www.freecodecamp.org/news/author/mehulmpt/\",\\n\\t\\t\"sameAs\": [\\n\\t\\t\\t\"https://codedamn.com\",\\n\\t\\t\\t\"https://www.facebook.com/mehulmpt\",\\n\\t\\t\\t\"https://twitter.com/mehulmpt\"\\n\\t\\t],\\n\\t\\t\"image\": {\\n\\t\\t\\t\"@type\": \"ImageObject\",\\n\\t\\t\\t\"url\": \"https://www.freecodecamp.org/news/content/images/2021/05/mehul-mohan-gravatar.jpeg\",\\n\\t\\t\\t\"width\": 250,\\n\\t\\t\\t\"height\": 250\\n\\t\\t}\\n\\t}\\n}</script>\\n\\n\\n        <meta name=\"generator\" content=\"Eleventy\">\\n        <link rel=\"alternate\" type=\"application/rss+xml\" title=\"freeCodeCamp.org\" href=\"https://www.freecodecamp.org/news/rss/\">\\n\\n        <link rel=\"preconnect\" href=\"https://fonts.gstatic.com\">\\n\\n<!-- dataLayer setup -->\\n<script>\\nwindow.dataLayer = window.dataLayer || [];\\n</script>\\n<!-- End dataLayer setup -->\\n\\n<!-- Google Tag Manager -->\\n<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':\\nnew Date().getTime(),event:\\'gtm.js\\'});var f=d.getElementsByTagName(s)[0],\\nj=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&l=\\'+l:\\'\\';j.async=true;j.src=\\n\\'https://www.googletagmanager.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);\\n})(window,document,\\'script\\',\\'dataLayer\\',\\'GTM-5D6RKKP\\');</script>\\n<!-- End Google Tag Manager -->\\n\\n\\n<meta name=\"google-site-verification\" content=\"b4tITLzEeeZGEpvD4mGNf3khKM4fvqejQaz9SYBQP8E\">\\n\\n        \\n        \\n\\n        \\n  <meta name=\"x-fcc-source\" data-test-label=\"x-fcc-source\" content=\"Ghost\">\\n\\n    </head>\\n\\n    \\n        <body class=\"home-template\">\\n    \\n        <div class=\"site-wrapper\">\\n            <nav class=\"site-nav nav-padding\">\\n    <div class=\"site-nav-left\">\\n        \\n<form id=\"search-form\" data-test-label=\"search-bar\">\\n    <div role=\"search\" class=\"searchbox__wrapper\">\\n        <label class=\"fcc_sr_only\" for=\"search-input\">\\n            Search\\n        </label>\\n        <input type=\"search\" placeholder=\"\\n    Search 7,600+ tutorials\\n\" id=\"search-input\">\\n        <button type=\"submit\" class=\"ais-SearchBox-submit\">\\n            <svg class=\"ais-SearchBox-submitIcon\" xmlns=\"https://www.w3.org/2000/svg\" width=\"10\" height=\"10\" viewBox=\"0 0 40 40\">\\n    <path d=\"M26.804 29.01c-2.832 2.34-6.465 3.746-10.426 3.746C7.333 32.756 0 25.424 0 16.378 0 7.333 7.333 0 16.378 0c9.046 0 16.378 7.333 16.378 16.378 0 3.96-1.406 7.594-3.746 10.426l10.534 10.534c.607.607.61 1.59-.004 2.202-.61.61-1.597.61-2.202.004L26.804 29.01zm-10.426.627c7.323 0 13.26-5.936 13.26-13.26 0-7.32-5.937-13.257-13.26-13.257C9.056 3.12 3.12 9.056 3.12 16.378c0 7.323 5.936 13.26 13.258 13.26z\"></path>\\n</svg>\\n\\n            <span class=\"fcc_sr_only\">Submit your search query</span>\\n        </button>\\n        <div id=\"dropdown-container\"></div>\\n    </div>\\n</form>\\n\\n    </div>\\n    <div class=\"site-nav-middle\">\\n        <a class=\"site-nav-logo\" href=\"https://www.freecodecamp.org/news/\" data-test-label=\"site-nav-logo\"><img src=\"https://cdn.freecodecamp.org/platform/universal/fcc_primary.svg\" alt=\"freeCodeCamp.org\"></a>\\n    </div>\\n    <div class=\"site-nav-right\">\\n        <div class=\"nav-group\">\\n            <a class=\"nav-forum\" id=\"nav-forum\" rel=\"noopener noreferrer\" href=\"https://forum.freecodecamp.org/\" target=\"_blank\" data-test-label=\"forum-button\">Forum</a>\\n            <a class=\"toggle-button-nav\" id=\"nav-donate\" rel=\"noopener noreferrer\" href=\"https://www.freecodecamp.org/donate/\" target=\"_blank\" data-test-label=\"donate-button\">Donate</a>\\n        </div>\\n    </div>\\n</nav>\\n\\n\\n            \\n            <a class=\"banner\" id=\"banner\" data-test-label=\"banner\" rel=\"noopener noreferrer\" target=\"_blank\">\\n    <p id=\"banner-text\"></p>\\n</a>\\n\\n\\n    \\n    <script>document.addEventListener(\"DOMContentLoaded\",(()=>{const e=document.getElementById(\"banner\"),t=document.getElementById(\"banner-text\");isAuthenticated?(t.innerHTML=\"Support our charity and our mission. <span>Donate to freeCodeCamp.org</span>.\",e.href=\"https://www.freecodecamp.org/donate\",e.setAttribute(\"text-variation\",\"authenticated\")):isDonor?(t.innerHTML=\"Thank you for supporting freeCodeCamp through <span>your donations</span>.\",e.href=\"https://www.freecodecamp.org/news/how-to-donate-to-free-code-camp/\",e.setAttribute(\"text-variation\",\"donor\")):(t.innerHTML=\"Learn to code — <span>free 3,000-hour curriculum</span>\",e.href=\"https://www.freecodecamp.org/\",e.setAttribute(\"text-variation\",\"default\"))}));</script>\\n\\n\\n            <div id=\"error-message\"></div>\\n\\n            \\n    <main id=\"site-main\" class=\"post-template site-main outer\">\\n        <div class=\"inner ad-layout\">\\n            <article class=\"post-full post\">\\n                <header class=\"post-full-header\">\\n                    <section class=\"post-full-meta\">\\n                        <time class=\"post-full-meta-date\" data-test-label=\"post-full-meta-date\" datetime=\"Fri Sep 25 2020 20:24:10 GMT+0000 (Coordinated Universal Time)\">\\n                            September 25, 2020\\n                        </time>\\n                        \\n                            <span class=\"date-divider\">/</span>\\n                            <a dir=\"ltr\" href=\"/news/tag/web-scraping/\">\\n                                #Web Scraping\\n                            </a>\\n                        \\n                    </section>\\n                    <h1 class=\"post-full-title\" data-test-label=\"post-full-title\">Web Scraping Python Tutorial – How to Scrape Data From A Website</h1>\\n                </header>\\n                <div class=\"post-full-author-header\" data-test-label=\"author-header-no-bio\">\\n                    \\n                        \\n                            \\n    \\n    \\n    \\n\\n    <section class=\"author-card\" data-test-label=\"author-card\">\\n        \\n            \\n    <img srcset=\"https://www.freecodecamp.org/news/content/images/size/w60/2021/05/mehul-mohan-gravatar.jpeg 60w\" sizes=\"60px\" src=\"https://www.freecodecamp.org/news/content/images/size/w60/2021/05/mehul-mohan-gravatar.jpeg\" class=\"author-profile-image\" alt=\"Mehul Mohan\" width=\"250\" height=\"250\" onerror=\"this.style.display=\\'none\\'\" data-test-label=\"profile-image\">\\n  \\n        \\n\\n        <section class=\"author-card-content author-card-content-no-bio\">\\n            <span class=\"author-card-name\">\\n                <a href=\"/news/author/mehulmpt/\" data-test-label=\"profile-link\">\\n                    \\n                        Mehul Mohan\\n                    \\n                </a>\\n            </span>\\n            \\n        </section>\\n    </section>\\n\\n                        \\n                    \\n                </div>\\n                <figure class=\"post-full-image\">\\n                    \\n    <picture>\\n      <source media=\"(max-width: 700px)\" sizes=\"1px\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 1w\">\\n      <source media=\"(min-width: 701px)\" sizes=\"(max-width: 800px) 400px, (max-width: 1170px) 700px, 1400px\" srcset=\"https://www.freecodecamp.org/news/content/images/size/w300/2020/09/webscrapingposter.jpg 300w, https://www.freecodecamp.org/news/content/images/size/w600/2020/09/webscrapingposter.jpg 600w, https://www.freecodecamp.org/news/content/images/size/w1000/2020/09/webscrapingposter.jpg 1000w, https://www.freecodecamp.org/news/content/images/size/w2000/2020/09/webscrapingposter.jpg 2000w\">\\n      <img onerror=\"this.style.display=\\'none\\'\" src=\"https://www.freecodecamp.org/news/content/images/size/w2000/2020/09/webscrapingposter.jpg\" alt=\"Web Scraping Python Tutorial – How to Scrape Data From A Website\" ,=\"\" width=\"1920\" height=\"1080\" data-test-label=\"feature-image\">\\n    </picture>\\n  \\n                </figure>\\n                <section class=\"post-full-content\">\\n                    <div class=\"post-and-sidebar\">\\n                        <section class=\"post-content \" data-test-label=\"post-content\">\\n                            \\n<p>Python is a beautiful language to code in. It has a great package ecosystem, there\\'s much less noise than you\\'ll find in other languages, and it is super easy to use.</p><p>Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping. </p><p>In this article, we will cover how to use Python for web scraping. We\\'ll also work through a complete hands-on classroom guide as we proceed.</p><p><em>Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.</em></p><h2 id=\"introduction-to-web-scraping-classroom\">Introduction to Web Scraping classroom</h2><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://www.freecodecamp.org/news/content/images/2020/09/screenzy-1601054558203.png\" class=\"kg-image\" alt=\"screenzy-1601054558203\" width=\"2000\" height=\"1272\" loading=\"lazy\"><figcaption>Preview of codedamn classroom</figcaption></figure><p>If you want to code along, you can use <a href=\"https://codedamn.com/practice/web-scraping-python-beautifulsoup\">this free codedamn classroom</a><strong> </strong>that consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.</p><p>In this classroom, you\\'ll be using this page to test web scraping: <a href=\"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\">https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/</a></p><p>This classroom consists of 7 labs, and you\\'ll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.</p><h2 id=\"part-1-loading-web-pages-with-request-\">Part 1: Loading Web Pages with \\'request\\'</h2><p>This is the <a href=\"https://codedamn.com/practice/web-scraping-python-beautifulsoup/a674e637-d958-4527-8930-cc53d1fb68e9\">link to this lab</a>.</p><p>The <code>requests</code> module allows you to send HTTP requests using Python.</p><p>The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:</p><pre><code class=\"language-py\">import requests\\n\\nres = requests.get(\\'https://codedamn.com\\')\\n\\nprint(res.text)\\nprint(res.status_code)</code></pre><h3 id=\"passing-requirements-\">Passing requirements:</h3><ul><li>Get the contents of the following URL using <code>requests</code> module: <strong>https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/</strong></li><li>Store the text response (as shown above) in a variable called <code>txt</code></li><li>Store the status code (as shown above) in a variable called <code>status</code></li><li>Print <code>txt</code> and <code>status</code> using <code>print</code> function</li></ul><p>Once you understand what is happening in the code above, it is fairly simple to pass this lab. Here\\'s the solution to this lab:</p><pre><code class=\"language-py\">import requests\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\n# Store the result in \\'res\\' variable\\nres = requests.get(\\n    \\'https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\')\\ntxt = res.text\\nstatus = res.status_code\\n\\nprint(txt, status)\\n# print the result</code></pre><p>Let\\'s move on to part 2 now where you\\'ll build more on top of your existing code.</p><h2 id=\"part-2-extracting-title-with-beautifulsoup\">Part 2: Extracting title with BeautifulSoup</h2><p>This is the <a href=\"https://codedamn.com/practice/web-scraping-python-beautifulsoup/e55282e8-8481-4fb9-9a95-5df4d4a526ce\">link to this lab</a>.</p><p>In this whole classroom, you’ll be using a library called <code>BeautifulSoup</code> in Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:</p><ol><li>It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn\\'t take much code to write an application</li><li>Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.</li></ol><p>Basically, BeautifulSoup can parse anything on the web you give it.</p><p>Here’s a simple example of BeautifulSoup:</p><pre><code class=\"language-py\">from bs4 import BeautifulSoup\\n\\npage = requests.get(\"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\ntitle = soup.title.text # gets you the text of the &lt;title&gt;(...)&lt;/title&gt;</code></pre><h3 id=\"passing-requirements--1\">Passing requirements:</h3><ul><li>Use the <code>requests</code> package to get title of the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/</li><li>Use BeautifulSoup to store the title of this page into a variable called <code>page_title</code></li></ul><p>Looking at the example above, you can see once we feed the <code>page.content</code> inside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request to https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# print the result\\nprint(page_title)</code></pre><p>This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.</p><h2 id=\"part-3-soup-ed-body-and-head\">Part 3: Soup-ed body and head</h2><p>This is the <a href=\"https://codedamn.com/practice/web-scraping-python-beautifulsoup/a91108fd-2f13-4640-ac62-d7877235376a\">link to this lab</a>.</p><p>In the last lab, you saw how you can extract the <code>title</code> from the page. It is equally easy to extract out certain sections too. </p><p>You also saw that you have to call <code>.text</code> on these to get the string, but you can print them without calling <code>.text</code> too, and it will give you the full markup. Try to run the example below:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn.com\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title.text\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_body, page_head)</code></pre><p>Let\\'s take a look at how you can extract out <code>body</code> and <code>head</code> sections from your pages.</p><h3 id=\"passing-requirements--2\">Passing requirements:</h3><ul><li>Repeat the experiment with URL: <code>https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/</code></li><li>Store page title (without calling .text) of URL in <code>page_title</code></li><li>Store body content (without calling .text) of URL in <code>page_body</code></li><li>Store head content (without calling .text) of URL in <code>page_head</code></li></ul><p>When you try to print the <code>page_body</code> or <code>page_head</code> you\\'ll see that those are printed as <code>strings</code>. But in reality, when you <code>print(type page_body)</code> you\\'ll see it is not a string but it works fine.</p><p>The solution of this example would be simple, based on the code above:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract title of page\\npage_title = soup.title\\n\\n# Extract body of page\\npage_body = soup.body\\n\\n# Extract head of page\\npage_head = soup.head\\n\\n# print the result\\nprint(page_title, page_head)</code></pre><h2 id=\"part-4-select-with-beautifulsoup\">Part 4: select with BeautifulSoup</h2><p>This is the <a href=\"https://codedamn.com/practice/web-scraping-python-beautifulsoup/0ee9fa0e-e7ac-4afa-ad8a-b4b3d16900ef\">link to this lab</a>.</p><p>Now that you have explored some parts of BeautifulSoup, let\\'s look how you can select DOM elements with BeautifulSoup methods.</p><p>Once you have the <code>soup</code> variable (like previous labs), you can work with <code>.select</code> on it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let\\'s look at an example:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Extract first &lt;h1&gt;(...)&lt;/h1&gt; text\\nfirst_h1 = soup.select(\\'h1\\')[0].text</code></pre><p><code>.select</code> returns a Python list of all the elements. This is why you selected only the first element here with the <code>[0]</code> index.</p><h3 id=\"passing-requirements--3\">Passing requirements:</h3><ul><li>Create a variable <code>all_h1_tags</code>. Set it to empty list.</li><li>Use <code>.select</code> to select all the <code>&lt;h1&gt;</code> tags and store the text of those h1 inside <code>all_h1_tags</code> list.</li><li>Create a variable <code>seventh_p_text</code> and store the text of the 7th <code>p</code> element (index 6) inside.</li></ul><p>The solution for this lab is:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create all_h1_tags as empty list\\nall_h1_tags = []\\n\\n# Set all_h1_tags to all h1 tags of the soup\\nfor element in soup.select(\\'h1\\'):\\n    all_h1_tags.append(element.text)\\n\\n# Create seventh_p_text and set it to 7th p element text of the page\\nseventh_p_text = soup.select(\\'p\\')[6].text\\n\\nprint(all_h1_tags, seventh_p_text)\\n</code></pre><p>Let\\'s keep going.</p><h2 id=\"part-5-top-items-being-scraped-right-now\">Part 5: Top items being scraped right now</h2><p>This is the <a href=\"https://codedamn.com/practice/web-scraping-python-beautifulsoup/0f404796-1b8f-491b-9b50-9e47893d2e47\">link to this lab</a>.</p><p>Let\\'s go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/</p><p>If you open this page in a new tab, you’ll see some top items. In this lab, your task is to scrape out their names and store them in a list called <code>top_items</code>. You will also extract out the reviews for these items as well.</p><p>To pass this challenge, take care of the following things:</p><ul><li>Use <code>.select</code> to extract the titles. (Hint: one selector for product titles could be <code>a.title</code>)</li><li>Use <code>.select</code> to extract the review count label for those product titles. (Hint: one selector for reviews could be <code>div.ratings</code>) Note: this is a complete label (i.e. <strong>2 reviews</strong>) and not just a number.</li><li>Create a new dictionary in the format:</li></ul><pre><code class=\"language-py\">info = {\\n   \"title\": \\'Asus AsusPro Adv...   \\'.strip(),\\n   \"review\": \\'2 reviews\\\\n\\\\n\\\\n\\'.strip()\\n}</code></pre><ul><li>Note that you are using the <code>strip</code> method to remove any extra newlines/whitespaces you might have in the output. This is <strong>important</strong> to pass this lab.</li><li>Append this dictionary in a list called <code>top_items</code></li><li>Print this list at the end</li></ul><p>There are quite a few tasks to be done in this challenge. Let\\'s take a look at the solution first and understand what is happening:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\ntop_items = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor elem in products:\\n    title = elem.select(\\'h4 &gt; a.title\\')[0].text\\n    review_label = elem.select(\\'div.ratings\\')[0].text\\n    info = {\\n        \"title\": title.strip(),\\n        \"review\": review_label.strip()\\n    }\\n    top_items.append(info)\\n\\nprint(top_items)</code></pre><p>Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:</p><ol><li>First of all you select all the <code>div.thumbnail</code> elements which gives you a list of individual products</li><li>Then you iterate over them</li><li>Because <code>select</code> allows you to chain over itself, you can use select again to get the title.</li><li>Note that because you\\'re running inside a loop for <code>div.thumbnail</code> already, the <code>h4 &gt; a.title</code> selector would only give you one result, inside a list. You select that list\\'s 0th element and extract out the text.</li><li>Finally you strip any extra whitespace and append it to your list.</li></ol><p>Straightforward right?</p><h2 id=\"part-6-extracting-links\">Part 6: Extracting Links</h2><p>This is the <a href=\"https://codedamn.com/practice/web-scraping-python-beautifulsoup/be9b007a-ff03-45d5-ad44-1adecdb21e2a\">link to this lab</a>.</p><p>So far you have seen how you can extract the text, or rather innerText of elements. Let\\'s now see how you can extract attributes by extracting links from the page.</p><p>Here’s an example of how to extract out all the image information from the page:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nimage_data = []\\n\\n# Extract and store in top_items according to instructions on the left\\nimages = soup.select(\\'img\\')\\nfor image in images:\\n    src = image.get(\\'src\\')\\n    alt = image.get(\\'alt\\')\\n    image_data.append({\"src\": src, \"alt\": alt})\\n\\nprint(image_data)</code></pre><p>In this lab, your task is to extract the <code>href</code> attribute of links with their <code>text</code> as well. Make sure of the following things:</p><ul><li>You have to create a list called <code>all_links</code></li><li>In this list, store all link dict information. It should be in the following format:</li></ul><pre><code class=\"language-py\">info = {\\n   \"href\": \"&lt;link here&gt;\",\\n   \"text\": \"&lt;link text here&gt;\"\\n}</code></pre><ul><li>Make sure your <code>text</code> is stripped of any whitespace</li><li>Make sure you check if your <code>.text</code> is None before you call <code>.strip()</code> on it.</li><li>Store all these dicts in the <code>all_links</code></li><li>Print this list at the end</li></ul><p>You are extracting the attribute values just like you extract values from a dict, using the <code>get</code> function. Let\\'s take a look at the solution for this lab:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_links = []\\n\\n# Extract and store in top_items according to instructions on the left\\nlinks = soup.select(\\'a\\')\\nfor ahref in links:\\n    text = ahref.text\\n    text = text.strip() if text is not None else \\'\\'\\n\\n    href = ahref.get(\\'href\\')\\n    href = href.strip() if href is not None else \\'\\'\\n    all_links.append({\"href\": href, \"text\": text})\\n\\nprint(all_links)\\n</code></pre><p>Here, you extract the <code>href</code> attribute just like you did in the image case. The only thing you\\'re doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace.</p><h2 id=\"part-7-generating-csv-from-data\">Part 7: Generating CSV from data</h2><p>This is the <a href=\"https://codedamn.com/practice/web-scraping-python-beautifulsoup/a10e33c1-7780-40bc-a541-adc632fab185\">link to this lab</a>.</p><p>Finally, let\\'s understand how you can generate CSV from a set of data. You will create a CSV with the following headings:</p><ol><li>Product Name</li><li>Price</li><li>Description</li><li>Reviews</li><li>Product Image</li></ol><p>These products are located in the <code>div.thumbnail</code>. The CSV boilerplate is given below:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\nall_products = []\\n\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    # TODO: Work\\n    print(\"Work on product here\")\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)\\n</code></pre><p>You have to extract data from the website and generate this CSV for the three products.</p><h3 id=\"passing-requirements--4\">Passing Requirements:</h3><ul><li>Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)</li><li>Price is the whitespace trimmed but full price label of the product (example - $1101.83)</li><li>The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)</li><li>Reviews are the whitespace trimmed version of the product (example - 7 reviews)</li><li>Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)</li><li>The name of the CSV file should be <strong>products.csv</strong> and should be stored in the same directory as your <strong>script.py</strong> file</li></ul><p>Let\\'s see the solution to this lab:</p><pre><code class=\"language-py\">import requests\\nfrom bs4 import BeautifulSoup\\nimport csv\\n# Make a request\\npage = requests.get(\\n    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\\nsoup = BeautifulSoup(page.content, \\'html.parser\\')\\n\\n# Create top_items as empty list\\nall_products = []\\n\\n# Extract and store in top_items according to instructions on the left\\nproducts = soup.select(\\'div.thumbnail\\')\\nfor product in products:\\n    name = product.select(\\'h4 &gt; a\\')[0].text.strip()\\n    description = product.select(\\'p.description\\')[0].text.strip()\\n    price = product.select(\\'h4.price\\')[0].text.strip()\\n    reviews = product.select(\\'div.ratings\\')[0].text.strip()\\n    image = product.select(\\'img\\')[0].get(\\'src\\')\\n\\n    all_products.append({\\n        \"name\": name,\\n        \"description\": description,\\n        \"price\": price,\\n        \"reviews\": reviews,\\n        \"image\": image\\n    })\\n\\n\\nkeys = all_products[0].keys()\\n\\nwith open(\\'products.csv\\', \\'w\\', newline=\\'\\') as output_file:\\n    dict_writer = csv.DictWriter(output_file, keys)\\n    dict_writer.writeheader()\\n    dict_writer.writerows(all_products)\\n</code></pre><p>The <code>for</code> block is the most interesting here. You extract all the elements and attributes from what you\\'ve learned so far in all the labs. </p><p>When you run this code, you end up with a nice CSV file. And that\\'s about all the basics of web scraping with BeautifulSoup!</p><h2 id=\"conclusion\">Conclusion</h2><p>I hope this interactive classroom from <a href=\"https://codedamn.com\">codedamn</a> helped you understand the basics of web scraping with Python. </p><p>If you liked this classroom and this blog, tell me about it on my <a href=\"https://twitter.com/mehulmpt\">twitter</a> and <a href=\"https://instagram.com/mehulmpt\">Instagram</a>. Would love to hear feedback!</p>\\n\\n                        </section>\\n                        \\n                            <div class=\"sidebar\">\\n                                \\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                        <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                                    \\n                                \\n                            </div>\\n                        \\n                    </div>\\n                    <hr>\\n                    \\n                        <div class=\"post-full-author-header\" data-test-label=\"author-header-with-bio\">\\n                            \\n                                \\n    \\n    \\n    \\n\\n    <section class=\"author-card\" data-test-label=\"author-card\">\\n        \\n            \\n    <img srcset=\"https://www.freecodecamp.org/news/content/images/size/w60/2021/05/mehul-mohan-gravatar.jpeg 60w\" sizes=\"60px\" src=\"https://www.freecodecamp.org/news/content/images/size/w60/2021/05/mehul-mohan-gravatar.jpeg\" class=\"author-profile-image\" alt=\"Mehul Mohan\" width=\"250\" height=\"250\" onerror=\"this.style.display=\\'none\\'\" loading=\"lazy\" data-test-label=\"profile-image\">\\n  \\n        \\n\\n        <section class=\"author-card-content \">\\n            <span class=\"author-card-name\">\\n                <a href=\"/news/author/mehulmpt/\" data-test-label=\"profile-link\">\\n                    \\n                        Mehul Mohan\\n                    \\n                </a>\\n            </span>\\n            \\n                \\n                    <p data-test-label=\"author-bio\">Independent developer, security engineering enthusiast, love to build and break stuff with code, and JavaScript &lt;3</p>\\n                \\n            \\n        </section>\\n    </section>\\n\\n                            \\n                        </div>\\n                        <hr>\\n                    \\n\\n                    \\n                    \\n                        \\n    \\n\\n\\n<p data-test-label=\"social-row-cta\" class=\"social-row\">\\n    If you read this far, thank the author to show them you care. <button id=\"tweet-btn\" class=\"cta-button\" data-test-label=\"tweet-button\">Say Thanks</button>\\n</p>\\n\\n\\n    \\n    <script>document.addEventListener(\"DOMContentLoaded\",(()=>{const t=document.getElementById(\"tweet-btn\"),e=window.location,n=\"Web%20Scraping%20Python%20Tutorial%20%E2%80%93%20How%20to%20Scrape%20Data%20From%20A%20Website\".replace(/&#39;/g,\"%27\"),o=\"\",i=\"@mehulmpt\",r=Boolean(\"\");let a;if(r&&(o||i)){const t={originalPostAuthor:\"\",currentPostAuthor:\"Mehul Mohan\"};a=encodeURIComponent(`Thank you ${o||t.originalPostAuthor} for writing this helpful article, and ${i||t.currentPostAuthor} for translating it.`)}else!r&&i&&(a=encodeURIComponent(`Thank you ${i} for writing this helpful article.`));const h=`window.open(\\\\n    \\'${a?`https://twitter.com/intent/tweet?text=${a}%0A%0A${n}%0A%0A${e}`:`https://twitter.com/intent/tweet?text=${n}%0A%0A${e}`}\\',\\\\n    \\'share-twitter\\',\\\\n    \\'width=550, height=235\\'\\\\n  ); return false;`;t.setAttribute(\"onclick\",h)}));</script>\\n\\n\\n                        \\n\\n<div class=\"learn-cta-row\" data-test-label=\"learn-cta-row\">\\n    <p>\\n        Learn to code for free. freeCodeCamp\\'s open source curriculum has helped more than 40,000 people get jobs as developers. <a href=\"https://www.freecodecamp.org/learn/\" class=\"cta-button\" id=\"learn-to-code-cta\" rel=\"noopener noreferrer\" target=\"_blank\">Get started</a>\\n    </p>\\n</div>\\n\\n                    \\n                </section>\\n                \\n                    <div class=\"banner-ad bottom\">\\n                        \\n                            <div class=\"ad-wrapper\" data-test-label=\"ad-wrapper\">\\n    \\n    <div class=\"ad-text\" data-test-label=\"ad-text\">ADVERTISEMENT</div>\\n    <ins class=\"adsbygoogle\" style=\"display: block; height: 280px;\" data-ad-client=\"ca-pub-9482786369113753\" data-ad-slot=\"5720086888\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins>\\n    <script>\\n        (adsbygoogle = window.adsbygoogle || []).push({});\\n    </script>\\n</div>\\n\\n                        \\n                    </div>\\n                \\n            </article>\\n        </div>\\n    </main>\\n\\n\\n            \\n\\n\\n<footer class=\"site-footer\">\\n    <div class=\"footer-top\">\\n        <div class=\"footer-desc-col\">\\n            <p data-test-label=\"tax-exempt-status\">freeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax Identification Number: 82-0779546)</p>\\n            <p data-test-label=\"mission-statement\">Our mission: to help people learn to code for free. We accomplish this by creating thousands of videos, articles, and interactive coding lessons - all freely available to the public.</p>\\n            <p data-test-label=\"donation-initiatives\">Donations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.</p>\\n            <p class=\"footer-donation\" data-test-label=\"donate-text\">\\n                You can <a href=\"https://www.freecodecamp.org/donate/\" class=\"inline\" rel=\"noopener noreferrer\" target=\"_blank\">make a tax-deductible donation here</a>.\\n            </p>\\n        </div>\\n        <div class=\"trending-guides\" data-test-label=\"trending-guides\">\\n            <h2 id=\"trending-guides\" class=\"col-header\">Trending Guides</h2>\\n            <ul class=\"trending-guides-articles\" aria-labelledby=\"trending-guides\">\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/complete-guide-to-css-transform-functions-and-properties/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn CSS Transform\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/how-to-create-a-static-blog-with-lume/\" rel=\"noopener noreferrer\" target=\"_blank\">Build a Static Blog\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/how-to-build-an-ai-chatbot-with-redis-python-and-gpt/\" rel=\"noopener noreferrer\" target=\"_blank\">Build an AI Chatbot\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/what-is-programming-tutorial-for-beginners/\" rel=\"noopener noreferrer\" target=\"_blank\">What is Programming?\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/python-code-examples-sample-script-coding-tutorial-for-beginners/\" rel=\"noopener noreferrer\" target=\"_blank\">Python Code Examples\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/a-practical-guide-to-start-opensource-contributions/\" rel=\"noopener noreferrer\" target=\"_blank\">Open Source for Devs\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/http-full-course/\" rel=\"noopener noreferrer\" target=\"_blank\">HTTP Networking in JS\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/how-to-write-unit-tests-in-react-redux/\" rel=\"noopener noreferrer\" target=\"_blank\">Write React Unit Tests\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/introduction-to-algorithms-with-javascript-examples/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn Algorithms in JS\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/how-to-write-clean-code/\" rel=\"noopener noreferrer\" target=\"_blank\">How to Write Clean Code\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/the-php-handbook/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn PHP\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/the-java-handbook/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn Java\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/the-swift-handbook/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn Swift\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/learn-golang-handbook/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn Golang\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/get-started-with-nodejs/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn Node.js\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/complete-guide-to-css-grid/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn CSS Grid\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/learn-solidity-handbook/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn Solidity\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/the-express-handbook/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn Express.js\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/javascript-es-modules-and-module-bundlers/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn JS Modules\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/apache-kafka-handbook/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn Apache Kafka\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/rest-api-design-best-practices-build-a-rest-api/\" rel=\"noopener noreferrer\" target=\"_blank\">REST API Best Practices\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/front-end-javascript-development-react-angular-vue-compared/\" rel=\"noopener noreferrer\" target=\"_blank\">Front-End JS Development\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/build-consume-and-document-a-rest-api/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn to Build REST APIs\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/build-strongly-typed-polymorphic-components-with-react-and-typescript/\" rel=\"noopener noreferrer\" target=\"_blank\">Intermediate TS and React\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/command-line-for-beginners/\" rel=\"noopener noreferrer\" target=\"_blank\">Command Line for Beginners\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/an-introduction-to-operating-systems/\" rel=\"noopener noreferrer\" target=\"_blank\">Intro to Operating Systems\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/building-consuming-and-documenting-a-graphql-api/\" rel=\"noopener noreferrer\" target=\"_blank\">Learn to Build GraphQL APIs\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/oss-security-best-practices/\" rel=\"noopener noreferrer\" target=\"_blank\">OSS Security Best Practices\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/design-patterns-for-distributed-systems/\" rel=\"noopener noreferrer\" target=\"_blank\">Distributed Systems Patterns\\n                    </a>\\n                </li>\\n                <li>\\n                    <a href=\"https://www.freecodecamp.org/news/an-introduction-to-software-architecture-patterns/\" rel=\"noopener noreferrer\" target=\"_blank\">Software Architecture Patterns\\n                    </a>\\n                </li>\\n            </ul>\\n            <div class=\"spacer\" style=\"padding: 15px 0;\"></div>\\n            <div>\\n                <h2 id=\"mobile-app\" class=\"col-header\">\\n                    Mobile App\\n                </h2>\\n                <div class=\"min-h-[1px] px-[15px] md:w-2/3 md:ml-[16.6%]\">\\n                    <ul aria-labelledby=\"mobile-app\" class=\"mobile-app-container\">\\n                        <li>\\n                            <a href=\"https://apps.apple.com/us/app/freecodecamp/id6446908151?itsct=apps_box_link&itscg=30200\" rel=\"noopener noreferrer\" target=\"_blank\">\\n                                <img src=\"https://cdn.freecodecamp.org/platform/universal/apple-store-badge.svg\" lang=\"en\" alt=\"Download on the App Store\">\\n                            </a>\\n                        </li>\\n                        <li>\\n                            <a href=\"https://play.google.com/store/apps/details?id=org.freecodecamp\" rel=\"noopener noreferrer\" target=\"_blank\">\\n                                <img src=\"https://cdn.freecodecamp.org/platform/universal/google-play-badge.svg\" lang=\"en\" alt=\"Get it on Google Play\">\\n                            </a>\\n                        </li>\\n                    </ul>\\n                </div>\\n            </div>\\n        </div>\\n    </div>\\n    <div class=\"footer-bottom\">\\n        <h2 class=\"col-header\" data-test-label=\"our-nonprofit\">Our Charity</h2>\\n        <div class=\"our-nonprofit\">\\n            <a href=\"https://www.freecodecamp.org/news/about/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"about\">\\n                About\\n            </a>\\n            <a href=\"https://www.linkedin.com/school/free-code-camp/people/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"alumni\">\\n                Alumni Network\\n            </a>\\n            <a href=\"https://github.com/freeCodeCamp/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"open-source\">\\n                Open Source\\n            </a>\\n            <a href=\"https://www.freecodecamp.org/news/shop/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"shop\">\\n                Shop\\n            </a>\\n            <a href=\"https://www.freecodecamp.org/news/support/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"support\">\\n                Support\\n            </a>\\n            <a href=\"https://www.freecodecamp.org/news/sponsors/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"sponsors\">\\n                Sponsors\\n            </a>\\n            <a href=\"https://www.freecodecamp.org/news/academic-honesty-policy/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"honesty\">\\n                Academic Honesty\\n            </a>\\n            <a href=\"https://www.freecodecamp.org/news/code-of-conduct/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"coc\">\\n                Code of Conduct\\n            </a>\\n            <a href=\"https://www.freecodecamp.org/news/privacy-policy/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"privacy\">\\n                Privacy Policy\\n            </a>\\n            <a href=\"https://www.freecodecamp.org/news/terms-of-service/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"tos\">\\n                Terms of Service\\n            </a>\\n            <a href=\"https://www.freecodecamp.org/news/copyright-policy/\" rel=\"noopener noreferrer\" target=\"_blank\" data-test-label=\"copyright\">\\n                Copyright Policy\\n            </a>\\n        </div>\\n    </div>\\n</footer>\\n\\n        </div>\\n\\n        \\n        \\n        \\n\\n        <!-- Google Tag Manager (noscript) -->\\n<noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-5D6RKKP\" height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>\\n<!-- End Google Tag Manager (noscript) -->\\n\\n        \\n    </body>\\n</html>\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "content3 = parse_content_markdown(text_html)\n",
    "# save text in a text file\n",
    "with open(\"output_mk.txt\", \"w\") as file:\n",
    "    file.write(content3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brave import AsyncBrave\n",
    "\n",
    "brave = AsyncBrave(api_key='BSA2ididmmVVYAHiX4CNsF598VVyXNA')\n",
    "\n",
    "query = \"Best way to webscrape in python\"\n",
    "num_results = 10\n",
    "\n",
    "# Use 'await' to perform the search asynchronously\n",
    "search_results = await brave.search(q=query, count=num_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_using_brave(query, num_results=5, api_key=api_key):\n",
    "    brave = AsyncBrave(api_key=api_key)\n",
    "    search_results = await brave.search(q=query, count=num_results)\n",
    "    web_results = search_results.web_results\n",
    "    urls = [x['url'].unicode_string() for x in web_results]\n",
    "    return {'results':[{'url':url} for url in urls]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://nanonets.com/blog/web-scraping-with-python-tutorial/'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0]['url'].unicode_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Web Scraping Python Tutorial - How to Scrape Data From A Website', 'href': 'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/', 'body': 'Note that this is only one of the solutions. You can attempt this in a different way too. In this solution: First of all you select all the div.thumbnail elements which gives you a list of individual products; Then you iterate over them; Because select allows you to chain over itself, you can use select again to get the title.'}, {'title': 'How To Scrape Data from Any Website: 5 Code and No-Code Methods', 'href': 'https://www.scrapin.io/blog/web-scraping', 'body': 'To help you with this, here are some of the methods that you can use depending on your data extraction needs: \\u200d. 1. Manual Scraping with Upwork and Fiverr. If you are interested in manual data scraping, you can hire a freelancer via popular freelancing platforms like Upwork and Fiverr.'}, {'title': 'My ultimate guide to web scraping : r/datascience - Reddit', 'href': 'https://www.reddit.com/r/datascience/comments/a116l5/my_ultimate_guide_to_web_scraping/', 'body': \"A space for data science professionals to engage in discussions and debates on the subject of data science. My ultimate guide to web scraping. I've been doing some freelance web scraping for a few years now and thought it might be interesting to create a multi-part tutorial on building a scraping project with a data science end goal.\"}, {'title': 'Python Web Scraping: Full Tutorial With Examples (2024)', 'href': 'https://www.scrapingbee.com/blog/web-scraping-101-with-python/', 'body': 'Copying Our Target XPath from Chrome Dev Tools. Open Chrome Dev Tools (press F12 key or right-click on the webpage and select \"Inspect\") Use the element selector tool to highlight the element you want to scrape. Right-click the highlighted element in the Dev Tools panel. Select \"Copy\" and then \"Copy XPath\".'}, {'title': 'Beautiful Soup: Build a Web Scraper With Python - Real Python', 'href': 'https://realpython.com/beautiful-soup-web-scraper-python/', 'body': \"The incredible amount of data on the Internet is a rich resource for any field of research or personal interest. To effectively harvest that data, you'll need to become skilled at web scraping.The Python libraries requests and Beautiful Soup are powerful tools for the job. If you like to learn with hands-on examples and have a basic understanding of Python and HTML, then this tutorial is for ...\"}]\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import AsyncDDGS\n",
    "\n",
    "results = await AsyncDDGS().atext(\"Best way to webscrape\", max_results=5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'results'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://www.scrapin.io/blog/web-scraping'</span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://www.reddit.com/r/datascience/comments/a116l5/my_ultimate_guide_to_web_scraping/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://www.scrapingbee.com/blog/web-scraping-101-with-python/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://realpython.com/beautiful-soup-web-scraper-python/'</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'results'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/'\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://www.scrapin.io/blog/web-scraping'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://www.reddit.com/r/datascience/comments/a116l5/my_ultimate_guide_to_web_scraping/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://www.scrapingbee.com/blog/web-scraping-101-with-python/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m{\u001b[0m\u001b[32m'url'\u001b[0m: \u001b[32m'https://realpython.com/beautiful-soup-web-scraper-python/'\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_results = {'results':[{'url': result['href']} for result in results]}\n",
    "pprint(formatted_results)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://r.jina.ai/' + results[0]['href']\n",
    "content = await fetch_jina_content(url, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def chunk_text_by_tokens(text, max_token_length=100, overlap=0):\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    tokens = enc.encode(text,disallowed_special=())\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(tokens), max_token_length - overlap):\n",
    "        chunk_tokens = tokens[i:i + max_token_length]\n",
    "        chunk_text = enc.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text_by_tokens(content,max_token_length=500, overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2098"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "from bm25s import BM25\n",
    "import Stemmer\n",
    "def create_bm25(chunks):\n",
    "    tokenized_chunks = bm25s.tokenize([chunk for chunk in chunks],stopwords='en',stemmer=Stemmer.Stemmer(\"english\"))\n",
    "\n",
    "    bm25 = BM25()\n",
    "    bm25.index(tokenized_chunks)\n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "bm25_index = create_bm25(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-code/)\\n*   [Learn PHP](https://www.freecodecamp.org/news/the-php-handbook/)\\n*   [Learn Java](https://www.freecodecamp.org/news/the-java-handbook/)\\n*   [Learn Swift](https://www.freecodecamp.org/news/the-swift-handbook/)\\n*   [Learn Golang](https://www.freecodecamp.org/news/learn-golang-handbook/)\\n*   [Learn Node.js](https://www.freecodecamp.org/news/get-started-with-nodejs/)\\n*   [Learn CSS Grid](https://www.freecodecamp.org/news/complete-guide-to-css-grid/)\\n*   [Learn Solidity](https://www.freecodecamp.org/news/learn-solidity-handbook/)\\n*   [Learn Express.js](https://www.freecodecamp.org/news/the-express-handbook/)\\n*   [Learn JS Modules](https://www.freecodecamp.org/news/javascript-es-modules-and-module-bundlers/)\\n*   [Learn Apache Kafka](https://www.freecodecamp.org/news/apache-kafka-handbook/)\\n*   [REST API Best Practices](https://www.freecodecamp.org/news/rest-api-design-best-practices-build-a-rest-api/)\\n*   [Front-End JS Development](https://www.freecodecamp.org/news/front-end-javascript-development-react-angular-vue-compared/)\\n*   [Learn to Build REST APIs](https://www.freecodecamp.org/news/build-consume-and-document-a-rest-api/)\\n*   [Intermediate TS and React](https://www.freecodecamp.org/news/build-strongly-typed-polymorphic-components-with-react-and-typescript/)\\n*   [Command Line for Beginners](https://www.freecodecamp.org/news/command-line-for-beginners/)\\n*   [Intro to Operating Systems](https://www.freecodecamp.org/news/an-introduction-to-operating-systems/)\\n*   [Learn to Build GraphQL APIs](https://www.freecodecamp.org/news/building-consuming-and-documenting-a-graphql-api/)\\n*   [OSS Security Best Practices](https://www.freecodecamp.org/news/oss-security-best-practices/)\\n*   [Distributed Systems Patterns](https://www.freecodecamp.org/news/design-patterns-for-distributed-systems/)\\n*   [Software Architecture Patterns](https://www.freecodecamp.org/news/an-introduction-to-software-architecture-patterns/)\\n\\nMobile App\\n----------\\n\\n*   [![Image 6: Download on the App Store]('"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "query_tokens = bm25s.tokenize('python best practice webscrape',stopwords='en',stemmer=Stemmer.Stemmer(\"english\"))\n",
    "docs, scores = bm25_index.retrieve(query_tokens, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-code/)\\n*   [Learn PHP](https://www.freecodecamp.org/news/the-php-handbook/)\\n*   [Learn Java](https://www.freecodecamp.org/news/the-java-handbook/)\\n*   [Learn Swift](https://www.freecodecamp.org/news/the-swift-handbook/)\\n*   [Learn Golang](https://www.freecodecamp.org/news/learn-golang-handbook/)\\n*   [Learn Node.js](https://www.freecodecamp.org/news/get-started-with-nodejs/)\\n*   [Learn CSS Grid](https://www.freecodecamp.org/news/complete-guide-to-css-grid/)\\n*   [Learn Solidity](https://www.freecodecamp.org/news/learn-solidity-handbook/)\\n*   [Learn Express.js](https://www.freecodecamp.org/news/the-express-handbook/)\\n*   [Learn JS Modules](https://www.freecodecamp.org/news/javascript-es-modules-and-module-bundlers/)\\n*   [Learn Apache Kafka](https://www.freecodecamp.org/news/apache-kafka-handbook/)\\n*   [REST API Best Practices](https://www.freecodecamp.org/news/rest-api-design-best-practices-build-a-rest-api/)\\n*   [Front-End JS Development](https://www.freecodecamp.org/news/front-end-javascript-development-react-angular-vue-compared/)\\n*   [Learn to Build REST APIs](https://www.freecodecamp.org/news/build-consume-and-document-a-rest-api/)\\n*   [Intermediate TS and React](https://www.freecodecamp.org/news/build-strongly-typed-polymorphic-components-with-react-and-typescript/)\\n*   [Command Line for Beginners](https://www.freecodecamp.org/news/command-line-for-beginners/)\\n*   [Intro to Operating Systems](https://www.freecodecamp.org/news/an-introduction-to-operating-systems/)\\n*   [Learn to Build GraphQL APIs](https://www.freecodecamp.org/news/building-consuming-and-documenting-a-graphql-api/)\\n*   [OSS Security Best Practices](https://www.freecodecamp.org/news/oss-security-best-practices/)\\n*   [Distributed Systems Patterns](https://www.freecodecamp.org/news/design-patterns-for-distributed-systems/)\\n*   [Software Architecture Patterns](https://www.freecodecamp.org/news/an-introduction-to-software-architecture-patterns/)\\n\\nMobile App\\n----------\\n\\n*   [![Image 6: Download on the App Store]('"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MiniLM-L-12-v2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default flashrank model for language en\n",
      "Default Model: ms-marco-MiniLM-L-12-v2\n",
      "Loading FlashRankRanker model ms-marco-MiniLM-L-12-v2\n",
      "Loading model FlashRank model ms-marco-MiniLM-L-12-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ms-marco-MiniLM-L-12-v2.zip: 100%|██████████| 21.6M/21.6M [00:03<00:00, 6.64MiB/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcontent\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
